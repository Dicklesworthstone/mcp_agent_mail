{"id":"mcp_agent_mail-01h","title":"Implement am doctor diagnostic command","description":"Add 'am doctor' CLI command to diagnose and repair common failure modes: stale locks, orphaned records, FTS sync, archive-DB consistency. Includes backup before any destructive operations.","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-06T19:49:53.466699-05:00","created_by":"jemanuel","updated_at":"2026-01-06T19:50:53.547583-05:00","closed_at":"2026-01-06T19:50:53.547583-05:00","close_reason":"Already implemented - discovered during exploration"}
{"id":"mcp_agent_mail-0uh","title":"Replace sorted()[:n] with heapq.nlargest() for top-N selection","description":"# Replace sorted()[:n] with heapq.nlargest()\n\n## Problem Statement\nSeveral places in the code use `sorted(items, key=...)[:n]` to get the top N items.\nThis is O(m log m) where m = total items, but only O(m log n) is needed.\n\n## Code Locations\n`src/mcp_agent_mail/app.py` - Lines approximately 3204, 3215, 6278\n\n## Current Code (THE PROBLEM)\n```python\n# Getting top 10 mentions\nsorted_mentions = sorted(mentions.items(), key=lambda x: -x[1])[:10]\n\n# This sorts ALL items (O(m log m)) just to get 10!\n```\n\n## Impact Analysis\n- For m=1000 items, sorted() does ~10,000 comparisons\n- heapq.nlargest(10, ...) does ~1000 comparisons (one pass + 10 heap ops)\n- 10x fewer comparisons for this example\n- Savings grow with collection size\n\n## Required Fix\n```python\nimport heapq\n\n# Before: O(m log m)\nsorted_mentions = sorted(mentions.items(), key=lambda x: -x[1])[:10]\n\n# After: O(m log n) where n=10\ntop_mentions = heapq.nlargest(10, mentions.items(), key=lambda x: x[1])\n```\n\n## Algorithm Comparison\n```\nCollection size (m) | sorted()[:10] | heapq.nlargest(10)\n--------------------|---------------|-------------------\n100                 | ~660 ops      | ~100 ops\n1,000               | ~10,000 ops   | ~1,000 ops\n10,000              | ~130,000 ops  | ~10,000 ops\n```\n\n## Implementation Details\n\n### Finding All Occurrences\nSearch for patterns like:\n```python\nsorted(...)[:N]\nsorted(...)[0:N]\nsorted(..., reverse=True)[:N]\n```\n\n### Key Function Adjustment\n- sorted() with `key=lambda x: -x[1]` (negative for descending)\n- heapq.nlargest() naturally returns largest, so use positive key:\n  `key=lambda x: x[1]`\n\n### For nsmallest\nIf code uses `sorted(...)[:n]` without reverse, that's getting smallest.\nUse `heapq.nsmallest(n, items, key=...)` instead.\n\n## Edge Cases\n- n \u003e= len(items): heapq still works, returns all items sorted\n- n = 0: returns empty list\n- n = 1: heapq is overkill but still correct\n\n## When NOT to Change\n- If n is close to len(items), sorted() is actually better\n- Heuristic: Use heapq when n \u003c len(items) / 5\n\n## Testing Strategy\n1. Verify output is identical for various inputs\n2. Benchmark with large collections\n3. Test edge cases (empty, n \u003e len)\n\n## Verification\n```python\nimport heapq\nimport random\n\nitems = [(f\"item{i}\", random.randint(0, 1000)) for i in range(10000)]\n\n# These should produce identical results\nresult1 = sorted(items, key=lambda x: -x[1])[:10]\nresult2 = heapq.nlargest(10, items, key=lambda x: x[1])\n\nassert result1 == result2, \"Results differ!\"\n```\n\n## Dependencies\nNone - self-contained changes","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-12T01:09:24.746029498-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:09:24.746029498-05:00"}
{"id":"mcp_agent_mail-16q","title":"Integration Tests: Archive Save/Restore","description":"priority: 2","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.486081-05:00","updated_at":"2026-01-05T21:00:46.059248-05:00","deleted_at":"2026-01-05T21:00:46.059248-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-1kw","title":"Fix triple snippet() calls in FTS search queries","description":"# Fix Triple snippet() Calls in Search\n\n## Problem Statement\nSearch queries call SQLite's snippet() function THREE times per row:\n1. Once to get the snippet\n2. Once to calculate length\n3. Once more inside replace() for hit counting\n\nThis triples the work for generating search results.\n\n## Code Location\n`src/mcp_agent_mail/http.py` - Lines approximately 1388-1389\n\n## Current Code (THE PROBLEM)\n```sql\nSELECT \n    snippet(fts_messages, 2, '\u003cmark\u003e', '\u003c/mark\u003e', '…', 18) AS body_snippet,\n    -- THIS CALLS snippet() TWO MORE TIMES!\n    (length(snippet(fts_messages, 2, '\u003cmark\u003e', '\u003c/mark\u003e', '…', 18)) \n     - length(replace(snippet(fts_messages, 2, '\u003cmark\u003e', '\u003c/mark\u003e', '…', 18), '\u003cmark\u003e', ''))) \n    / 6 AS hits\nFROM fts_messages WHERE ...\n```\n\n## Impact Analysis\n- snippet() scans the FTS index and extracts matching fragments\n- Calling 3x per row triples the FTS extraction work\n- For 100 search results, that's 300 snippet() calls instead of 100\n\n## Required Fix\nCall snippet() once in SQL, count hits in Python:\n\n```sql\nSELECT \n    snippet(fts_messages, 2, '\u003cmark\u003e', '\u003c/mark\u003e', '…', 18) AS body_snippet\n    -- No more hit calculation in SQL\nFROM fts_messages WHERE ...\n```\n\n```python\n# In Python, after fetching results:\nfor row in results:\n    body_snippet = row.body_snippet\n    # Count hits efficiently in Python\n    hits = body_snippet.count('\u003cmark\u003e')\n    # Or if you need the formula: (len with marks - len without) / len('\u003cmark\u003e')\n```\n\n## Why Python Is Better Here\n- String operations in Python are highly optimized (C implementation)\n- We already have the snippet in memory\n- Avoids additional FTS index scans\n- Code is more readable and maintainable\n\n## Implementation Details\n\n### Finding the Query\nSearch http.py for \"snippet(\" to find all occurrences.\nMay be multiple search endpoints that need fixing.\n\n### Handling Edge Cases\n- Empty snippet: hits = 0\n- No \u003cmark\u003e tags: hits = 0\n- Very long snippets: Still O(n) string scan, but only once\n\n## Testing Strategy\n1. Search query returning 100 results\n2. Time before/after fix\n3. Verify hit counts are identical\n4. Verify snippets are identical\n\n## Performance Measurement\n```python\nimport time\n\n# Before fix\nstart = time.perf_counter()\nresults = await search(\"test query\", limit=100)\nbefore_time = time.perf_counter() - start\n\n# After fix\nstart = time.perf_counter()\nresults = await search(\"test query\", limit=100)\nafter_time = time.perf_counter() - start\n\nprint(f\"Before: {before_time:.3f}s, After: {after_time:.3f}s\")\n# Expected: ~3x improvement\n```\n\n## Dependencies\nNone - self-contained fix","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-12T01:09:23.69685367-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:09:23.69685367-05:00"}
{"id":"mcp_agent_mail-21j","title":"Add LIMIT to potentially unbounded queries","description":"# Add LIMIT to Unbounded Queries\n\n## Problem Statement\nSome queries use hardcoded high LIMIT values (e.g., LIMIT 10000) or no LIMIT\nat all, potentially returning far more data than needed.\n\n## Code Locations\nSearch for:\n- `LIMIT 10000` or similar high limits\n- SELECT statements without LIMIT\n- `.all()` calls on potentially large tables\n\n## Current Code Examples (THE PROBLEM)\n```python\n# Hardcoded high limit - might fetch 10,000 when user needs 20\nstmt = select(Message).where(...).limit(10000)\n\n# No limit at all - could return millions\nstmt = select(FileReservation).where(FileReservation.project_id == project_id)\n```\n\n## Impact Analysis\n- Overfetching wastes database I/O\n- Wastes network bandwidth\n- Wastes application memory\n- Wastes serialization CPU\n- Slower response times for users who need small results\n\n## Required Fix\nAdd reasonable defaults with user override:\n\n```python\nasync def list_messages(\n    project_id: int,\n    limit: int = 100,  # Sensible default\n    offset: int = 0,\n) -\u003e list[Message]:\n    \"\"\"List messages with pagination.\n    \n    Args:\n        project_id: Project to query\n        limit: Max results (default 100, max 1000)\n        offset: Skip first N results\n    \"\"\"\n    # Enforce maximum to prevent abuse\n    limit = min(limit, 1000)\n    \n    stmt = (\n        select(Message)\n        .where(Message.project_id == project_id)\n        .order_by(Message.created_ts.desc())\n        .limit(limit)\n        .offset(offset)\n    )\n    ...\n```\n\n## Pagination Pattern\nFor large result sets, implement cursor-based pagination:\n\n```python\nasync def list_messages_paginated(\n    project_id: int,\n    after_id: int | None = None,\n    limit: int = 100,\n) -\u003e tuple[list[Message], int | None]:\n    \"\"\"List messages with cursor pagination.\n    \n    Returns:\n        Tuple of (messages, next_cursor) where next_cursor is None if no more.\n    \"\"\"\n    stmt = select(Message).where(Message.project_id == project_id)\n    \n    if after_id:\n        stmt = stmt.where(Message.id \u003e after_id)\n    \n    stmt = stmt.order_by(Message.id).limit(limit + 1)  # Fetch one extra\n    \n    result = await session.execute(stmt)\n    messages = list(result.scalars().all())\n    \n    # Check if there are more\n    if len(messages) \u003e limit:\n        messages = messages[:limit]\n        next_cursor = messages[-1].id\n    else:\n        next_cursor = None\n    \n    return messages, next_cursor\n```\n\n## Where to Apply\n1. Internal list functions that feed API endpoints\n2. Search queries\n3. Any query that could return unbounded results\n\n## Testing Strategy\n1. Verify default limit is applied\n2. Verify user can request smaller limit\n3. Verify max limit is enforced\n4. Verify pagination works correctly\n\n## Dependencies\nNone - can be applied incrementally","status":"open","priority":3,"issue_type":"task","created_at":"2026-01-12T01:12:24.387994975-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:12:24.387994975-05:00"}
{"id":"mcp_agent_mail-23f","title":"Unit Tests: guard.py - Pre-commit","description":"priority: 2","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.481906-05:00","updated_at":"2026-01-05T21:00:46.98086-05:00","deleted_at":"2026-01-05T21:00:46.98086-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-24j","title":"Replace string normalization while loop with regex","description":"# Replace String Normalization While Loop with Regex\n\n## Problem Statement\nString normalization uses a while loop to collapse multiple spaces, which\niterates O(k) times where k is the number of space sequences to collapse.\n\n## Code Location\n`src/mcp_agent_mail/app.py` - Lines approximately 1054-1055\n\n## Current Code (THE PROBLEM)\n```python\n# Collapse multiple spaces - O(n*k) where k = number of iterations\nwhile \"  \" in trimmed:\n    trimmed = trimmed.replace(\"  \", \" \")\n```\n\n## Impact Analysis\n- Input: \"hello     world\" (5 spaces)\n- Iteration 1: \"hello   world\" (3 spaces) - scans entire string\n- Iteration 2: \"hello  world\" (2 spaces) - scans entire string again\n- Iteration 3: \"hello world\" (1 space) - scans entire string again\n- Total: 3 full string scans for one normalization\n\nWith very long strings or many space sequences, this compounds significantly.\n\n## Required Fix\n```python\nimport re\n\n# O(n) single pass with regex\ntrimmed = re.sub(r' +', ' ', trimmed)\n```\n\n## Regex Explanation\n- `' +'` matches one or more spaces\n- Replacement `' '` collapses to single space\n- Single pass through string, no iteration\n\n## Pre-compiled Pattern (Optional Optimization)\nIf this function is called frequently:\n```python\n# At module level\n_MULTI_SPACE_RE = re.compile(r' +')\n\n# In function\ntrimmed = _MULTI_SPACE_RE.sub(' ', trimmed)\n```\n\n## Semantic Differences to Consider\nThe regex `' +'` matches one or more spaces, which will also \"replace\"\nsingle spaces with themselves. This is slightly different from the while\nloop which only triggers on 2+ spaces.\n\nMore precise regex to match original semantics:\n```python\ntrimmed = re.sub(r' {2,}', ' ', trimmed)  # Only matches 2+ spaces\n```\n\n## Testing Strategy\n1. Test with single space - unchanged\n2. Test with double space - collapsed\n3. Test with 10 spaces - collapsed to 1\n4. Test with multiple space groups - all collapsed\n5. Test empty string - unchanged\n6. Test no spaces - unchanged\n\n## Verification\n```python\ntest_cases = [\n    (\"hello world\", \"hello world\"),      # Single space unchanged\n    (\"hello  world\", \"hello world\"),     # Double collapsed\n    (\"hello     world\", \"hello world\"),  # Many collapsed\n    (\"  leading\", \" leading\"),           # Leading collapsed\n    (\"trailing  \", \"trailing \"),         # Trailing collapsed\n    (\"\", \"\"),                            # Empty unchanged\n]\n\nfor input_str, expected in test_cases:\n    result = re.sub(r' +', ' ', input_str)\n    assert result == expected, f\"Failed: {input_str!r}\"\n```\n\n## Dependencies\nNone - self-contained fix","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-12T01:10:48.840958136-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:10:48.840958136-05:00"}
{"id":"mcp_agent_mail-24o","title":"Integration Tests: Concurrent Access","description":"priority: 2","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.486681-05:00","updated_at":"2026-01-05T21:00:45.888166-05:00","deleted_at":"2026-01-05T21:00:45.888166-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-24w","title":"P2 - Guard Hook Tests","description":"Test pre-commit and pre-push guards.","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T21:02:17.867107-05:00","updated_at":"2026-01-05T21:02:37.56359-05:00","deleted_at":"2026-01-05T21:02:37.56359-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-2e4","title":"Integration Tests: Contact Management Flow","description":"priority: 2","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.484957-05:00","updated_at":"2026-01-05T21:00:46.302378-05:00","deleted_at":"2026-01-05T21:00:46.302378-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-2qc","title":"P2 - CLI Integration Tests","description":"Test CLI commands work correctly.","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T21:02:17.854864-05:00","updated_at":"2026-01-05T21:02:37.391256-05:00","deleted_at":"2026-01-05T21:02:37.391256-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-2rk","title":"Unit Tests: http.py - Rate Limiting","description":"priority: 3","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.480753-05:00","updated_at":"2026-01-05T21:00:47.069147-05:00","deleted_at":"2026-01-05T21:00:47.069147-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-2uf","title":"Phase 4: Cleanup and Polish","description":"# Phase 4: Low Priority Optimizations (Marginal Gains)\n\n## Scope\nThis phase addresses minor inefficiencies and cleanup items that provide marginal\nperformance benefits but improve code quality and prevent future issues.\n\n## Issues Addressed\n\n### Memory and Resource Management\n1. **fetchall() vs streaming** - Loading entire result sets into memory\n2. **Process lock dict growth** - Never cleaned up, potential memory leak\n3. **Connection pool tuning** - Default settings may not be optimal\n\n### Query Optimization\n1. **LIMIT 10000 overfetching** - Fetching far more than needed\n2. **FTS5 trigger overhead** - Triggers fire on every insert/update\n\n### Configuration\n1. **PRAGMA tuning** - synchronous, journal_mode, cache_size\n2. **Pool size optimization** - Based on expected concurrency\n\n## Expected Impact\n- Memory usage: Reduced peak memory for large operations\n- Query efficiency: Marginal latency improvements\n- Stability: Prevent potential issues under sustained load\n\n## Why Low Priority\nThese items:\n- Have small absolute impact on typical workloads\n- Require careful testing to avoid regressions\n- May have complex trade-offs (e.g., PRAGMA synchronous affects durability)\n\n## Implementation Notes\n- Streaming requires refactoring callers to handle iterators\n- Pool tuning should be based on profiling data\n- PRAGMA changes need thorough testing for data integrity","status":"open","priority":3,"issue_type":"feature","created_at":"2026-01-12T01:05:53.354148779-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:05:53.354148779-05:00","dependencies":[{"issue_id":"mcp_agent_mail-2uf","depends_on_id":"mcp_agent_mail-vih","type":"blocks","created_at":"2026-01-12T01:06:18.812119619-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-2uf","depends_on_id":"mcp_agent_mail-gt4","type":"blocks","created_at":"2026-01-12T01:13:12.103227533-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-2uf","depends_on_id":"mcp_agent_mail-b11","type":"blocks","created_at":"2026-01-12T01:13:12.125274015-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-2uf","depends_on_id":"mcp_agent_mail-21j","type":"blocks","created_at":"2026-01-12T01:13:12.1485374-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-2uf","depends_on_id":"mcp_agent_mail-6m1","type":"blocks","created_at":"2026-01-12T01:13:12.172238549-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-2uf","depends_on_id":"mcp_agent_mail-9zj","type":"blocks","created_at":"2026-01-12T01:13:12.194594685-05:00","created_by":"ubuntu"}]}
{"id":"mcp_agent_mail-2x6","title":"Unit Tests: storage.py - Inbox/Outbox","description":"priority: 2","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.472327-05:00","updated_at":"2026-01-05T21:00:47.596135-05:00","deleted_at":"2026-01-05T21:00:47.596135-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-2xf","title":"Regression: Agent Name Validation","description":"priority: 0","status":"closed","priority":0,"issue_type":"task","created_at":"2026-01-05T21:02:17.831463-05:00","updated_at":"2026-01-05T21:43:01.516522-05:00","closed_at":"2026-01-05T21:43:01.516522-05:00","close_reason":"Completed: 38 regression tests for agent name validation covering validate_agent_name_format, generate_agent_name, sanitize_agent_name, and MCP integration flows in both coerce and strict modes"}
{"id":"mcp_agent_mail-3ph","title":"Test Coverage: Achieve 80% Overall","description":"priority: 2","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.488404-05:00","updated_at":"2026-01-05T21:00:45.374664-05:00","deleted_at":"2026-01-05T21:00:45.374664-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-3x5","title":"CLI: Guard Commands","description":"priority: 2","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-05T21:02:17.858346-05:00","updated_at":"2026-01-06T00:13:06.442902-05:00","closed_at":"2026-01-06T00:13:06.442902-05:00","close_reason":"26 tests pass for CLI Guard Commands (status, install, uninstall, check)","dependencies":[{"issue_id":"mcp_agent_mail-3x5","depends_on_id":"mcp_agent_mail-aew","type":"blocks","created_at":"2026-01-05T21:02:54.828161-05:00","created_by":"jemanuel"}]}
{"id":"mcp_agent_mail-47v","title":"Unit Tests: db.py - Session Management","description":"priority: 2","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.468243-05:00","updated_at":"2026-01-05T21:00:48.679337-05:00","deleted_at":"2026-01-05T21:00:48.679337-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-48a","title":"Unit Tests: cli.py - Guard Commands","description":"priority: 2","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.472972-05:00","updated_at":"2026-01-05T21:00:47.425206-05:00","deleted_at":"2026-01-05T21:00:47.425206-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-4em","title":"Performance: Baseline Benchmarks","description":"priority: 3","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-05T21:02:17.877976-05:00","updated_at":"2026-01-06T02:13:34.992751-05:00","closed_at":"2026-01-06T02:13:34.992751-05:00","close_reason":"Added MCP tool latency benchmarks: message send, inbox fetch, search, file reservation conflict check, archive write, and summary report. Tests use unique project keys and create_agent_identity for proper isolation. All tests pass with p95 thresholds.","dependencies":[{"issue_id":"mcp_agent_mail-4em","depends_on_id":"mcp_agent_mail-yzu","type":"blocks","created_at":"2026-01-05T21:02:55.70686-05:00","created_by":"jemanuel"}]}
{"id":"mcp_agent_mail-4qc","title":"Unit Tests: cli.py - Archive Commands","description":"priority: 2","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.47394-05:00","updated_at":"2026-01-05T21:00:47.326503-05:00","deleted_at":"2026-01-05T21:00:47.326503-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-4yy","title":"Add composite database indexes for common query patterns","description":"# Add Composite Database Indexes\n\n## Problem Statement\nThe database has individual column indexes but lacks composite indexes that cover\ncommon multi-column query patterns, forcing table scans and index merges.\n\n## Code Location\n`src/mcp_agent_mail/db.py` - Schema definitions section\n\n## Current State\nIndividual indexes likely exist on primary keys and foreign keys, but composite\nindexes for common WHERE clause combinations are missing.\n\n## Required Indexes\n\n### 1. message_recipients - Project + Recipient Name\n```python\nIndex(\"ix_msgrecip_project_recipient\", \"project_id\", \"recipient_name\")\n```\n**Justification**: Inbox queries filter by project_id AND recipient_name.\nWithout this, requires index merge or table scan.\n\n### 2. file_reservations - Project + Agent + Active Status\n```python\nIndex(\"ix_fileresv_project_agent_active\", \"project_id\", \"agent_id\", \"released_ts\")\n```\n**Justification**: Reservation conflict checks query active reservations\n(released_ts IS NULL) for a project. This covers that pattern.\n\n### 3. agent_links - Project + Target Agent + Status\n```python\nIndex(\"ix_agentlink_project_target_status\", \"project_id\", \"to_agent_id\", \"status\")\n```\n**Justification**: Contact policy lookups filter by project, target, and status.\n\n### 4. messages - Project + Created Timestamp (Descending)\n```python\nIndex(\"ix_msg_project_created_desc\", \"project_id\", \"created_ts\", postgresql_using=\"btree\")\n```\n**Justification**: Most message queries are \"recent messages in project\" which\nneeds project filtering with timestamp ordering.\n\n### 5. FTS5 Covering Index (if applicable)\nReview FTS5 configuration to ensure it includes project_id for filtering\nwithout requiring a JOIN back to the messages table.\n\n## Implementation Notes\n\n### SQLite Index Creation\n```python\nfrom sqlmodel import Index\n\nclass MessageRecipient(SQLModel, table=True):\n    # ... fields ...\n    \n    __table_args__ = (\n        Index(\"ix_msgrecip_project_recipient\", \"project_id\", \"recipient_name\"),\n    )\n```\n\n### Migration Strategy\nSince this project has no production users, we can modify the schema directly.\nIf there was existing data, we'd need an Alembic migration.\n\n## Testing Strategy\n1. Use EXPLAIN QUERY PLAN on common queries before/after\n2. Verify indexes are being used (no \"SCAN TABLE\" in plan)\n3. Benchmark query times with realistic data volumes\n\n## Verification Commands\n```sql\n-- Check existing indexes\nSELECT name, sql FROM sqlite_master WHERE type='index';\n\n-- Analyze query plan\nEXPLAIN QUERY PLAN SELECT * FROM message_recipients \nWHERE project_id = 1 AND recipient_name = 'TestAgent';\n```\n\n## Risk Assessment\n- **Risk**: Indexes slow down INSERT/UPDATE operations\n- **Mitigation**: These tables are read-heavy; read improvement \u003e\u003e write cost\n- **Monitoring**: Track insert times if concerned\n\n## Dependencies\nNone - can be done independently of N+1 fixes","status":"open","priority":0,"issue_type":"task","created_at":"2026-01-12T01:07:57.068827018-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:07:57.068827018-05:00","dependencies":[{"issue_id":"mcp_agent_mail-4yy","depends_on_id":"mcp_agent_mail-k92","type":"blocks","created_at":"2026-01-12T01:12:50.102341328-05:00","created_by":"ubuntu"}]}
{"id":"mcp_agent_mail-55z","title":"Test Coverage: Achieve 95% Overall","description":"priority: 3","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.488615-05:00","updated_at":"2026-01-05T21:00:45.283815-05:00","deleted_at":"2026-01-05T21:00:45.283815-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-5cx","title":"Unit Tests: app.py - Core Helpers","description":"priority: 2","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.468876-05:00","updated_at":"2026-01-05T21:00:48.514677-05:00","deleted_at":"2026-01-05T21:00:48.514677-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-6aq","title":"Test Coverage: Achieve 50% Overall","description":"priority: 1","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.488157-05:00","updated_at":"2026-01-05T21:00:45.462014-05:00","deleted_at":"2026-01-05T21:00:45.462014-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-6m1","title":"Review and tune SQLite PRAGMA settings","description":"# Review SQLite PRAGMA Settings\n\n## Problem Statement\nDefault SQLite settings may not be optimal for the MCP Agent Mail workload.\nTuning PRAGMAs can improve performance significantly.\n\n## Code Location\n`src/mcp_agent_mail/db.py` - Engine creation and initialization\n\n## Key PRAGMAs to Review\n\n### 1. journal_mode (Currently likely: DELETE)\n```sql\nPRAGMA journal_mode = WAL;\n```\n- WAL allows concurrent reads during writes\n- Better for read-heavy workloads (which this is)\n- Slight increase in disk usage\n\n### 2. synchronous (Currently likely: FULL)\n```sql\n-- For development/testing:\nPRAGMA synchronous = NORMAL;\n\n-- For production with data safety:\nPRAGMA synchronous = FULL;\n```\n- NORMAL is 2-10x faster but slightly less durable\n- FULL ensures data survives power loss\n- Consider making configurable\n\n### 3. cache_size (Currently likely: default ~2MB)\n```sql\nPRAGMA cache_size = -64000;  -- 64MB (negative = KB)\n```\n- Larger cache = fewer disk reads\n- Set based on available memory\n- Should be 10-25% of expected database size\n\n### 4. temp_store (Currently likely: DEFAULT)\n```sql\nPRAGMA temp_store = MEMORY;\n```\n- Keeps temp tables in memory\n- Faster sorts and joins\n- More memory usage\n\n### 5. mmap_size (Currently likely: 0)\n```sql\nPRAGMA mmap_size = 268435456;  -- 256MB\n```\n- Memory-map the database file\n- Can improve read performance\n- Platform-dependent behavior\n\n## Implementation\n```python\nasync def configure_sqlite_pragmas(engine: AsyncEngine):\n    \"\"\"Configure SQLite for optimal performance.\"\"\"\n    async with engine.begin() as conn:\n        await conn.execute(text(\"PRAGMA journal_mode = WAL\"))\n        await conn.execute(text(\"PRAGMA synchronous = NORMAL\"))\n        await conn.execute(text(\"PRAGMA cache_size = -64000\"))\n        await conn.execute(text(\"PRAGMA temp_store = MEMORY\"))\n        await conn.execute(text(\"PRAGMA mmap_size = 268435456\"))\n```\n\n## Configuration Approach\nMake PRAGMAs configurable via settings:\n\n```python\nclass DatabaseSettings:\n    sqlite_journal_mode: str = \"WAL\"\n    sqlite_synchronous: str = \"NORMAL\"  # NORMAL for dev, FULL for prod\n    sqlite_cache_size_kb: int = 64000\n    sqlite_mmap_size: int = 256 * 1024 * 1024\n```\n\n## Testing Strategy\n1. Benchmark with default settings\n2. Benchmark with optimized settings\n3. Verify data integrity with power-loss simulation\n4. Test on different platforms (Linux, macOS, Windows)\n\n## Risk Assessment\n- **journal_mode=WAL**: Low risk, widely used\n- **synchronous=NORMAL**: Medium risk, document for users\n- **cache_size**: Low risk, just memory tradeoff\n- **mmap_size**: Platform-dependent, test thoroughly\n\n## Verification\n```python\n# Check current settings\nasync with engine.begin() as conn:\n    for pragma in [\"journal_mode\", \"synchronous\", \"cache_size\", \"temp_store\"]:\n        result = await conn.execute(text(f\"PRAGMA {pragma}\"))\n        print(f\"{pragma}: {result.scalar()}\")\n```\n\n## Dependencies\nNone - can be done independently","status":"open","priority":3,"issue_type":"task","created_at":"2026-01-12T01:12:24.57380359-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:12:24.57380359-05:00"}
{"id":"mcp_agent_mail-7a4","title":"P3 - Security Tests","description":"Test security-sensitive areas.","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T21:02:17.869383-05:00","updated_at":"2026-01-05T21:02:37.650483-05:00","deleted_at":"2026-01-05T21:02:37.650483-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-7pp","title":"Integration Tests: Full Messaging Flow","description":"priority: 1","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.484043-05:00","updated_at":"2026-01-05T21:00:46.467406-05:00","deleted_at":"2026-01-05T21:00:46.467406-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-7ue","title":"Add serve-stdio command for stdio transport","description":"Add CLI command to run MCP server via stdio transport (requested in GitHub #41). This enables project-local installation patterns where Claude Code launches the server directly.","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-06T23:39:36.679856-05:00","created_by":"jemanuel","updated_at":"2026-01-06T23:45:11.386347-05:00","closed_at":"2026-01-06T23:45:11.386347-05:00","close_reason":"Implemented serve-stdio command with test"}
{"id":"mcp_agent_mail-88l","title":"Unit Tests: llm.py - Provider Integration","description":"priority: 3","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.483764-05:00","updated_at":"2026-01-05T21:00:46.562021-05:00","deleted_at":"2026-01-05T21:00:46.562021-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-8hr","title":"Guards: Pre-push Enforcement","description":"priority: 2","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-05T21:02:17.868497-05:00","updated_at":"2026-01-06T00:21:17.959806-05:00","closed_at":"2026-01-06T00:21:17.959806-05:00","close_reason":"Added 20 comprehensive pre-push enforcement tests covering hook rendering, env controls, conflict detection, pattern matching, multi-commit scenarios, and edge cases. All tests pass.","dependencies":[{"issue_id":"mcp_agent_mail-8hr","depends_on_id":"mcp_agent_mail-irp","type":"blocks","created_at":"2026-01-05T21:02:55.109306-05:00","created_by":"jemanuel"}]}
{"id":"mcp_agent_mail-8rr","title":"Unit Tests: app.py - MCP Resources","description":"priority: 2","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.470814-05:00","updated_at":"2026-01-05T21:00:47.866436-05:00","deleted_at":"2026-01-05T21:00:47.866436-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-9bz","title":"Milestone: Full Integration Coverage","description":"priority: 2","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-05T21:02:17.881845-05:00","updated_at":"2026-01-06T01:04:04.832257-05:00","closed_at":"2026-01-06T01:04:04.832257-05:00","close_reason":"Full Integration Coverage achieved: CLI Guard Commands (26), HTTP Server/Transport (19), CLI Mail Commands (29), plus Critical Path Coverage milestone complete. All 74 core tests passing.","dependencies":[{"issue_id":"mcp_agent_mail-9bz","depends_on_id":"mcp_agent_mail-tm6","type":"blocks","created_at":"2026-01-05T21:02:57.199388-05:00","created_by":"jemanuel"},{"issue_id":"mcp_agent_mail-9bz","depends_on_id":"mcp_agent_mail-n6z","type":"blocks","created_at":"2026-01-05T21:02:57.30292-05:00","created_by":"jemanuel"},{"issue_id":"mcp_agent_mail-9bz","depends_on_id":"mcp_agent_mail-3x5","type":"blocks","created_at":"2026-01-05T21:02:57.404731-05:00","created_by":"jemanuel"},{"issue_id":"mcp_agent_mail-9bz","depends_on_id":"mcp_agent_mail-9z6","type":"blocks","created_at":"2026-01-05T21:02:57.494389-05:00","created_by":"jemanuel"}]}
{"id":"mcp_agent_mail-9he","title":"Unit Tests: app.py - Messaging","description":"priority: 2","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.469601-05:00","updated_at":"2026-01-05T21:00:48.26081-05:00","deleted_at":"2026-01-05T21:00:48.26081-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-9sh","title":"Unit Tests: app.py - MCP Tools","description":"priority: 1","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.471023-05:00","updated_at":"2026-01-05T21:00:47.765853-05:00","deleted_at":"2026-01-05T21:00:47.765853-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-9tj","title":"Add asyncio.Lock to rate limiter bucket access","description":"# Add Synchronization to Rate Limiter\n\n## Problem Statement\nThe token bucket rate limiter accesses shared state (bucket dict) without\nsynchronization, leading to potential race conditions under concurrent load.\n\n## Code Location\n`src/mcp_agent_mail/http.py` - Lines approximately 306-314\n\n## Current Code (THE PROBLEM)\n```python\nclass TokenBucketRateLimiter:\n    def __init__(self, rate: float, burst: int):\n        self._rate = rate\n        self._burst = burst\n        self._buckets: dict[str, tuple[float, float]] = {}  # No lock!\n    \n    async def check(self, key: str) -\u003e bool:\n        now = time.monotonic()\n        tokens, last_ts = self._buckets.get(key, (float(self._burst), now))\n        \n        # Time-based token replenishment\n        elapsed = now - last_ts\n        tokens = min(float(self._burst), tokens + elapsed * self._rate)\n        \n        if tokens \u003e= 1.0:\n            tokens -= 1.0\n            self._buckets[key] = (tokens, now)  # Race condition!\n            return True\n        \n        self._buckets[key] = (tokens, now)\n        return False\n```\n\n## Race Condition Scenario\n1. Request A reads bucket: tokens=1.0\n2. Request B reads bucket: tokens=1.0 (before A writes)\n3. Request A writes: tokens=0.0\n4. Request B writes: tokens=0.0\n5. Both requests allowed, but should have been 1 allowed, 1 denied!\n\n## Impact Analysis\n- Under concurrent load, rate limits can be exceeded\n- Security/DoS implications if rate limiting is for protection\n- Subtle bug - may only manifest under high concurrency\n\n## Required Fix\n```python\nimport asyncio\n\nclass TokenBucketRateLimiter:\n    def __init__(self, rate: float, burst: int):\n        self._rate = rate\n        self._burst = burst\n        self._buckets: dict[str, tuple[float, float]] = {}\n        self._lock = asyncio.Lock()  # Add lock\n    \n    async def check(self, key: str) -\u003e bool:\n        async with self._lock:  # Synchronize access\n            now = time.monotonic()\n            tokens, last_ts = self._buckets.get(key, (float(self._burst), now))\n            \n            elapsed = now - last_ts\n            tokens = min(float(self._burst), tokens + elapsed * self._rate)\n            \n            if tokens \u003e= 1.0:\n                tokens -= 1.0\n                self._buckets[key] = (tokens, now)\n                return True\n            \n            self._buckets[key] = (tokens, now)\n            return False\n```\n\n## Performance Consideration\nA single global lock may be too coarse if there are many independent keys.\nConsider per-key locking for better concurrency:\n\n```python\nclass TokenBucketRateLimiter:\n    def __init__(self, rate: float, burst: int):\n        self._rate = rate\n        self._burst = burst\n        self._buckets: dict[str, tuple[float, float]] = {}\n        self._locks: dict[str, asyncio.Lock] = {}\n        self._locks_lock = asyncio.Lock()  # Lock for creating locks\n    \n    async def _get_lock(self, key: str) -\u003e asyncio.Lock:\n        if key not in self._locks:\n            async with self._locks_lock:\n                if key not in self._locks:  # Double-check\n                    self._locks[key] = asyncio.Lock()\n        return self._locks[key]\n    \n    async def check(self, key: str) -\u003e bool:\n        lock = await self._get_lock(key)\n        async with lock:\n            # ... rate limit logic\n```\n\n## Testing Strategy\n1. Unit test: Single request succeeds\n2. Unit test: Burst requests up to limit succeed\n3. Unit test: Request after burst denied\n4. Concurrency test: Many concurrent requests don't exceed limit\n5. Stress test: High concurrency doesn't cause errors\n\n## Verification\n```python\nimport asyncio\n\nlimiter = TokenBucketRateLimiter(rate=10, burst=10)\n\nasync def hammer(n_requests: int) -\u003e int:\n    \"\"\"Make many concurrent requests, count allowed.\"\"\"\n    tasks = [limiter.check(\"test-key\") for _ in range(n_requests)]\n    results = await asyncio.gather(*tasks)\n    return sum(results)\n\n# Should allow at most burst (10) requests\nallowed = asyncio.run(hammer(100))\nassert allowed \u003c= 10, f\"Rate limit exceeded: {allowed} \u003e 10\"\n```\n\n## Dependencies\nNone - self-contained fix","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-12T01:10:52.411836481-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:10:52.411836481-05:00"}
{"id":"mcp_agent_mail-9z5","title":"HTTP: Rate Limiting","description":"priority: 2","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-05T21:02:17.865169-05:00","updated_at":"2026-01-06T00:43:39.507538-05:00","closed_at":"2026-01-06T00:43:39.507538-05:00","close_reason":"Added 13 comprehensive rate limiting tests covering token bucket, burst, refill, Redis backend, per-identity keying, and edge cases. All tests pass."}
{"id":"mcp_agent_mail-9z6","title":"HTTP: Server and Transport","description":"priority: 2","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-05T21:02:17.86218-05:00","updated_at":"2026-01-06T00:27:46.401925-05:00","closed_at":"2026-01-06T00:27:46.401925-05:00","close_reason":"19 HTTP server/transport tests implemented and passing: config, health, SSE, tool calls, resources, CORS, error handling"}
{"id":"mcp_agent_mail-9zj","title":"Tune connection pool size based on workload","description":"# Tune Connection Pool Size\n\n## Problem Statement\nDefault connection pool settings may not be optimal for expected concurrency\nlevels, leading to either connection exhaustion or wasted resources.\n\n## Code Location\n`src/mcp_agent_mail/db.py` - async_sessionmaker and engine creation\n\n## Current Settings (Likely Defaults)\n```python\nengine = create_async_engine(\n    database_url,\n    # SQLAlchemy defaults:\n    # pool_size=5\n    # max_overflow=10\n    # pool_timeout=30\n    # pool_recycle=-1\n)\n```\n\n## Impact Analysis\n- **pool_size too small**: Connection exhaustion, timeouts under load\n- **pool_size too large**: Wasted memory, file descriptors\n- **max_overflow too small**: Can't handle burst traffic\n- **pool_timeout too short**: Spurious timeout errors\n\n## Recommended Settings\n```python\n# Calculate based on expected concurrency\n# Rule of thumb: pool_size = max_concurrent_requests / 2\n\nengine = create_async_engine(\n    database_url,\n    pool_size=10,           # Base connections always open\n    max_overflow=20,        # Additional connections for bursts\n    pool_timeout=30,        # Wait time before timeout\n    pool_recycle=3600,      # Recycle connections hourly\n    pool_pre_ping=True,     # Verify connection before use\n    echo=False,             # Disable SQL logging (set True for debug)\n)\n```\n\n## Configuration Approach\nMake pool settings configurable:\n\n```python\nfrom pydantic import BaseSettings\n\nclass DatabaseSettings(BaseSettings):\n    pool_size: int = 10\n    max_overflow: int = 20\n    pool_timeout: int = 30\n    pool_recycle: int = 3600\n    \n    class Config:\n        env_prefix = \"DB_\"\n\n\nsettings = DatabaseSettings()\n\nengine = create_async_engine(\n    database_url,\n    pool_size=settings.pool_size,\n    max_overflow=settings.max_overflow,\n    pool_timeout=settings.pool_timeout,\n    pool_recycle=settings.pool_recycle,\n)\n```\n\n## Monitoring\nAdd connection pool metrics:\n\n```python\nfrom sqlalchemy import event\n\n@event.listens_for(engine.sync_engine, \"checkout\")\ndef on_checkout(dbapi_conn, connection_record, connection_proxy):\n    pool = engine.sync_engine.pool\n    logger.debug(\n        f\"Pool: size={pool.size()}, \"\n        f\"checked_out={pool.checkedout()}, \"\n        f\"overflow={pool.overflow()}\"\n    )\n```\n\n## Testing Strategy\n1. Load test with default settings, observe pool behavior\n2. Load test with tuned settings, compare\n3. Test connection exhaustion scenario\n4. Test burst traffic handling\n\n## Pool Size Guidelines\n| Concurrent Users | pool_size | max_overflow |\n|------------------|-----------|--------------|\n| 1-10             | 5         | 10           |\n| 10-50            | 10        | 20           |\n| 50-100           | 20        | 40           |\n| 100+             | 50        | 100          |\n\n## Dependencies\nNone - configuration change","status":"open","priority":3,"issue_type":"task","created_at":"2026-01-12T01:12:24.745350248-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:12:24.745350248-05:00"}
{"id":"mcp_agent_mail-ada","title":"Unit Tests: app.py - Agent Operations","description":"priority: 2","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.469373-05:00","updated_at":"2026-01-05T21:00:48.350222-05:00","deleted_at":"2026-01-05T21:00:48.350222-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-aea","title":"Errors: Database Failures","description":"priority: 2","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-05T21:02:17.851404-05:00","updated_at":"2026-01-06T00:21:22.479073-05:00","closed_at":"2026-01-06T00:21:22.479073-05:00","close_reason":"Created tests/test_database_failures.py with 20 tests covering database auto-creation, retry_on_db_lock decorator, transaction rollback, session cleanup, and SQLite configuration (WAL mode, busy_timeout, synchronous mode)","dependencies":[{"issue_id":"mcp_agent_mail-aea","depends_on_id":"mcp_agent_mail-mm2","type":"blocks","created_at":"2026-01-05T21:02:54.533343-05:00","created_by":"jemanuel"}]}
{"id":"mcp_agent_mail-aew","title":"Core: File Reservation Lifecycle","description":"priority: 1","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-05T21:02:17.836703-05:00","updated_at":"2026-01-05T22:02:46.295934-05:00","closed_at":"2026-01-05T22:02:46.295934-05:00","close_reason":"17 tests (16 pass, 1 skip) covering: create exclusive/shared reservations, conflict detection (ex-ex, ex-sh, sh-sh), pattern overlap, TTL, manual release, renew, force release, multiple paths, git artifacts"}
{"id":"mcp_agent_mail-aid","title":"Fix N+1 pattern in _list_inbox() recipient fetching","description":"# Fix N+1 Query Pattern in _list_inbox()\n\n## Problem Statement\nSimilar to _list_outbox(), the inbox listing likely fetches recipients per-message,\ncausing N+1 query behavior.\n\n## Code Location\n`src/mcp_agent_mail/app.py` - Search for \"_list_inbox\" or \"fetch_inbox\"\n\n## Investigation Required\n1. Find the inbox listing implementation\n2. Verify it has the same N+1 pattern as outbox\n3. Apply the same fix pattern\n\n## Expected Fix Pattern\nSame as _list_outbox:\n1. Collect all message IDs\n2. Single query for all recipients: `WHERE message_id IN (...)`\n3. Group results by message_id using defaultdict\n4. Use pre-fetched data in the message loop\n\n## Testing Strategy\nSame as _list_outbox tests but for inbox endpoint.\n\n## Dependencies\n- Should be done alongside or after _list_outbox fix to ensure consistency\n\n## Note\nThis task exists because the analysis identified N+1 patterns in outbox listing.\nInbox almost certainly has the same pattern since the code is likely similar.\nVerify during implementation.","status":"open","priority":0,"issue_type":"task","created_at":"2026-01-12T01:07:10.291628635-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:07:10.291628635-05:00","dependencies":[{"issue_id":"mcp_agent_mail-aid","depends_on_id":"mcp_agent_mail-jxj","type":"blocks","created_at":"2026-01-12T01:13:19.430013059-05:00","created_by":"ubuntu"}]}
{"id":"mcp_agent_mail-aop","title":"Milestones","description":"---","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T21:02:17.88018-05:00","updated_at":"2026-01-05T21:02:38.054787-05:00","deleted_at":"2026-01-05T21:02:38.054787-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-aqs","title":"E2E: Multi-Agent Development Workflow","description":"priority: 4","status":"closed","priority":4,"issue_type":"task","created_at":"2026-01-05T21:02:17.878946-05:00","updated_at":"2026-01-06T02:13:22.627246-05:00","closed_at":"2026-01-06T02:13:22.627246-05:00","close_reason":"9 E2E tests implemented and passing (test_e2e_multi_agent_workflow.py)","dependencies":[{"issue_id":"mcp_agent_mail-aqs","depends_on_id":"mcp_agent_mail-n6z","type":"blocks","created_at":"2026-01-05T21:02:56.043931-05:00","created_by":"jemanuel"},{"issue_id":"mcp_agent_mail-aqs","depends_on_id":"mcp_agent_mail-irp","type":"blocks","created_at":"2026-01-05T21:02:56.345936-05:00","created_by":"jemanuel"}]}
{"id":"mcp_agent_mail-az3","title":"Implement read-write lock for schema operations","description":"# Implement Read-Write Lock for Schema Operations\n\n## Problem Statement\nSchema operations use a single coarse-grained lock, preventing concurrent\nreads even when no writes are happening.\n\n## Code Location\n`src/mcp_agent_mail/db.py` - Lines approximately 235-251\n\n## Current Code (THE PROBLEM)\n```python\n# Single lock for ALL schema operations\n_schema_lock = asyncio.Lock()\n\nasync def ensure_schema():\n    async with _schema_lock:  # Blocks ALL concurrent schema ops\n        # Check if schema exists\n        # Create tables if needed\n        pass\n\nasync def get_schema_version():\n    async with _schema_lock:  # Even reads block each other!\n        # Just reading version\n        pass\n```\n\n## Impact Analysis\n- Multiple agents checking schema version block each other\n- Read operations don't need exclusive access\n- Write operations (schema creation/migration) are rare\n- Current lock is write-lock for all operations\n\n## Required Fix\nImplement a read-write lock:\n\n```python\nimport asyncio\nfrom contextlib import asynccontextmanager\n\nclass AsyncRWLock:\n    \"\"\"Async read-write lock allowing concurrent reads, exclusive writes.\"\"\"\n    \n    def __init__(self):\n        self._read_count = 0\n        self._read_lock = asyncio.Lock()\n        self._write_lock = asyncio.Lock()\n    \n    @asynccontextmanager\n    async def read_lock(self):\n        \"\"\"Acquire read lock - multiple readers allowed.\"\"\"\n        async with self._read_lock:\n            self._read_count += 1\n            if self._read_count == 1:\n                await self._write_lock.acquire()\n        try:\n            yield\n        finally:\n            async with self._read_lock:\n                self._read_count -= 1\n                if self._read_count == 0:\n                    self._write_lock.release()\n    \n    @asynccontextmanager\n    async def write_lock(self):\n        \"\"\"Acquire write lock - exclusive access.\"\"\"\n        await self._write_lock.acquire()\n        try:\n            yield\n        finally:\n            self._write_lock.release()\n\n\n# Usage\n_schema_rwlock = AsyncRWLock()\n\nasync def ensure_schema():\n    async with _schema_rwlock.write_lock():  # Exclusive\n        # Schema modification\n        pass\n\nasync def get_schema_version():\n    async with _schema_rwlock.read_lock():  # Shared\n        # Just reading\n        pass\n```\n\n## RWLock Properties\n- Multiple readers can hold read_lock simultaneously\n- Writer waits for all readers to release\n- Readers wait for writer to release\n- Prevents writer starvation (new readers don't queue-jump waiting writer)\n\n## Alternative: Use Existing Library\n```python\nfrom aiorwlock import RWLock  # pip install aiorwlock\n\n_schema_lock = RWLock()\n\nasync def read_schema():\n    async with _schema_lock.reader:\n        ...\n\nasync def write_schema():\n    async with _schema_lock.writer:\n        ...\n```\n\n## Testing Strategy\n1. Multiple concurrent reads succeed simultaneously\n2. Write blocks new reads\n3. Reads block writes until complete\n4. No deadlocks under stress\n\n## Verification\n```python\nimport asyncio\nimport time\n\nrwlock = AsyncRWLock()\nread_times = []\n\nasync def reader(id: int):\n    start = time.perf_counter()\n    async with rwlock.read_lock():\n        await asyncio.sleep(0.1)  # Simulate read\n    read_times.append(time.perf_counter() - start)\n\n# Run 10 concurrent readers\nasyncio.run(asyncio.gather(*[reader(i) for i in range(10)]))\n\n# With RWLock: All readers finish in ~0.1s (parallel)\n# With plain Lock: Takes ~1.0s (serial)\ntotal_time = max(read_times)\nassert total_time \u003c 0.2, f\"Reads should be parallel: {total_time:.3f}s\"\n```\n\n## Dependencies\nNone - self-contained implementation","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-12T01:10:53.813736679-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:10:53.813736679-05:00"}
{"id":"mcp_agent_mail-b11","title":"Fix process lock dict memory leak - add cleanup","description":"# Fix Process Lock Dictionary Memory Leak\n\n## Problem Statement\nThe process lock dictionary grows unboundedly as new lock keys are added\nbut never removed, even after the operations complete.\n\n## Code Location\nSearch for lock dictionary usage in storage.py or db.py.\n\n## Current Code (THE PROBLEM)\n```python\n_process_locks: dict[str, asyncio.Lock] = {}\n\nasync def get_lock(key: str) -\u003e asyncio.Lock:\n    if key not in _process_locks:\n        _process_locks[key] = asyncio.Lock()\n    return _process_locks[key]\n\n# Locks are never removed!\n# After 10,000 unique keys, dict has 10,000 entries\n```\n\n## Impact Analysis\n- Each Lock object is ~100 bytes\n- 10,000 unique lock keys = ~1MB leaked\n- Over long-running server lifetime, can grow significantly\n- Also pollutes dict lookup performance\n\n## Required Fix Options\n\n### Option 1: WeakValueDictionary (Automatic Cleanup)\n```python\nfrom weakref import WeakValueDictionary\n\n_process_locks: WeakValueDictionary[str, asyncio.Lock] = WeakValueDictionary()\n\n# Locks automatically removed when no longer referenced\n```\n\n**Caveat**: Lock might be GC'd between get and use. Need careful handling.\n\n### Option 2: LRU Dict with Expiry\n```python\nfrom collections import OrderedDict\nimport time\n\nclass ExpiringLockDict:\n    def __init__(self, max_age: float = 3600, max_size: int = 10000):\n        self._locks: OrderedDict[str, tuple[asyncio.Lock, float]] = OrderedDict()\n        self._max_age = max_age\n        self._max_size = max_size\n    \n    def get(self, key: str) -\u003e asyncio.Lock:\n        self._cleanup()\n        if key in self._locks:\n            lock, _ = self._locks[key]\n            self._locks.move_to_end(key)\n            self._locks[key] = (lock, time.monotonic())\n            return lock\n        lock = asyncio.Lock()\n        self._locks[key] = (lock, time.monotonic())\n        return lock\n    \n    def _cleanup(self):\n        now = time.monotonic()\n        # Remove expired\n        while self._locks:\n            key, (lock, ts) = next(iter(self._locks.items()))\n            if now - ts \u003e self._max_age and not lock.locked():\n                self._locks.pop(key)\n            else:\n                break\n        # Remove oldest if over size\n        while len(self._locks) \u003e self._max_size:\n            key, (lock, _) = self._locks.popitem(last=False)\n            if lock.locked():\n                # Don't remove locked locks, put back\n                self._locks[key] = (lock, time.monotonic())\n                break\n```\n\n### Option 3: Explicit Release\n```python\n@asynccontextmanager\nasync def acquire_lock(key: str):\n    lock = _process_locks.setdefault(key, asyncio.Lock())\n    try:\n        await lock.acquire()\n        yield lock\n    finally:\n        lock.release()\n        # Clean up if no waiters\n        if key in _process_locks and not lock.locked():\n            del _process_locks[key]\n```\n\n## Recommended Approach\nOption 2 (LRU with expiry) is safest:\n- Bounded size prevents unbounded growth\n- Expiry handles inactive keys\n- Doesn't remove actively-used locks\n\n## Testing Strategy\n1. Create 10,000 unique lock keys\n2. Verify dict size stays bounded\n3. Verify old unused keys are cleaned up\n4. Verify active locks are not removed\n5. Memory profile over extended run\n\n## Verification\n```python\nlock_dict = ExpiringLockDict(max_age=1.0, max_size=100)\n\n# Create many locks\nfor i in range(1000):\n    lock_dict.get(f\"key-{i}\")\n\n# Size should be bounded\nassert len(lock_dict._locks) \u003c= 100\n\n# Wait for expiry\nawait asyncio.sleep(2.0)\nlock_dict.get(\"trigger-cleanup\")\n\n# Old locks should be cleaned\nassert len(lock_dict._locks) \u003c 50\n```\n\n## Dependencies\nNone - self-contained fix","status":"open","priority":3,"issue_type":"task","created_at":"2026-01-12T01:12:24.227385301-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:12:24.227385301-05:00"}
{"id":"mcp_agent_mail-b41","title":"Unit Tests: db.py - Migrations","description":"priority: 3","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.468601-05:00","updated_at":"2026-01-05T21:00:48.597636-05:00","deleted_at":"2026-01-05T21:00:48.597636-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-bko","title":"Unit Tests: http.py - Server Setup","description":"priority: 2","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.475868-05:00","updated_at":"2026-01-05T21:00:47.240944-05:00","deleted_at":"2026-01-05T21:00:47.240944-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-c2x","title":"Errors: Git Archive Failures","description":"priority: 2","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-05T21:02:17.85316-05:00","updated_at":"2026-01-06T00:36:13.482789-05:00","closed_at":"2026-01-06T00:36:13.482789-05:00","close_reason":"Added 21 comprehensive tests for Git archive failure handling covering auto-creation, repo initialization, concurrent writes, lock healing, file path sanitization, and large attachments. All tests pass.","dependencies":[{"issue_id":"mcp_agent_mail-c2x","depends_on_id":"mcp_agent_mail-mm2","type":"blocks","created_at":"2026-01-05T21:02:54.631277-05:00","created_by":"jemanuel"}]}
{"id":"mcp_agent_mail-ctq","title":"Milestone: Production Ready","description":"priority: 3","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-05T21:02:17.882507-05:00","updated_at":"2026-01-06T02:10:13.146794-05:00","closed_at":"2026-01-06T02:10:13.146794-05:00","close_reason":"All dependencies satisfied: Full Integration Coverage (74 tests), Concurrency (13 tests), Security Input Sanitization (28 tests). Production ready milestone complete.","dependencies":[{"issue_id":"mcp_agent_mail-ctq","depends_on_id":"mcp_agent_mail-9bz","type":"blocks","created_at":"2026-01-05T21:02:57.578989-05:00","created_by":"jemanuel"},{"issue_id":"mcp_agent_mail-ctq","depends_on_id":"mcp_agent_mail-yh8","type":"blocks","created_at":"2026-01-05T21:02:57.684143-05:00","created_by":"jemanuel"},{"issue_id":"mcp_agent_mail-ctq","depends_on_id":"mcp_agent_mail-e4m","type":"blocks","created_at":"2026-01-05T21:02:57.801425-05:00","created_by":"jemanuel"}]}
{"id":"mcp_agent_mail-cyw","title":"Fix N+1 query pattern in _deliver_message() agent lookups","description":"# Fix N+1 Query Pattern in _deliver_message()\n\n## Problem Statement\nWhen sending a message to multiple recipients, `_deliver_message()` looks up each agent\nindividually, resulting in N database queries for N recipients.\n\n## Code Location\n`src/mcp_agent_mail/app.py` - Lines approximately 3409-3411\n\n## Current Code (THE PROBLEM)\n```python\n# This issues 3 SEPARATE database queries per recipient list!\nto_agents = [await _get_agent(project, name) for name in to_names]\ncc_agents = [await _get_agent(project, name) for name in cc_names]\nbcc_agents = [await _get_agent(project, name) for name in bcc_names]\n```\n\n## Impact Analysis\n- Message to 5 TO recipients + 2 CC + 1 BCC = 8 SELECT queries\n- Each query has connection overhead, query parsing, result serialization\n- Latency: ~5-10ms per query × 8 = 40-80ms of unnecessary latency\n- Under load: Connection pool exhaustion, query queue buildup\n\n## Required Fix\nReplace with single batched lookup:\n\n```python\n# Combine all names and fetch in ONE query\nall_recipient_names = list(set(to_names + cc_names + bcc_names))\nagents_map = await _get_agents_batch(project, all_recipient_names)\n\n# Now partition by role using O(1) dict lookups\nto_agents = [agents_map.get(name) for name in to_names if name in agents_map]\ncc_agents = [agents_map.get(name) for name in cc_names if name in agents_map]\nbcc_agents = [agents_map.get(name) for name in bcc_names if name in agents_map]\n\n# Validate all recipients were found\nmissing = set(to_names + cc_names + bcc_names) - set(agents_map.keys())\nif missing:\n    raise ValueError(f\"Unknown recipients: {missing}\")\n```\n\n## Error Handling Consideration\nThe current code may silently ignore non-existent recipients (returns None).\nNeed to verify existing behavior and preserve it exactly:\n- Does the current code raise an error for unknown recipients?\n- If so, the fix must also raise\n- If not, the fix must also silently skip\n\nRead the full function to understand error handling before implementing.\n\n## Testing Strategy\n1. Send message to single recipient - verify 1 query\n2. Send to 5 TO + 3 CC + 2 BCC - verify still 1 query\n3. Send to non-existent recipient - verify same error as before\n4. Send with duplicate names in to/cc - verify deduplication works\n\n## Verification\nUse SQLAlchemy echo=True or query logging to count queries before/after.\nExpected: Query count drops from N to 1.\n\n## Dependencies\n- Requires: _get_agents_batch function (mcp_agent_mail-XXX)","status":"open","priority":0,"issue_type":"task","created_at":"2026-01-12T01:07:08.40582703-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:07:08.40582703-05:00","dependencies":[{"issue_id":"mcp_agent_mail-cyw","depends_on_id":"mcp_agent_mail-dwu","type":"blocks","created_at":"2026-01-12T01:12:50.014777861-05:00","created_by":"ubuntu"}]}
{"id":"mcp_agent_mail-dcy","title":"Fix N+1 in summarize_thread() - batch message and recipient fetching","description":"# Fix N+1 in summarize_thread()\n\n## Problem Statement\nThread summarization likely fetches messages and their recipients in a loop,\ncausing N+1 query patterns similar to inbox/outbox listing.\n\n## Code Location\n`src/mcp_agent_mail/app.py` - Search for \"summarize_thread\" function\n\n## Investigation Required\n1. Find the summarize_thread implementation\n2. Trace the data access patterns\n3. Identify N+1 loops (per-message queries for participants, recipients, etc.)\n\n## Expected Issues\nBased on analysis, summarize_thread likely:\n- Fetches all messages in a thread (1 query - OK)\n- For each message, fetches sender info (N queries - BAD)\n- For each message, fetches recipients (N queries - BAD)\n- For each message, fetches read/ack status (N queries - BAD)\n\n## Fix Pattern\nSame as other N+1 fixes:\n1. Collect all message IDs from the thread\n2. Batch fetch all related data:\n   - All senders: `WHERE id IN (sender_ids)`\n   - All recipients: `WHERE message_id IN (message_ids)`\n   - All read statuses: `WHERE message_id IN (message_ids)`\n3. Build lookup dicts from results\n4. Assemble thread summary using lookups\n\n## Performance Impact\nA thread with 50 messages currently issues 50-150 queries.\nAfter fix: 4-5 queries total regardless of thread size.\n\n## Testing Strategy\n1. Create thread with 1 message, verify query count\n2. Create thread with 20 messages, verify still ~5 queries\n3. Verify summary output is identical before/after\n\n## Dependencies\n- Requires _get_agents_batch for sender lookups\n- Similar pattern to inbox/outbox fixes","status":"open","priority":0,"issue_type":"task","created_at":"2026-01-12T01:07:58.023043838-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:07:58.023043838-05:00","dependencies":[{"issue_id":"mcp_agent_mail-dcy","depends_on_id":"mcp_agent_mail-dwu","type":"blocks","created_at":"2026-01-12T01:12:50.037727325-05:00","created_by":"ubuntu"}]}
{"id":"mcp_agent_mail-dvk","title":"Unit Tests: app.py - Project Operations","description":"priority: 2","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.469149-05:00","updated_at":"2026-01-05T21:00:48.431232-05:00","deleted_at":"2026-01-05T21:00:48.431232-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-dwu","title":"Create batched agent lookup function (_get_agents_batch)","description":"# Create Batched Agent Lookup Function\n\n## Problem Statement\nThe current `_get_agent()` function fetches a single agent by name, requiring N separate\ndatabase queries when looking up N agents. This is a classic N+1 pattern.\n\n## Current Code Location\n`src/mcp_agent_mail/app.py` - The `_get_agent()` function (search for \"async def _get_agent\")\n\n## Current Implementation Pattern\n```python\nasync def _get_agent(project: Project, name: str) -\u003e Agent | None:\n    async with _async_session() as session:\n        stmt = select(Agent).where(Agent.project_id == project.id, Agent.name == name)\n        result = await session.execute(stmt)\n        return result.scalar_one_or_none()\n```\n\n## Required Implementation\nCreate a new function `_get_agents_batch()` that accepts a list of names and returns all\nmatching agents in a single query:\n\n```python\nasync def _get_agents_batch(project: Project, names: list[str]) -\u003e dict[str, Agent]:\n    \"\"\"Batch fetch agents by name - single query instead of N.\n    \n    Args:\n        project: The project to search within\n        names: List of agent names to look up\n        \n    Returns:\n        Dict mapping name -\u003e Agent for found agents. Missing names are not in dict.\n        \n    Performance:\n        O(1) database query regardless of len(names)\n        Previous: O(n) queries where n = len(names)\n    \"\"\"\n    if not names:\n        return {}\n    \n    # Deduplicate names to avoid redundant work\n    unique_names = list(set(names))\n    \n    async with _async_session() as session:\n        stmt = select(Agent).where(\n            Agent.project_id == project.id,\n            Agent.name.in_(unique_names)\n        )\n        result = await session.execute(stmt)\n        agents = result.scalars().all()\n        return {agent.name: agent for agent in agents}\n```\n\n## Why This Design\n1. **Returns dict** - O(1) lookup by name for callers\n2. **Deduplicates** - Handles case where same name appears in to/cc/bcc\n3. **Handles missing** - Missing names simply aren't in the dict (callers can check)\n4. **Single session** - Reuses connection from pool efficiently\n\n## Testing Requirements\n1. Unit test: Empty names list returns empty dict\n2. Unit test: Single name returns correct agent\n3. Unit test: Multiple names returns all agents\n4. Unit test: Duplicate names handled correctly\n5. Unit test: Non-existent names not in result dict\n6. Integration test: Verify query count is 1 regardless of input size\n\n## Verification\nAfter implementation, run with SQL logging enabled and verify only 1 SELECT is issued\nregardless of how many names are passed in.\n\n## Dependencies\nNone - this is a new utility function that will be used by subsequent tasks.","status":"open","priority":0,"issue_type":"task","created_at":"2026-01-12T01:07:07.828287763-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:07:07.828287763-05:00","dependencies":[{"issue_id":"mcp_agent_mail-dwu","depends_on_id":"mcp_agent_mail-k92","type":"blocks","created_at":"2026-01-12T01:12:50.081521085-05:00","created_by":"ubuntu"}]}
{"id":"mcp_agent_mail-e4m","title":"Concurrency: Multiple Agents","description":"priority: 3","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-05T21:02:17.876769-05:00","updated_at":"2026-01-06T02:19:46.303396-05:00","closed_at":"2026-01-06T02:19:46.303396-05:00","close_reason":"13 concurrency tests pass: agent registration, message sending, file reservation conflicts, inbox fetching, archive writes, deadlock prevention, race conditions. Tests made resilient to transient async failures.","dependencies":[{"issue_id":"mcp_agent_mail-e4m","depends_on_id":"mcp_agent_mail-uvf","type":"blocks","created_at":"2026-01-05T21:02:55.413216-05:00","created_by":"jemanuel"},{"issue_id":"mcp_agent_mail-e4m","depends_on_id":"mcp_agent_mail-aew","type":"blocks","created_at":"2026-01-05T21:02:55.576713-05:00","created_by":"jemanuel"}]}
{"id":"mcp_agent_mail-e9z","title":"P4 - E2E Scenario Tests","description":"Complete end-to-end scenarios.","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T21:02:17.878396-05:00","updated_at":"2026-01-05T21:02:37.963853-05:00","deleted_at":"2026-01-05T21:02:37.963853-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-enu","title":"CLI: Archive Commands","description":"priority: 2","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-05T21:02:17.859253-05:00","updated_at":"2026-01-05T23:39:38.961247-05:00","closed_at":"2026-01-05T23:39:38.961247-05:00","close_reason":"24 comprehensive CLI archive command tests added and passing","dependencies":[{"issue_id":"mcp_agent_mail-enu","depends_on_id":"mcp_agent_mail-uvf","type":"blocks","created_at":"2026-01-05T21:02:54.9184-05:00","created_by":"jemanuel"}]}
{"id":"mcp_agent_mail-f26","title":"Unit Tests: share.py - Archive Save","description":"priority: 2","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.483229-05:00","updated_at":"2026-01-05T21:00:46.787388-05:00","deleted_at":"2026-01-05T21:00:46.787388-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-fuj","title":"Integration Tests: File Reservation Conflicts","description":"priority: 1","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.484292-05:00","updated_at":"2026-01-05T21:00:46.383572-05:00","deleted_at":"2026-01-05T21:00:46.383572-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-g77","title":"Fix LRU cache O(n) operations - replace list with OrderedDict","description":"# Fix LRU Cache O(n) → O(1) Operations\n\n## Problem Statement\nThe SimpleLRUCache class uses a list to track access order, requiring O(n) \nlist.remove() and list operations on every cache hit. This makes cache access\nlinear time instead of constant time.\n\n## Code Location\n`src/mcp_agent_mail/storage.py` - Line 87 and surrounding SimpleLRUCache class\n\n## Current Code (THE PROBLEM)\n```python\nclass SimpleLRUCache:\n    def __init__(self, maxsize: int = 128):\n        self._cache: dict[str, Any] = {}\n        self._order: list[str] = []  # THIS IS THE PROBLEM\n        self._maxsize = maxsize\n    \n    def get(self, key: str) -\u003e Any | None:\n        if key not in self._cache:\n            return None\n        self._order.remove(key)  # O(n) - scans entire list!\n        self._order.append(key)  # O(1) amortized\n        return self._cache[key]\n    \n    def set(self, key: str, value: Any) -\u003e None:\n        if key in self._cache:\n            self._order.remove(key)  # O(n) again!\n        self._cache[key] = value\n        self._order.append(key)\n        while len(self._order) \u003e self._maxsize:\n            oldest = self._order.pop(0)  # O(n) - shifts all elements!\n            del self._cache[oldest]\n```\n\n## Impact Analysis\n- Every cache HIT is O(n) where n = cache size\n- With maxsize=128, every hit scans up to 128 elements\n- Under load with frequent cache access, this dominates runtime\n- Cache meant to IMPROVE performance is actually hurting it!\n\n## Required Fix\nReplace list-based tracking with collections.OrderedDict which provides O(1)\nmove_to_end() and popitem() operations:\n\n```python\nfrom collections import OrderedDict\nfrom typing import Any\n\nclass SimpleLRUCache:\n    \"\"\"LRU cache with O(1) access, update, and eviction.\"\"\"\n    \n    def __init__(self, maxsize: int = 128):\n        self._cache: OrderedDict[str, Any] = OrderedDict()\n        self._maxsize = maxsize\n    \n    def get(self, key: str) -\u003e Any | None:\n        \"\"\"Get value and mark as recently used. O(1) operation.\"\"\"\n        if key not in self._cache:\n            return None\n        # O(1) - OrderedDict maintains doubly-linked list internally\n        self._cache.move_to_end(key)\n        return self._cache[key]\n    \n    def set(self, key: str, value: Any) -\u003e None:\n        \"\"\"Set value and mark as recently used. O(1) operation.\"\"\"\n        if key in self._cache:\n            self._cache.move_to_end(key)\n        self._cache[key] = value\n        # Evict oldest if over capacity - O(1) per eviction\n        while len(self._cache) \u003e self._maxsize:\n            self._cache.popitem(last=False)  # Remove oldest (first)\n    \n    def __contains__(self, key: str) -\u003e bool:\n        \"\"\"Check if key exists without updating LRU order.\"\"\"\n        return key in self._cache\n    \n    def clear(self) -\u003e None:\n        \"\"\"Clear all cached entries.\"\"\"\n        self._cache.clear()\n```\n\n## Why OrderedDict\n- Python's OrderedDict uses a doubly-linked list internally\n- move_to_end() is O(1) - just pointer manipulation\n- popitem(last=False) is O(1) - removes head of list\n- No manual bookkeeping needed\n\n## Alternative: functools.lru_cache\nCould also use @lru_cache decorator, but:\n- Requires hashable arguments\n- Less control over cache behavior\n- SimpleLRUCache may have specific semantics we need to preserve\n\n## Testing Strategy\n1. Unit test: get() returns correct value\n2. Unit test: get() updates access order (verify with iteration)\n3. Unit test: set() evicts oldest when at capacity\n4. Unit test: maxsize=1 works correctly\n5. Performance test: 10000 operations completes in \u003c100ms\n\n## Verification\n```python\nimport timeit\n\ncache = SimpleLRUCache(maxsize=1000)\n# Populate\nfor i in range(1000):\n    cache.set(f\"key{i}\", i)\n\n# Benchmark get operations\ntime = timeit.timeit(lambda: cache.get(\"key500\"), number=100000)\nprint(f\"100k gets: {time:.3f}s\")  # Should be \u003c 1s with O(1), \u003e\u003e 1s with O(n)\n```\n\n## Dependencies\nNone - self-contained fix","status":"open","priority":0,"issue_type":"task","created_at":"2026-01-12T01:07:57.207854635-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:07:57.207854635-05:00","dependencies":[{"issue_id":"mcp_agent_mail-g77","depends_on_id":"mcp_agent_mail-k92","type":"blocks","created_at":"2026-01-12T01:12:50.12277926-05:00","created_by":"ubuntu"}]}
{"id":"mcp_agent_mail-gt4","title":"Replace fetchall() with cursor iteration for large result sets","description":"# Replace fetchall() with Streaming for Large Results\n\n## Problem Statement\nCode uses fetchall() which loads entire result sets into memory, causing\nmemory pressure for large queries.\n\n## Code Locations\nSearch for `.fetchall()` or `.all()` on large queries throughout codebase.\n\n## Current Pattern (THE PROBLEM)\n```python\nresult = await session.execute(select(Message).where(...))\nmessages = result.scalars().all()  # Loads ALL into memory\n\nfor msg in messages:  # Memory already allocated\n    process(msg)\n```\n\n## Impact Analysis\n- 10,000 messages × 10KB each = 100MB memory spike\n- GC pressure from large allocations\n- Potential OOM for very large result sets\n\n## Required Fix\nUse streaming/cursor iteration:\n\n```python\n# Option 1: stream_scalars for row-by-row processing\nasync with session.stream_scalars(select(Message).where(...)) as stream:\n    async for msg in stream:\n        process(msg)  # Memory for one row at a time\n\n# Option 2: yield_per for batched streaming\nresult = await session.execute(\n    select(Message).where(...).execution_options(yield_per=100)\n)\nfor msg in result.scalars():\n    process(msg)  # Fetches in batches of 100\n```\n\n## SQLModel/SQLAlchemy Async Streaming\n```python\nfrom sqlalchemy.ext.asyncio import AsyncSession\n\nasync def process_large_results(session: AsyncSession):\n    \"\"\"Process large result set with minimal memory.\"\"\"\n    stmt = select(Message).where(Message.project_id == project_id)\n    \n    # stream() returns async iterator\n    async with session.stream(stmt) as result:\n        async for row in result:\n            msg = row[0]  # Unpack from Row\n            await process_message(msg)\n```\n\n## When to Use Streaming\n- Unknown result size that could be large\n- Processing can be done row-by-row\n- Memory is constrained\n- Latency to first result matters\n\n## When fetchall() Is OK\n- Known small result sets (\u003c 1000 rows)\n- Need random access to results\n- Need to process results multiple times\n- Results needed all at once for aggregation\n\n## Implementation Notes\n- Streaming keeps database cursor open longer\n- May conflict with connection pool settings\n- Test with realistic data volumes\n\n## Testing Strategy\n1. Query returning 100,000 rows with fetchall() - measure memory\n2. Same query with streaming - measure memory\n3. Verify identical results\n4. Check for connection pool exhaustion\n\n## Verification\n```python\nimport tracemalloc\n\ntracemalloc.start()\n\n# fetchall approach\nresult = await session.execute(select(Message).limit(10000))\nmessages = result.scalars().all()\nfetchall_memory = tracemalloc.get_traced_memory()[1]\n\ntracemalloc.reset_peak()\n\n# streaming approach\nasync with session.stream_scalars(select(Message).limit(10000)) as stream:\n    async for msg in stream:\n        pass  # Just iterate\nstreaming_memory = tracemalloc.get_traced_memory()[1]\n\nprint(f\"fetchall: {fetchall_memory / 1e6:.1f}MB\")\nprint(f\"streaming: {streaming_memory / 1e6:.1f}MB\")\n```\n\n## Dependencies\nNone - can be done independently, file by file","status":"open","priority":3,"issue_type":"task","created_at":"2026-01-12T01:12:23.423590282-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:12:23.423590282-05:00"}
{"id":"mcp_agent_mail-h1m","title":"P1 - Core Functionality Tests","description":"These test the primary user-facing functionality.","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T21:02:17.834592-05:00","updated_at":"2026-01-05T21:02:37.107921-05:00","deleted_at":"2026-01-05T21:02:37.107921-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-h4o","title":"Add @lru_cache to PathSpec pattern compilation","description":"# Cache PathSpec Pattern Compilation\n\n## Problem Statement\nThe `_patterns_overlap()` function compiles PathSpec patterns on every call.\nPathSpec compilation involves regex compilation which is expensive. In\nreservation conflict detection loops, the same patterns are recompiled repeatedly.\n\n## Code Location\n`src/mcp_agent_mail/app.py` - Lines approximately 2938-2939\n\n## Current Code (THE PROBLEM)\n```python\ndef _patterns_overlap(a: str, b: str) -\u003e bool:\n    \"\"\"Check if two gitignore-style patterns could match overlapping paths.\"\"\"\n    # COMPILES REGEX EVERY SINGLE CALL!\n    a_spec = PathSpec.from_lines(\"gitwildmatch\", [a])\n    b_spec = PathSpec.from_lines(\"gitwildmatch\", [b])\n    \n    # ... overlap detection logic ...\n```\n\n## Impact Analysis\n- Reservation conflict check compares pattern A against N existing reservations\n- Each comparison compiles both patterns = 2 compilations\n- N reservations × 2 = 2N regex compilations per conflict check\n- With 100 active reservations, that's 200 regex compilations per check!\n\n## Required Fix\nAdd memoization to pattern compilation:\n\n```python\nfrom functools import lru_cache\nfrom pathspec import PathSpec\n\n@lru_cache(maxsize=512)\ndef _compile_pathspec(pattern: str) -\u003e PathSpec:\n    \"\"\"Compile a gitignore pattern to PathSpec with caching.\n    \n    Cache hit ratio should be very high since:\n    - Same reservation patterns are checked repeatedly\n    - Agents typically use a small set of patterns\n    \n    maxsize=512 balances memory usage with hit rate.\n    \"\"\"\n    return PathSpec.from_lines(\"gitwildmatch\", [pattern])\n\n\ndef _patterns_overlap(a: str, b: str) -\u003e bool:\n    \"\"\"Check if two gitignore-style patterns could match overlapping paths.\"\"\"\n    a_spec = _compile_pathspec(a)  # O(1) if cached\n    b_spec = _compile_pathspec(b)  # O(1) if cached\n    \n    # ... rest of function unchanged ...\n```\n\n## Why lru_cache\n- Standard library, no dependencies\n- Thread-safe in Python 3.2+\n- Automatic eviction when maxsize exceeded\n- Can inspect cache stats with `_compile_pathspec.cache_info()`\n\n## Cache Size Reasoning\n- maxsize=512 chosen because:\n  - Typical project has 10-50 distinct reservation patterns\n  - 512 handles 10 projects × 50 patterns with room to spare\n  - Each PathSpec is relatively small (pattern string + compiled regex)\n  - Memory overhead ~50KB for 512 entries\n\n## Testing Strategy\n1. Call _patterns_overlap with same patterns 1000 times\n2. Verify cache_info shows high hit rate\n3. Benchmark: should be 10-50x faster than uncached version\n\n## Verification\n```python\n# After implementation\n_compile_pathspec.cache_clear()  # Reset cache\nfor _ in range(1000):\n    _patterns_overlap(\"src/**/*.py\", \"src/api/*.py\")\n    \ninfo = _compile_pathspec.cache_info()\nprint(f\"Hits: {info.hits}, Misses: {info.misses}\")\n# Expected: ~2 misses, ~1998 hits\n```\n\n## Edge Cases\n- Empty pattern: Should be handled by PathSpec\n- Invalid pattern: Let PathSpec raise its normal error (don't cache errors)\n- Very long pattern: Still cached, but consider if this could be DoS vector\n\n## Dependencies\nNone - self-contained fix","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-12T01:09:21.988589258-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:09:21.988589258-05:00"}
{"id":"mcp_agent_mail-hqk","title":"MCP Resources: Read Access","description":"priority: 1","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-05T21:02:17.843304-05:00","updated_at":"2026-01-05T22:46:29.773477-05:00","closed_at":"2026-01-05T22:46:29.773477-05:00","close_reason":"Completed P1 MCP Resources Read Access tests - 18 tests covering project, agents, inbox, outbox, thread, and file_reservations resources","dependencies":[{"issue_id":"mcp_agent_mail-hqk","depends_on_id":"mcp_agent_mail-mm2","type":"blocks","created_at":"2026-01-05T21:02:54.329922-05:00","created_by":"jemanuel"}]}
{"id":"mcp_agent_mail-i0u","title":"E2E Test Script: Multi-Agent Workflow","description":"priority: 1","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.486948-05:00","updated_at":"2026-01-05T21:00:45.7694-05:00","deleted_at":"2026-01-05T21:00:45.7694-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-i3n","title":"Integration Tests: Guard Pre-commit","description":"priority: 2","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.485595-05:00","updated_at":"2026-01-05T21:00:46.147751-05:00","deleted_at":"2026-01-05T21:00:46.147751-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-i8h","title":"E2E Test Script: Disaster Recovery","description":"priority: 2","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.487445-05:00","updated_at":"2026-01-05T21:00:45.650175-05:00","deleted_at":"2026-01-05T21:00:45.650175-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-i9c","title":"Replace list membership check with set for O(1) lookup","description":"# Replace List Membership with Set for O(1) Lookup\n\n## Problem Statement\nCode checks `if item not in list` inside a loop, resulting in O(n²) total\ncomplexity when the list grows.\n\n## Code Location\n`src/mcp_agent_mail/share.py` - Line approximately 696\n\n## Current Code (THE PROBLEM)\n```python\nselected = []\nfor record in all_records:\n    # O(n) membership check on every iteration!\n    if found_record not in selected:\n        selected.append(found_record)\n```\n\n## Impact Analysis\n- First check: scan 0 items\n- Second check: scan 1 item\n- Third check: scan 2 items\n- ...\n- Nth check: scan N-1 items\n- Total: 0 + 1 + 2 + ... + (N-1) = N(N-1)/2 = O(n²)\n\nFor 1000 records, that's ~500,000 comparisons!\n\n## Required Fix\n```python\nselected_ids: set[int] = set()  # O(1) membership test\nselected_list: list[Record] = []  # Maintain order if needed\n\nfor record in all_records:\n    if record.id not in selected_ids:  # O(1)!\n        selected_ids.add(record.id)\n        selected_list.append(record)\n```\n\n## Why Use ID for Set\nRecords may not be hashable directly (SQLModel objects with relationships).\nUsing the ID (which is just an int) guarantees hashability.\n\n## Alternative: Use dict for Deduplication\n```python\n# If order doesn't matter and you just need unique records\nselected = {record.id: record for record in all_records}\nunique_records = list(selected.values())\n```\n\n## Finding Similar Patterns\nSearch the codebase for:\n```\nif .* not in .*:\n    .*.append(\n```\n\nThis pattern often indicates O(n²) list membership checks.\n\n## Testing Strategy\n1. Verify deduplication works correctly\n2. Verify order is preserved (if required)\n3. Benchmark with 1000, 10000 records\n4. Verify O(n) vs O(n²) scaling\n\n## Verification\n```python\nimport time\n\ndef old_way(n):\n    selected = []\n    for i in range(n):\n        if i not in selected:\n            selected.append(i)\n    return selected\n\ndef new_way(n):\n    selected_set = set()\n    selected_list = []\n    for i in range(n):\n        if i not in selected_set:\n            selected_set.add(i)\n            selected_list.append(i)\n    return selected_list\n\n# Time both\nfor n in [100, 1000, 10000]:\n    t1 = time.perf_counter()\n    old_way(n)\n    old_time = time.perf_counter() - t1\n    \n    t2 = time.perf_counter()\n    new_way(n)\n    new_time = time.perf_counter() - t2\n    \n    print(f\"n={n}: old={old_time:.4f}s, new={new_time:.4f}s, speedup={old_time/new_time:.1f}x\")\n```\n\n## Dependencies\nNone - self-contained fix","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-12T01:10:49.542961875-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:10:49.542961875-05:00"}
{"id":"mcp_agent_mail-ihs","title":"P3 - Concurrent Access Tests","description":"Test behavior under concurrent load.","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T21:02:17.87517-05:00","updated_at":"2026-01-05T21:02:37.760344-05:00","deleted_at":"2026-01-05T21:02:37.760344-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-ill","title":"Unit Tests: app.py - Contact Management","description":"priority: 2","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.470372-05:00","updated_at":"2026-01-05T21:00:48.065732-05:00","deleted_at":"2026-01-05T21:00:48.065732-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-ipa","title":"P2 - Error Handling Tests","description":"Test that errors are handled gracefully with clear messages.","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T21:02:17.846757-05:00","updated_at":"2026-01-05T21:02:37.297157-05:00","deleted_at":"2026-01-05T21:02:37.297157-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-irp","title":"Guards: Pre-commit Enforcement","description":"priority: 2","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-05T21:02:17.867804-05:00","updated_at":"2026-01-05T23:24:02.101521-05:00","closed_at":"2026-01-05T23:24:02.101521-05:00","close_reason":"14 comprehensive pre-commit enforcement tests added and passing","dependencies":[{"issue_id":"mcp_agent_mail-irp","depends_on_id":"mcp_agent_mail-aew","type":"blocks","created_at":"2026-01-05T21:02:55.010915-05:00","created_by":"jemanuel"}]}
{"id":"mcp_agent_mail-jto","title":"Bug: File handle exhaustion (EMFILE) under heavy load","description":"GitHub Issue #59 - After extended sessions, 'Too many open files' error occurs.\n\n**Root Cause Analysis:**\n- LRU repo cache (maxsize=8) helps but doesn't prevent EMFILE\n- Evicted repos tracked in _evicted list may not close promptly due to reference counts\n- Error message shows 'Freed 1 cached repos' indicating cache had minimal repos when error occurred\n\n**Investigation Needed:**\n1. Check if other file handles accumulate (locks, config files, etc.)\n2. Consider increasing LRU cache size or more aggressive cleanup\n3. Add file handle monitoring/metrics\n\n**Current Mitigation:**\n- clear_repo_cache() called on error (app.py:324)\n- Recovery works but user must retry operation\n\nReference: https://github.com/Dicklesworthstone/mcp_agent_mail/issues/59","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-05T21:17:09.114719-05:00","created_by":"jemanuel","updated_at":"2026-01-05T23:53:47.633305-05:00","closed_at":"2026-01-05T23:53:47.633305-05:00","close_reason":"Implemented LRU cache improvements: increased maxsize to 16, added opportunistic cleanup on get() calls, added warning logging, added monitoring stats. Added 19 tests."}
{"id":"mcp_agent_mail-jts","title":"P1 - MCP Protocol Tests","description":"Test all MCP tools and resources work correctly.","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T21:02:17.840562-05:00","updated_at":"2026-01-05T21:02:37.19954-05:00","deleted_at":"2026-01-05T21:02:37.19954-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-jxj","title":"Fix N+1 query pattern in _list_outbox() recipient fetching","description":"# Fix N+1 Query Pattern in _list_outbox()\n\n## Problem Statement\nWhen listing outbox messages, the code fetches recipients for each message in a loop,\nresulting in N additional queries for N messages.\n\n## Code Location\n`src/mcp_agent_mail/app.py` - Lines approximately 3024-3029\n\n## Current Code (THE PROBLEM)\n```python\nfor msg in message_rows:\n    # THIS RUNS ONCE PER MESSAGE - N+1!\n    recs = await session.execute(\n        select(MessageRecipient.recipient_name, MessageRecipient.kind)\n        .where(MessageRecipient.message_id == msg.id)\n    )\n    recipients = recs.all()\n    # ... build response\n```\n\n## Impact Analysis\n- Listing 20 messages = 20 additional SELECT queries\n- Plus 1 initial query for messages = 21 total queries\n- Each listing operation is dramatically slower than necessary\n- User-facing latency: 200-400ms instead of 20-40ms\n\n## Required Fix\nFetch all recipients in a single query using IN clause:\n\n```python\n# First, collect all message IDs\nmsg_ids = [msg.id for msg in message_rows]\n\n# Single query for ALL recipients across all messages\nif msg_ids:\n    recs_stmt = select(MessageRecipient).where(\n        MessageRecipient.message_id.in_(msg_ids)\n    )\n    all_recs = (await session.execute(recs_stmt)).scalars().all()\n    \n    # Group by message_id in Python - O(n) single pass\n    from collections import defaultdict\n    recs_by_msg: dict[int, list[MessageRecipient]] = defaultdict(list)\n    for rec in all_recs:\n        recs_by_msg[rec.message_id].append(rec)\nelse:\n    recs_by_msg = {}\n\n# Now iterate messages and use pre-fetched recipients\nfor msg in message_rows:\n    recipients = recs_by_msg.get(msg.id, [])\n    # ... build response using recipients\n```\n\n## Why defaultdict\n- Handles messages with no recipients gracefully\n- Avoids KeyError for edge cases\n- O(1) lookup per message\n\n## Alternative: JOIN Approach\nCould also rewrite the initial query to JOIN messages with recipients:\n```python\nstmt = (\n    select(Message, MessageRecipient)\n    .outerjoin(MessageRecipient)\n    .where(Message.sender_id == agent.id)\n)\n```\nHowever, this complicates result processing. The two-query approach is simpler and\nstill reduces N+1 to 2 queries total.\n\n## Testing Strategy\n1. List outbox with 0 messages - verify no errors\n2. List outbox with 1 message, 3 recipients - verify 2 queries total\n3. List outbox with 20 messages - verify still 2 queries total\n4. List outbox with message that has 0 recipients - verify handled\n\n## Verification\nEnable query logging. Before: 1 + N queries. After: 2 queries total.\n\n## Similar Patterns to Check\nAfter fixing _list_outbox, search for similar patterns:\n- _list_inbox (likely same issue)\n- fetch_inbox tool handler\n- Any other place that loops over messages and fetches related data","status":"open","priority":0,"issue_type":"task","created_at":"2026-01-12T01:07:09.321252784-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:07:09.321252784-05:00"}
{"id":"mcp_agent_mail-k92","title":"Create performance benchmarking suite","description":"# Create Performance Benchmarking Suite\n\n## Problem Statement\nBefore implementing optimizations, we need baseline performance metrics.\nAfter implementing, we need to verify improvements.\n\n## Objective\nCreate a reusable benchmarking suite that measures:\n- Query latency (p50, p95, p99)\n- Throughput (requests/second)\n- Memory usage (peak, average)\n- Query counts (to detect N+1)\n\n## Proposed Structure\n```\ntests/\n  benchmarks/\n    __init__.py\n    conftest.py           # Shared fixtures\n    bench_send_message.py\n    bench_fetch_inbox.py\n    bench_list_outbox.py\n    bench_search.py\n    bench_summarize.py\n    utils.py              # Timing, memory helpers\n```\n\n## Core Utilities\n```python\n# tests/benchmarks/utils.py\nimport time\nimport tracemalloc\nfrom dataclasses import dataclass\nfrom statistics import mean, quantiles\n\n@dataclass\nclass BenchmarkResult:\n    name: str\n    iterations: int\n    total_time: float\n    times: list[float]\n    peak_memory: int\n    query_count: int\n    \n    @property\n    def p50(self) -\u003e float:\n        return quantiles(self.times, n=100)[49]\n    \n    @property\n    def p95(self) -\u003e float:\n        return quantiles(self.times, n=100)[94]\n    \n    @property\n    def p99(self) -\u003e float:\n        return quantiles(self.times, n=100)[98]\n    \n    @property\n    def throughput(self) -\u003e float:\n        return self.iterations / self.total_time\n    \n    def __str__(self) -\u003e str:\n        return (\n            f\"{self.name}:\\n\"\n            f\"  Iterations: {self.iterations}\\n\"\n            f\"  Latency: p50={self.p50*1000:.2f}ms, p95={self.p95*1000:.2f}ms, p99={self.p99*1000:.2f}ms\\n\"\n            f\"  Throughput: {self.throughput:.1f} req/s\\n\"\n            f\"  Peak Memory: {self.peak_memory / 1e6:.1f}MB\\n\"\n            f\"  Queries/request: {self.query_count / self.iterations:.1f}\"\n        )\n\n\nclass QueryCounter:\n    \"\"\"Context manager to count database queries.\"\"\"\n    def __init__(self):\n        self.count = 0\n    \n    def __enter__(self):\n        # Hook into SQLAlchemy query events\n        ...\n        return self\n    \n    def __exit__(self, *args):\n        # Unhook\n        ...\n```\n\n## Example Benchmark\n```python\n# tests/benchmarks/bench_send_message.py\nimport pytest\nfrom .utils import BenchmarkResult, QueryCounter\n\n@pytest.mark.benchmark\nasync def test_send_message_performance(db_session, sample_project, sample_agents):\n    \"\"\"Benchmark send_message operation.\"\"\"\n    iterations = 100\n    times = []\n    \n    tracemalloc.start()\n    \n    with QueryCounter() as qc:\n        total_start = time.perf_counter()\n        \n        for i in range(iterations):\n            start = time.perf_counter()\n            await send_message(\n                project=sample_project,\n                sender=sample_agents[0],\n                to=[a.name for a in sample_agents[1:6]],  # 5 recipients\n                subject=f\"Test {i}\",\n                body=\"Benchmark test message\",\n            )\n            times.append(time.perf_counter() - start)\n        \n        total_time = time.perf_counter() - total_start\n    \n    peak_memory = tracemalloc.get_traced_memory()[1]\n    tracemalloc.stop()\n    \n    result = BenchmarkResult(\n        name=\"send_message (5 recipients)\",\n        iterations=iterations,\n        total_time=total_time,\n        times=times,\n        peak_memory=peak_memory,\n        query_count=qc.count,\n    )\n    \n    print(result)\n    \n    # Assertions for regression testing\n    assert result.p95 \u003c 0.100, \"p95 latency should be \u003c 100ms\"\n    assert result.query_count / iterations \u003c 10, \"Should use \u003c 10 queries per send\"\n```\n\n## Running Benchmarks\n```bash\n# Run all benchmarks\npytest tests/benchmarks/ -v --benchmark\n\n# Run specific benchmark\npytest tests/benchmarks/bench_send_message.py -v\n\n# Save results for comparison\npytest tests/benchmarks/ --benchmark-save=baseline\n\n# Compare against baseline\npytest tests/benchmarks/ --benchmark-compare=baseline\n```\n\n## Integration with CI\n```yaml\n# .github/workflows/benchmark.yml\non:\n  pull_request:\n    paths: ['src/**', 'tests/benchmarks/**']\n\njobs:\n  benchmark:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - run: uv sync\n      - run: pytest tests/benchmarks/ --benchmark-compare=main\n```\n\n## Dependencies\nShould be created early in Phase 1 to establish baselines before changes.","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-12T01:12:24.910676087-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:12:24.910676087-05:00"}
{"id":"mcp_agent_mail-kkp","title":"Regression: Session Context Management","description":"priority: 0","status":"closed","priority":0,"issue_type":"task","created_at":"2026-01-05T21:02:17.830137-05:00","updated_at":"2026-01-05T21:41:50.572476-05:00","closed_at":"2026-01-05T21:41:50.572476-05:00","close_reason":"All 10 session context regression tests implemented (9 pass, 1 skip)"}
{"id":"mcp_agent_mail-l69","title":"P2 - HTTP Transport Tests","description":"Test HTTP/SSE transport layer.","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T21:02:17.860107-05:00","updated_at":"2026-01-05T21:02:37.476429-05:00","deleted_at":"2026-01-05T21:02:37.476429-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-lps","title":"Unit Tests: config.py","description":"priority: 2","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.467634-05:00","updated_at":"2026-01-05T21:00:48.766376-05:00","deleted_at":"2026-01-05T21:00:48.766376-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-m55","title":"Unit Tests: app.py - File Reservations","description":"priority: 2","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.469857-05:00","updated_at":"2026-01-05T21:00:48.16821-05:00","deleted_at":"2026-01-05T21:00:48.16821-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-mik","title":"Unit Tests: guard.py - Pre-push","description":"priority: 2","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.482758-05:00","updated_at":"2026-01-05T21:00:46.89123-05:00","deleted_at":"2026-01-05T21:00:46.89123-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-mj0","title":"Errors: Invalid Inputs","description":"priority: 2","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-05T21:02:17.849989-05:00","updated_at":"2026-01-05T23:30:14.939838-05:00","closed_at":"2026-01-05T23:30:14.939838-05:00","close_reason":"Created 25 tests for P2 Errors: Invalid Inputs - all tests pass","dependencies":[{"issue_id":"mcp_agent_mail-mj0","depends_on_id":"mcp_agent_mail-mm2","type":"blocks","created_at":"2026-01-05T21:02:54.436249-05:00","created_by":"jemanuel"}]}
{"id":"mcp_agent_mail-mm2","title":"Core: Project and Agent Setup","description":"priority: 1","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-05T21:02:17.838873-05:00","updated_at":"2026-01-05T21:51:54.451595-05:00","closed_at":"2026-01-05T21:51:54.451595-05:00","close_reason":"All 14 project/agent setup tests implemented and passing"}
{"id":"mcp_agent_mail-mzo","title":"Phase 1: Critical N+1 Query Elimination and Index Optimization","description":"# Phase 1: Critical Priority Optimizations (10x+ Latency Reduction)\n\n## Scope\nThis phase targets the most impactful performance issues - N+1 query patterns and missing indexes\nthat cause order-of-magnitude performance degradation.\n\n## Why This Is Critical\nN+1 queries are the #1 cause of database performance issues. In MCP Agent Mail:\n- send_message with 5 recipients issues 5 separate SELECT queries instead of 1\n- Listing 20 inbox messages issues 20 additional queries for recipients\n- These compound: sending to 5 recipients who each have 20 messages = 100+ queries\n\n## Expected Impact\n- send_message: 5-10x latency reduction\n- fetch_inbox/list_outbox: 10-20x latency reduction  \n- Search queries: 3-5x improvement with proper indexes\n\n## Key Files\n- src/mcp_agent_mail/app.py - Lines 3409-3411, 3024-3029\n- src/mcp_agent_mail/db.py - Schema definitions and indexes\n- src/mcp_agent_mail/storage.py - Line 87 (LRU cache)\n\n## Verification Strategy\n1. Write baseline benchmark capturing query counts and latencies\n2. Implement fix\n3. Re-run benchmark, verify identical outputs with fewer queries\n4. Run full test suite to confirm no behavioral changes\n\n## Tasks\n1. Create batch agent lookup function (_get_agents_batch)\n2. Fix N+1 in _deliver_message() \n3. Fix N+1 in _list_outbox()\n4. Add composite indexes for common query patterns\n5. Fix LRU cache O(n) → O(1)","status":"open","priority":0,"issue_type":"feature","created_at":"2026-01-12T01:05:50.822840177-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:05:50.822840177-05:00","dependencies":[{"issue_id":"mcp_agent_mail-mzo","depends_on_id":"mcp_agent_mail-dwu","type":"blocks","created_at":"2026-01-12T01:13:00.951350401-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-mzo","depends_on_id":"mcp_agent_mail-cyw","type":"blocks","created_at":"2026-01-12T01:13:00.992335862-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-mzo","depends_on_id":"mcp_agent_mail-jxj","type":"blocks","created_at":"2026-01-12T01:13:01.016724045-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-mzo","depends_on_id":"mcp_agent_mail-aid","type":"blocks","created_at":"2026-01-12T01:13:01.038265436-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-mzo","depends_on_id":"mcp_agent_mail-4yy","type":"blocks","created_at":"2026-01-12T01:13:01.060561799-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-mzo","depends_on_id":"mcp_agent_mail-g77","type":"blocks","created_at":"2026-01-12T01:13:01.083023924-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-mzo","depends_on_id":"mcp_agent_mail-dcy","type":"blocks","created_at":"2026-01-12T01:13:01.103826413-05:00","created_by":"ubuntu"}]}
{"id":"mcp_agent_mail-n6z","title":"CLI: Mail Commands","description":"priority: 2","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-05T21:02:17.856438-05:00","updated_at":"2026-01-05T23:31:35.272046-05:00","closed_at":"2026-01-05T23:31:35.272046-05:00","close_reason":"29 comprehensive CLI mail command tests added and passing","dependencies":[{"issue_id":"mcp_agent_mail-n6z","depends_on_id":"mcp_agent_mail-uvf","type":"blocks","created_at":"2026-01-05T21:02:54.732874-05:00","created_by":"jemanuel"}]}
{"id":"mcp_agent_mail-njf","title":"Core: Contact Management Flow","description":"priority: 1","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-05T21:02:17.837354-05:00","updated_at":"2026-01-05T22:33:52.72945-05:00","closed_at":"2026-01-05T22:33:52.72945-05:00","close_reason":"Completed P1 Contact Management Flow tests - 15 tests covering contact request/approval workflow, policies, cross-project contacts, and macros"}
{"id":"mcp_agent_mail-nvh","title":"Unit Tests: app.py - Macros","description":"priority: 2","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.470616-05:00","updated_at":"2026-01-05T21:00:47.960622-05:00","deleted_at":"2026-01-05T21:00:47.960622-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-nwz","title":"Performance Optimization Initiative: Eliminate Gross Inefficiencies in Core System","description":"# Performance Optimization Initiative\n\n## Overview\nComprehensive performance optimization effort targeting provably isomorphic transformations that\nimprove latency, throughput, and responsiveness while guaranteeing identical outputs for identical inputs.\n\n## Background \u0026 Motivation\nDeep code analysis identified 20+ specific optimization opportunities across the MCP Agent Mail\ncodebase. These range from critical N+1 query patterns causing 10x+ unnecessary latency to\nalgorithmic improvements that reduce O(n²) operations to O(n) or O(1).\n\n## Guiding Principles\n1. **Isomorphic transformations only** - Changes must produce identical outputs for identical inputs\n2. **Measurable improvements** - Each change should have baseline metrics and post-implementation verification\n3. **No feature changes** - Pure performance work, no behavioral modifications\n4. **Incremental delivery** - Ship improvements in phases, validate each before proceeding\n\n## Success Criteria\n- Eliminate all identified N+1 query patterns\n- Reduce p95 latency for send_message by 5-10x\n- Reduce p95 latency for inbox/outbox listing by 10-20x\n- Enable true parallelism across projects (currently serialized)\n- Fix all O(n²) algorithms to O(n) or better\n\n## Phase Structure\n- Phase 1: Critical Priority (N+1 elimination, indexes, LRU cache) - 10x+ improvement potential\n- Phase 2: High Priority (caching, batching, per-project locks) - 3-5x improvement\n- Phase 3: Medium Priority (algorithms, concurrency) - 30-50% improvement\n- Phase 4: Low Priority (cleanup, polish) - marginal gains\n\n## Technical Context\n- Stack: Python 3.14, SQLModel/SQLAlchemy async, SQLite with FTS5, Git persistence\n- Key files: app.py (~9000 lines), storage.py (~2500 lines), db.py, http.py, share.py\n- No backwards compatibility concerns (early development, no production users)","status":"open","priority":1,"issue_type":"feature","created_at":"2026-01-12T01:05:12.406011106-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:05:12.406011106-05:00","dependencies":[{"issue_id":"mcp_agent_mail-nwz","depends_on_id":"mcp_agent_mail-mzo","type":"blocks","created_at":"2026-01-12T01:13:19.34487395-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-nwz","depends_on_id":"mcp_agent_mail-r6n","type":"blocks","created_at":"2026-01-12T01:13:19.366586463-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-nwz","depends_on_id":"mcp_agent_mail-vih","type":"blocks","created_at":"2026-01-12T01:13:19.386311312-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-nwz","depends_on_id":"mcp_agent_mail-2uf","type":"blocks","created_at":"2026-01-12T01:13:19.40646028-05:00","created_by":"ubuntu"}]}
{"id":"mcp_agent_mail-p78","title":"Unit Tests: http.py - Authentication","description":"priority: 2","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.478298-05:00","updated_at":"2026-01-05T21:00:47.157832-05:00","deleted_at":"2026-01-05T21:00:47.157832-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-pk2","title":"Implement per-project commit locks instead of global lock","description":"# Implement Per-Project Commit Locks\n\n## Problem Statement\nAll projects share a single global `.commit.lock` file, serializing ALL Git\noperations across ALL projects. This prevents any parallelism.\n\n## Code Location\n`src/mcp_agent_mail/storage.py` - Line approximately 1218\n\n## Current Code (THE PROBLEM)\n```python\n# Single lock for the entire working tree!\ncommit_lock_path = Path(working_tree).resolve() / \".commit.lock\"\n```\n\n## Impact Analysis\n- Agent A working on Project X blocks Agent B working on Project Y\n- With 10 concurrent agents on different projects, only 1 can commit at a time\n- Throughput limited to ~1 commit per 50-100ms = 10-20 commits/sec TOTAL\n- Should be 10-20 commits/sec PER PROJECT\n\n## Required Fix\nMove lock to per-project directory:\n\n```python\ndef _get_project_lock_path(archive_path: Path) -\u003e Path:\n    \"\"\"Get the lock file path for a specific project.\n    \n    Lock is placed within the project's archive directory,\n    enabling parallel commits across different projects.\n    \"\"\"\n    return archive_path / \".commit.lock\"\n\n# Usage:\ncommit_lock_path = _get_project_lock_path(project_archive_path)\n```\n\n## Directory Structure Change\n```\nBefore:\n  /working/tree/.commit.lock     \u003c- Global lock\n  /working/tree/projects/foo/\n  /working/tree/projects/bar/\n\nAfter:\n  /working/tree/projects/foo/.commit.lock  \u003c- Per-project\n  /working/tree/projects/bar/.commit.lock  \u003c- Per-project\n```\n\n## Implementation Details\n\n### Finding Project Archive Path\nNeed to trace from operation context to project archive path.\nMay require passing archive_path through more functions.\n\n### Lock File Cleanup\n- Stale locks should be cleaned up\n- Consider using fcntl.flock() for automatic cleanup on process exit\n- Or implement lock timeout with staleness check\n\n### Backwards Compatibility\n- Old global lock file can be removed or ignored\n- No migration needed since locks are ephemeral\n\n## Concurrency Model After Fix\n```\nProject A operations: [lock A] -\u003e commit -\u003e [unlock A]\nProject B operations: [lock B] -\u003e commit -\u003e [unlock B]\n                      ^^^ These can now happen in parallel!\n```\n\n## Testing Strategy\n1. Start two agents on different projects simultaneously\n2. Have both perform commit operations\n3. Verify both complete without blocking each other\n4. Verify no data corruption in either project\n\n## Verification\nUse strace or timing to verify parallel execution:\n```bash\n# In terminal 1\ntime python -c \"agent_operation_on_project_a()\"\n\n# In terminal 2 (simultaneously)  \ntime python -c \"agent_operation_on_project_b()\"\n\n# Both should complete in ~same time, not 2x the time\n```\n\n## Risk Assessment\n- **Medium risk**: Locking changes require careful testing\n- **Concern**: Race conditions during lock acquisition\n- **Mitigation**: Thorough integration tests with concurrent access\n\n## Dependencies\n- Should be done after commit batching for clean implementation","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-12T01:09:23.557640132-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:09:23.557640132-05:00","dependencies":[{"issue_id":"mcp_agent_mail-pk2","depends_on_id":"mcp_agent_mail-roy","type":"blocks","created_at":"2026-01-12T01:12:50.058614282-05:00","created_by":"ubuntu"}]}
{"id":"mcp_agent_mail-qe7","title":"Security: Path Traversal","description":"priority: 3","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-05T21:02:17.871514-05:00","updated_at":"2026-01-06T00:49:03.166984-05:00","closed_at":"2026-01-06T00:49:03.166984-05:00","close_reason":"Created 27 tests covering path traversal security: agent name sanitization, archive tree/content path validation, project slug sanitization, file reservation patterns, attachment path handling, and archive extraction security.","dependencies":[{"issue_id":"mcp_agent_mail-qe7","depends_on_id":"mcp_agent_mail-aew","type":"blocks","created_at":"2026-01-05T21:02:55.30258-05:00","created_by":"jemanuel"}]}
{"id":"mcp_agent_mail-qem","title":"Replace list.index() in sort key with pre-computed dict","description":"# Replace list.index() in Sort Key with Pre-computed Dict\n\n## Problem Statement\nSort key function uses `list.index()` which is O(p) where p = list length,\nmaking the overall sort O(n * p * log n) instead of O(n log n).\n\n## Code Location\n`src/mcp_agent_mail/share.py` - Line approximately 383\n\n## Current Code (THE PROBLEM)\n```python\npreferred_order = [\"high\", \"normal\", \"low\", \"info\"]\n\n# This calls .index() for EVERY comparison!\nhints.sort(key=lambda hint: preferred_order.index(hint.key) if hint.key in preferred_order else len(preferred_order))\n```\n\n## Impact Analysis\n- sort() makes O(n log n) comparisons\n- Each comparison calls .index() which is O(p)\n- Total: O(n * p * log n) instead of O(n log n)\n- For n=1000 hints and p=4 priorities: 4x slower than necessary\n\n## Required Fix\nPre-compute a dict for O(1) lookups:\n\n```python\npreferred_order = [\"high\", \"normal\", \"low\", \"info\"]\n\n# Pre-compute once: O(p)\norder_map = {key: idx for idx, key in enumerate(preferred_order)}\ndefault_order = len(preferred_order)\n\n# Sort with O(1) lookups\nhints.sort(key=lambda hint: order_map.get(hint.key, default_order))\n```\n\n## Why This Is Better\n- Dict lookup is O(1) vs list.index() O(p)\n- Dict creation is O(p) one-time cost\n- Sort is now pure O(n log n)\n\n## Finding Similar Patterns\nSearch for `.index(` usage within lambda or sort key contexts:\n```\n\\.sort\\(.*\\.index\\(\nsorted\\(.*\\.index\\(\n```\n\n## Testing Strategy\n1. Verify sort order is identical before/after\n2. Benchmark with large hint lists\n3. Test with keys not in preferred_order\n\n## Verification\n```python\nimport random\nimport time\n\npreferred_order = [\"high\", \"normal\", \"low\", \"info\", \"debug\", \"trace\"]\n\n# Generate test data\nhints = [type('Hint', (), {'key': random.choice(preferred_order + ['unknown'])})() \n         for _ in range(10000)]\n\n# Old way\ndef old_sort(hints):\n    return sorted(hints, key=lambda h: preferred_order.index(h.key) if h.key in preferred_order else len(preferred_order))\n\n# New way\ndef new_sort(hints):\n    order_map = {k: i for i, k in enumerate(preferred_order)}\n    default = len(preferred_order)\n    return sorted(hints, key=lambda h: order_map.get(h.key, default))\n\n# Verify identical results\nold_result = old_sort(hints.copy())\nnew_result = new_sort(hints.copy())\nassert [h.key for h in old_result] == [h.key for h in new_result]\n\n# Benchmark\nt1 = time.perf_counter()\nfor _ in range(100):\n    old_sort(hints.copy())\nold_time = time.perf_counter() - t1\n\nt2 = time.perf_counter()\nfor _ in range(100):\n    new_sort(hints.copy())\nnew_time = time.perf_counter() - t2\n\nprint(f\"Old: {old_time:.3f}s, New: {new_time:.3f}s, Speedup: {old_time/new_time:.1f}x\")\n```\n\n## Dependencies\nNone - self-contained fix","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-12T01:10:51.163791202-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:10:51.163791202-05:00"}
{"id":"mcp_agent_mail-r6n","title":"Phase 2: Caching and Git Operation Batching","description":"# Phase 2: High Priority Optimizations (3-5x Improvement)\n\n## Scope\nThis phase targets expensive repeated computations (PathSpec compilation) and inefficient\nGit operations (multiple commits, global locks).\n\n## Why This Matters\n- PathSpec.from_lines() compiles regex patterns - expensive when called repeatedly\n- Each Git commit involves index manipulation, SHA computation, and disk sync\n- Global commit lock serializes ALL projects - no parallelism possible\n\n## Expected Impact\n- PathSpec operations: 10-50x improvement through caching\n- Git operations: 3-5x improvement through batching\n- Cross-project operations: Full parallelism instead of serialization\n\n## Key Files\n- src/mcp_agent_mail/app.py - Line 2938-2939 (PathSpec compilation)\n- src/mcp_agent_mail/storage.py - Lines 737-851 (commits), 1218 (global lock)\n- src/mcp_agent_mail/http.py - Lines 1388-1389 (snippet calls)\n\n## Technical Approach\n\n### PathSpec Caching\nThe pathspec library compiles glob patterns into regex. This compilation is deterministic\nand expensive. Adding @lru_cache(maxsize=512) around pattern compilation eliminates\nredundant work in reservation conflict detection loops.\n\n### Git Commit Batching\nCurrently, operations like register_agent + file_reservation create separate commits.\nThese can be consolidated into atomic multi-path commits, reducing Git overhead.\n\n### Per-Project Locks\nThe current `.commit.lock` is at the working tree root, serializing all projects.\nMoving to `projects/\u003cslug\u003e/.commit.lock` enables true parallelism.\n\n## Tasks\n1. Add @lru_cache to PathSpec compilation\n2. Consolidate Git commits for related operations\n3. Implement per-project commit locks\n4. Fix triple snippet() calls in search\n5. Replace sorted()[:n] with heapq.nlargest()","status":"open","priority":1,"issue_type":"feature","created_at":"2026-01-12T01:05:51.817252389-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:05:51.817252389-05:00","dependencies":[{"issue_id":"mcp_agent_mail-r6n","depends_on_id":"mcp_agent_mail-mzo","type":"blocks","created_at":"2026-01-12T01:06:18.773869151-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-r6n","depends_on_id":"mcp_agent_mail-h4o","type":"blocks","created_at":"2026-01-12T01:13:10.500589976-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-r6n","depends_on_id":"mcp_agent_mail-roy","type":"blocks","created_at":"2026-01-12T01:13:10.523414854-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-r6n","depends_on_id":"mcp_agent_mail-pk2","type":"blocks","created_at":"2026-01-12T01:13:10.546568452-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-r6n","depends_on_id":"mcp_agent_mail-1kw","type":"blocks","created_at":"2026-01-12T01:13:10.568191737-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-r6n","depends_on_id":"mcp_agent_mail-0uh","type":"blocks","created_at":"2026-01-12T01:13:10.587622482-05:00","created_by":"ubuntu"}]}
{"id":"mcp_agent_mail-roy","title":"Consolidate Git commits for related operations","description":"# Consolidate Git Commits for Related Operations\n\n## Problem Statement\nOperations that modify multiple files create separate Git commits for each file,\nmultiplying Git overhead unnecessarily.\n\n## Code Location\n`src/mcp_agent_mail/storage.py` - Lines approximately 737-851 and other commit sites\n\n## Current Behavior\n```python\n# register_agent creates one commit\nawait _commit(archive.repo, settings, f\"agent: profile {agent['name']}\", [profile_path])\n\n# Then file_reservation_paths creates another commit  \nawait _commit(archive.repo, settings, f\"file_reservation: {pattern}\", [reservation_path])\n\n# Two separate commits for what is logically one operation!\n```\n\n## Impact Analysis\nEach Git commit involves:\n- Reading and updating the index\n- Computing SHA-1 hashes for tree objects\n- Writing commit object to pack\n- Potentially triggering hooks\n- fsync() for durability\n\nTwo commits = 2x this overhead.\n\n## Required Fix\nCreate a batched commit helper:\n\n```python\nasync def _commit_batch(\n    repo: Repo,\n    settings: ServerSettings,\n    message: str,\n    paths: list[str],\n) -\u003e str | None:\n    \"\"\"Batch multiple paths into a single atomic commit.\n    \n    Args:\n        repo: Git repository\n        settings: Server settings for author info\n        message: Commit message\n        paths: List of paths to add and commit\n        \n    Returns:\n        Commit SHA if successful, None if nothing to commit\n    \"\"\"\n    if not paths:\n        return None\n        \n    # Add all paths to index\n    for path in paths:\n        repo.index.add([path])\n    \n    # Single commit\n    return repo.index.commit(\n        message,\n        author=Actor(settings.git_author_name, settings.git_author_email),\n        committer=Actor(settings.git_author_name, settings.git_author_email),\n    ).hexsha\n```\n\n## Usage Pattern\n```python\n# Before: 2 commits\nawait _commit(repo, settings, \"agent profile\", [profile_path])\nawait _commit(repo, settings, \"reservation\", [reservation_path])\n\n# After: 1 commit with both files\nawait _commit_batch(\n    repo, settings,\n    f\"agent: {agent['name']} + reservation: {pattern}\",\n    [profile_path, reservation_path]\n)\n```\n\n## Implementation Considerations\n\n### Commit Message Format\nWith multiple files, commit messages should be descriptive:\n```\nagent: BlueLake session setup\n\n- profile: agents/BlueLake/profile.json\n- reservation: file_reservations/abc123.json\n```\n\n### Atomicity\nBatched commits are atomic - either all files are committed or none.\nThis is actually BETTER than separate commits for related operations.\n\n### Where to Apply\nFind all places where multiple commits happen in sequence for related operations:\n1. Agent registration + initial reservation\n2. Message send + inbox/outbox copies\n3. Any other multi-file operations\n\n## Testing Strategy\n1. Perform operation that currently creates 2 commits\n2. Verify only 1 commit is created after fix\n3. Verify commit contains all expected files\n4. Verify git log shows proper history\n\n## Risk Assessment\n- **Low risk**: Git commits are atomic\n- **Consideration**: Commit messages less granular\n- **Mitigation**: Use detailed commit messages listing all changes\n\n## Dependencies\nNone - but should be coordinated with per-project locks task","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-12T01:09:23.409860588-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:09:23.409860588-05:00"}
{"id":"mcp_agent_mail-tm6","title":"Milestone: Critical Path Coverage","description":"priority: 1","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-05T21:02:17.880711-05:00","updated_at":"2026-01-06T00:13:24.562538-05:00","closed_at":"2026-01-06T00:13:24.562538-05:00","close_reason":"All P0-P1 critical path tests completed: kkp (Session Context), yhk (Datetime), aew (File Reservation), uvf (Message Delivery)","dependencies":[{"issue_id":"mcp_agent_mail-tm6","depends_on_id":"mcp_agent_mail-yhk","type":"blocks","created_at":"2026-01-05T21:02:56.643878-05:00","created_by":"jemanuel"},{"issue_id":"mcp_agent_mail-tm6","depends_on_id":"mcp_agent_mail-kkp","type":"blocks","created_at":"2026-01-05T21:02:56.778506-05:00","created_by":"jemanuel"},{"issue_id":"mcp_agent_mail-tm6","depends_on_id":"mcp_agent_mail-uvf","type":"blocks","created_at":"2026-01-05T21:02:56.920383-05:00","created_by":"jemanuel"},{"issue_id":"mcp_agent_mail-tm6","depends_on_id":"mcp_agent_mail-aew","type":"blocks","created_at":"2026-01-05T21:02:57.079543-05:00","created_by":"jemanuel"}]}
{"id":"mcp_agent_mail-try","title":"Consolidate repeated file existence checks into single stat()","description":"# Consolidate File Existence Checks\n\n## Problem Statement\nCode performs multiple separate file existence checks (stat syscalls) when\na single stat() call with exception handling would suffice.\n\n## Code Location\n`src/mcp_agent_mail/storage.py` - Lines approximately 403-424\n\n## Current Code (THE PROBLEM)\n```python\n# Multiple stat() syscalls for the same logical check\nif not self._path.exists():        # stat() #1\n    return False\n    \nif self._metadata_path.exists():   # stat() #2\n    metadata = self._load_metadata()\nelif self._legacy_path.exists():   # stat() #3\n    metadata = self._load_legacy()\n```\n\n## Impact Analysis\n- Each .exists() is a stat() syscall\n- stat() requires kernel context switch\n- 3 syscalls where 1-2 would suffice\n- On network filesystems, latency compounds significantly\n\n## Required Fix\nUse exception handling for control flow:\n\n```python\ndef _load_project_data(self) -\u003e ProjectData | None:\n    \"\"\"Load project data with minimal syscalls.\"\"\"\n    try:\n        # Try primary path first - one stat() + read if exists\n        return self._load_metadata()\n    except FileNotFoundError:\n        pass\n    \n    try:\n        # Fall back to legacy path\n        return self._load_legacy()\n    except FileNotFoundError:\n        return None\n```\n\nOr use single stat() with result caching:\n\n```python\ndef _check_paths(self) -\u003e tuple[bool, bool, bool]:\n    \"\"\"Check multiple path existence with minimal syscalls.\"\"\"\n    main_exists = self._path.exists()\n    if not main_exists:\n        return False, False, False\n    \n    # Only check sub-paths if main exists\n    metadata_exists = self._metadata_path.exists()\n    legacy_exists = self._legacy_path.exists() if not metadata_exists else False\n    \n    return main_exists, metadata_exists, legacy_exists\n```\n\n## EAFP vs LBYL\nPython idiom \"Easier to Ask Forgiveness than Permission\" (EAFP) suggests:\n- Try the operation, handle exception if it fails\n- Rather than checking if operation will succeed then doing it\n\nThis is especially efficient for file operations where the check and\noperation are separate syscalls.\n\n## Finding Similar Patterns\nSearch for consecutive .exists() calls:\n```python\nif .*\\.exists\\(\\)\n```\n\n## Testing Strategy\n1. Verify behavior unchanged for existing files\n2. Verify behavior unchanged for missing files\n3. Use strace to count stat() syscalls before/after\n4. Benchmark on local and network filesystems\n\n## Verification\n```bash\n# Count syscalls before fix\nstrace -e stat,statx,newfstatat python -c \"project._check_paths()\" 2\u003e\u00261 | grep -c stat\n\n# Count syscalls after fix\nstrace -e stat,statx,newfstatat python -c \"project._check_paths()\" 2\u003e\u00261 | grep -c stat\n```\n\n## Dependencies\nNone - self-contained fix","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-12T01:10:51.317969551-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:10:51.317969551-05:00"}
{"id":"mcp_agent_mail-u8g","title":"Integration Tests: Thread Conversations","description":"priority: 2","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.485294-05:00","updated_at":"2026-01-05T21:00:46.22444-05:00","deleted_at":"2026-01-05T21:00:46.22444-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-udi","title":"E2E: Disaster Recovery","description":"priority: 4","status":"closed","priority":4,"issue_type":"task","created_at":"2026-01-05T21:02:17.879357-05:00","updated_at":"2026-01-06T02:34:40.022403-05:00","closed_at":"2026-01-06T02:34:40.022403-05:00","close_reason":"5 E2E disaster recovery tests implemented and passing","dependencies":[{"issue_id":"mcp_agent_mail-udi","depends_on_id":"mcp_agent_mail-enu","type":"blocks","created_at":"2026-01-05T21:02:56.527536-05:00","created_by":"jemanuel"}]}
{"id":"mcp_agent_mail-uvf","title":"Core: Message Delivery Flow","description":"priority: 1","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-05T21:02:17.835672-05:00","updated_at":"2026-01-05T22:06:09.098428-05:00","closed_at":"2026-01-05T22:06:09.098428-05:00","close_reason":"Created 21 comprehensive tests for message delivery flow covering: basic sending, multiple recipients (to/cc/bcc), threads, importance levels, ack flags, inbox fetching, replies, read/acknowledge workflows, and error handling. All tests passing."}
{"id":"mcp_agent_mail-vih","title":"Phase 3: Algorithm and Concurrency Improvements","description":"# Phase 3: Medium Priority Optimizations (30-50% Improvement)\n\n## Scope\nThis phase addresses algorithmic inefficiencies and concurrency issues that cause\nmoderate but noticeable performance degradation.\n\n## Issues Addressed\n\n### Algorithmic Problems\n1. **String normalization O(n*k)** - While loop with replace() iterates k times\n2. **List membership O(n²)** - Checking `if x not in list` in a loop\n3. **Index lookup in sort O(n*p)** - Using list.index() as sort key\n4. **Repeated stat() calls** - Multiple file existence checks\n\n### Concurrency Problems\n1. **Rate limiter race condition** - No synchronization on bucket access\n2. **Schema lock too coarse** - Single lock for all schema operations\n3. **Memory leak in process locks** - Lock dict grows unboundedly\n\n## Expected Impact\n- String operations: k× improvement (k = iterations eliminated)\n- Collection operations: n× improvement (n = collection size)\n- Concurrent operations: Reduced contention, better throughput\n\n## Key Files\n- src/mcp_agent_mail/app.py - Lines 1054-1055\n- src/mcp_agent_mail/share.py - Lines 383, 696\n- src/mcp_agent_mail/storage.py - Lines 403-424\n- src/mcp_agent_mail/http.py - Lines 306-314\n- src/mcp_agent_mail/db.py - Lines 235-251\n\n## Technical Notes\n\n### Why These Are Medium Priority\nThese issues cause real performance problems but typically in specific scenarios:\n- String normalization: Only affects subjects/bodies with multiple spaces\n- List membership: Only affects collections that grow large\n- Race conditions: Only manifest under concurrent load\n\n### Implementation Considerations\n- Regex for string normalization is cleaner but slightly different semantics (tabs?)\n- Set membership requires hashable elements (may need to hash by ID)\n- RWLock implementation adds complexity - ensure it's worth it","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-12T01:05:52.421204634-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:05:52.421204634-05:00","dependencies":[{"issue_id":"mcp_agent_mail-vih","depends_on_id":"mcp_agent_mail-r6n","type":"blocks","created_at":"2026-01-12T01:06:18.790884586-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-vih","depends_on_id":"mcp_agent_mail-24j","type":"blocks","created_at":"2026-01-12T01:13:11.153106468-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-vih","depends_on_id":"mcp_agent_mail-i9c","type":"blocks","created_at":"2026-01-12T01:13:11.17538116-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-vih","depends_on_id":"mcp_agent_mail-qem","type":"blocks","created_at":"2026-01-12T01:13:11.195094317-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-vih","depends_on_id":"mcp_agent_mail-try","type":"blocks","created_at":"2026-01-12T01:13:11.216408409-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-vih","depends_on_id":"mcp_agent_mail-9tj","type":"blocks","created_at":"2026-01-12T01:13:11.237984335-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-vih","depends_on_id":"mcp_agent_mail-az3","type":"blocks","created_at":"2026-01-12T01:13:11.259260066-05:00","created_by":"ubuntu"}]}
{"id":"mcp_agent_mail-w51","title":"HTTP: Authentication","description":"priority: 2","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-05T21:02:17.864235-05:00","updated_at":"2026-01-06T00:34:24.246687-05:00","closed_at":"2026-01-06T00:34:24.246687-05:00","close_reason":"23 HTTP authentication tests: bearer auth, localhost bypass, CORS/health bypass, RBAC, OAuth metadata, JWT helpers, rate limiting"}
{"id":"mcp_agent_mail-xhf","title":"E2E Test Script: Performance Under Load","description":"priority: 3","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.487882-05:00","updated_at":"2026-01-05T21:00:45.544252-05:00","deleted_at":"2026-01-05T21:00:45.544252-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-y35","title":"Testing Infrastructure Foundation","description":"priority: 1","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.465256-05:00","updated_at":"2026-01-05T21:00:48.939985-05:00","deleted_at":"2026-01-05T21:00:48.939985-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-y59","title":"P3 - Performance Tests","description":"Test performance characteristics.","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T21:02:17.877469-05:00","updated_at":"2026-01-05T21:02:37.860358-05:00","deleted_at":"2026-01-05T21:02:37.860358-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-yh8","title":"Security: Input Sanitization","description":"priority: 3","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-05T21:02:17.870336-05:00","updated_at":"2026-01-06T00:53:26.090299-05:00","closed_at":"2026-01-06T00:53:26.090299-05:00","close_reason":"Added 28 comprehensive security tests: FTS sanitization, SQL injection prevention, path traversal, DoS handling, null byte injection, unicode attacks, XSS prevention, and malformed JSON handling. All tests pass.","dependencies":[{"issue_id":"mcp_agent_mail-yh8","depends_on_id":"mcp_agent_mail-uvf","type":"blocks","created_at":"2026-01-05T21:02:55.202381-05:00","created_by":"jemanuel"}]}
{"id":"mcp_agent_mail-yhk","title":"Regression: Datetime Naive/Aware Handling","description":"priority: 0","status":"closed","priority":0,"issue_type":"task","created_at":"2026-01-05T21:02:17.828319-05:00","updated_at":"2026-01-05T21:34:15.142142-05:00","closed_at":"2026-01-05T21:34:15.142142-05:00","close_reason":"All 20 datetime regression tests implemented and passing"}
{"id":"mcp_agent_mail-yi0","title":"Unit Tests: cli.py - Core Commands","description":"priority: 2","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.472698-05:00","updated_at":"2026-01-05T21:00:47.51517-05:00","deleted_at":"2026-01-05T21:00:47.51517-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-ypm","title":"Unit Tests: storage.py - Archive Operations","description":"priority: 2","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.471913-05:00","updated_at":"2026-01-05T21:00:47.676906-05:00","deleted_at":"2026-01-05T21:00:47.676906-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-yzu","title":"MCP Tools: Happy Path Coverage","description":"priority: 1","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-05T21:02:17.841695-05:00","updated_at":"2026-01-05T22:36:39.811899-05:00","closed_at":"2026-01-05T22:36:39.811899-05:00","close_reason":"All MCP tools have happy path coverage across existing test suite: test_server.py, test_project_agent_setup.py, test_message_delivery_regression.py, test_contact_management_flow.py, test_file_reservation_lifecycle.py, test_macros.py, test_guard_tools.py","dependencies":[{"issue_id":"mcp_agent_mail-yzu","depends_on_id":"mcp_agent_mail-mm2","type":"blocks","created_at":"2026-01-05T21:02:54.212411-05:00","created_by":"jemanuel"}]}
{"id":"mcp_agent_mail-z00","title":"Integration Tests: HTTP Transport","description":"priority: 2","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.486362-05:00","updated_at":"2026-01-05T21:00:45.977935-05:00","deleted_at":"2026-01-05T21:00:45.977935-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-z9g","title":"Unit Tests: share.py - Archive Restore","description":"priority: 2","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.483519-05:00","updated_at":"2026-01-05T21:00:46.661028-05:00","deleted_at":"2026-01-05T21:00:46.661028-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-zgs","title":"P0 - Critical Regression Tests","description":"These tests prevent recurrence of known bugs and must pass before any release.","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T21:02:17.82546-05:00","updated_at":"2026-01-05T21:02:37.011981-05:00","deleted_at":"2026-01-05T21:02:37.011981-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-zmk","title":"Unit Tests: models.py","description":"priority: 2","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.466891-05:00","updated_at":"2026-01-05T21:00:48.845938-05:00","deleted_at":"2026-01-05T21:00:48.845938-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
