{"id":"mcp_agent_mail-01h","title":"Implement am doctor diagnostic command","description":"Add 'am doctor' CLI command to diagnose and repair common failure modes: stale locks, orphaned records, FTS sync, archive-DB consistency. Includes backup before any destructive operations.","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-06T19:49:53.466699-05:00","created_by":"jemanuel","updated_at":"2026-01-06T19:50:53.547583-05:00","closed_at":"2026-01-06T19:50:53.547583-05:00","close_reason":"Already implemented - discovered during exploration"}
{"id":"mcp_agent_mail-0uh","title":"Replace sorted()[:n] with heapq.nlargest() for top-N selection","description":"# Replace sorted()[:n] with heapq.nlargest()\n\n## Problem Statement\nSeveral places in the code use `sorted(items, key=...)[:n]` to get the top N items.\nThis is O(m log m) where m = total items, but only O(m log n) is needed.\n\n## Code Locations\n`src/mcp_agent_mail/app.py` - Lines approximately 3204, 3215, 6278\n\n## Current Code (THE PROBLEM)\n```python\n# Getting top 10 mentions\nsorted_mentions = sorted(mentions.items(), key=lambda x: -x[1])[:10]\n\n# This sorts ALL items (O(m log m)) just to get 10!\n```\n\n## Impact Analysis\n- For m=1000 items, sorted() does ~10,000 comparisons\n- heapq.nlargest(10, ...) does ~1000 comparisons (one pass + 10 heap ops)\n- 10x fewer comparisons for this example\n- Savings grow with collection size\n\n## Required Fix\n```python\nimport heapq\n\n# Before: O(m log m)\nsorted_mentions = sorted(mentions.items(), key=lambda x: -x[1])[:10]\n\n# After: O(m log n) where n=10\ntop_mentions = heapq.nlargest(10, mentions.items(), key=lambda x: x[1])\n```\n\n## Algorithm Comparison\n```\nCollection size (m) | sorted()[:10] | heapq.nlargest(10)\n--------------------|---------------|-------------------\n100                 | ~660 ops      | ~100 ops\n1,000               | ~10,000 ops   | ~1,000 ops\n10,000              | ~130,000 ops  | ~10,000 ops\n```\n\n## Implementation Details\n\n### Finding All Occurrences\nSearch for patterns like:\n```python\nsorted(...)[:N]\nsorted(...)[0:N]\nsorted(..., reverse=True)[:N]\n```\n\n### Key Function Adjustment\n- sorted() with `key=lambda x: -x[1]` (negative for descending)\n- heapq.nlargest() naturally returns largest, so use positive key:\n  `key=lambda x: x[1]`\n\n### For nsmallest\nIf code uses `sorted(...)[:n]` without reverse, that's getting smallest.\nUse `heapq.nsmallest(n, items, key=...)` instead.\n\n## Edge Cases\n- n \u003e= len(items): heapq still works, returns all items sorted\n- n = 0: returns empty list\n- n = 1: heapq is overkill but still correct\n\n## When NOT to Change\n- If n is close to len(items), sorted() is actually better\n- Heuristic: Use heapq when n \u003c len(items) / 5\n\n## Testing Strategy\n1. Verify output is identical for various inputs\n2. Benchmark with large collections\n3. Test edge cases (empty, n \u003e len)\n\n## Verification\n```python\nimport heapq\nimport random\n\nitems = [(f\"item{i}\", random.randint(0, 1000)) for i in range(10000)]\n\n# These should produce identical results\nresult1 = sorted(items, key=lambda x: -x[1])[:10]\nresult2 = heapq.nlargest(10, items, key=lambda x: x[1])\n\nassert result1 == result2, \"Results differ!\"\n```\n\n## Dependencies\nNone - self-contained changes","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-12T01:09:24.746029498-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:09:24.746029498-05:00"}
{"id":"mcp_agent_mail-16q","title":"Integration Tests: Archive Save/Restore","description":"priority: 2","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.486081-05:00","updated_at":"2026-01-05T21:00:46.059248-05:00","deleted_at":"2026-01-05T21:00:46.059248-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-1kw","title":"Fix triple snippet() calls in FTS search queries","description":"# Fix Triple snippet() Calls in Search\n\n## Problem Statement\nSearch queries call SQLite's snippet() function THREE times per row:\n1. Once to get the snippet\n2. Once to calculate length\n3. Once more inside replace() for hit counting\n\nThis triples the work for generating search results.\n\n## Code Location\n`src/mcp_agent_mail/http.py` - Lines approximately 1388-1389\n\n## Current Code (THE PROBLEM)\n```sql\nSELECT \n    snippet(fts_messages, 2, '\u003cmark\u003e', '\u003c/mark\u003e', '…', 18) AS body_snippet,\n    -- THIS CALLS snippet() TWO MORE TIMES!\n    (length(snippet(fts_messages, 2, '\u003cmark\u003e', '\u003c/mark\u003e', '…', 18)) \n     - length(replace(snippet(fts_messages, 2, '\u003cmark\u003e', '\u003c/mark\u003e', '…', 18), '\u003cmark\u003e', ''))) \n    / 6 AS hits\nFROM fts_messages WHERE ...\n```\n\n## Impact Analysis\n- snippet() scans the FTS index and extracts matching fragments\n- Calling 3x per row triples the FTS extraction work\n- For 100 search results, that's 300 snippet() calls instead of 100\n\n## Required Fix\nCall snippet() once in SQL, count hits in Python:\n\n```sql\nSELECT \n    snippet(fts_messages, 2, '\u003cmark\u003e', '\u003c/mark\u003e', '…', 18) AS body_snippet\n    -- No more hit calculation in SQL\nFROM fts_messages WHERE ...\n```\n\n```python\n# In Python, after fetching results:\nfor row in results:\n    body_snippet = row.body_snippet\n    # Count hits efficiently in Python\n    hits = body_snippet.count('\u003cmark\u003e')\n    # Or if you need the formula: (len with marks - len without) / len('\u003cmark\u003e')\n```\n\n## Why Python Is Better Here\n- String operations in Python are highly optimized (C implementation)\n- We already have the snippet in memory\n- Avoids additional FTS index scans\n- Code is more readable and maintainable\n\n## Implementation Details\n\n### Finding the Query\nSearch http.py for \"snippet(\" to find all occurrences.\nMay be multiple search endpoints that need fixing.\n\n### Handling Edge Cases\n- Empty snippet: hits = 0\n- No \u003cmark\u003e tags: hits = 0\n- Very long snippets: Still O(n) string scan, but only once\n\n## Testing Strategy\n1. Search query returning 100 results\n2. Time before/after fix\n3. Verify hit counts are identical\n4. Verify snippets are identical\n\n## Performance Measurement\n```python\nimport time\n\n# Before fix\nstart = time.perf_counter()\nresults = await search(\"test query\", limit=100)\nbefore_time = time.perf_counter() - start\n\n# After fix\nstart = time.perf_counter()\nresults = await search(\"test query\", limit=100)\nafter_time = time.perf_counter() - start\n\nprint(f\"Before: {before_time:.3f}s, After: {after_time:.3f}s\")\n# Expected: ~3x improvement\n```\n\n## Dependencies\nNone - self-contained fix","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-12T01:09:23.69685367-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:09:23.69685367-05:00"}
{"id":"mcp_agent_mail-21j","title":"Add LIMIT to potentially unbounded queries","description":"# Add LIMIT to Unbounded Queries\n\n## Problem Statement\nSome queries use hardcoded high LIMIT values (e.g., LIMIT 10000) or no LIMIT\nat all, potentially returning far more data than needed.\n\n## Code Locations\nSearch for:\n- `LIMIT 10000` or similar high limits\n- SELECT statements without LIMIT\n- `.all()` calls on potentially large tables\n\n## Current Code Examples (THE PROBLEM)\n```python\n# Hardcoded high limit - might fetch 10,000 when user needs 20\nstmt = select(Message).where(...).limit(10000)\n\n# No limit at all - could return millions\nstmt = select(FileReservation).where(FileReservation.project_id == project_id)\n```\n\n## Impact Analysis\n- Overfetching wastes database I/O\n- Wastes network bandwidth\n- Wastes application memory\n- Wastes serialization CPU\n- Slower response times for users who need small results\n\n## Required Fix\nAdd reasonable defaults with user override:\n\n```python\nasync def list_messages(\n    project_id: int,\n    limit: int = 100,  # Sensible default\n    offset: int = 0,\n) -\u003e list[Message]:\n    \"\"\"List messages with pagination.\n    \n    Args:\n        project_id: Project to query\n        limit: Max results (default 100, max 1000)\n        offset: Skip first N results\n    \"\"\"\n    # Enforce maximum to prevent abuse\n    limit = min(limit, 1000)\n    \n    stmt = (\n        select(Message)\n        .where(Message.project_id == project_id)\n        .order_by(Message.created_ts.desc())\n        .limit(limit)\n        .offset(offset)\n    )\n    ...\n```\n\n## Pagination Pattern\nFor large result sets, implement cursor-based pagination:\n\n```python\nasync def list_messages_paginated(\n    project_id: int,\n    after_id: int | None = None,\n    limit: int = 100,\n) -\u003e tuple[list[Message], int | None]:\n    \"\"\"List messages with cursor pagination.\n    \n    Returns:\n        Tuple of (messages, next_cursor) where next_cursor is None if no more.\n    \"\"\"\n    stmt = select(Message).where(Message.project_id == project_id)\n    \n    if after_id:\n        stmt = stmt.where(Message.id \u003e after_id)\n    \n    stmt = stmt.order_by(Message.id).limit(limit + 1)  # Fetch one extra\n    \n    result = await session.execute(stmt)\n    messages = list(result.scalars().all())\n    \n    # Check if there are more\n    if len(messages) \u003e limit:\n        messages = messages[:limit]\n        next_cursor = messages[-1].id\n    else:\n        next_cursor = None\n    \n    return messages, next_cursor\n```\n\n## Where to Apply\n1. Internal list functions that feed API endpoints\n2. Search queries\n3. Any query that could return unbounded results\n\n## Testing Strategy\n1. Verify default limit is applied\n2. Verify user can request smaller limit\n3. Verify max limit is enforced\n4. Verify pagination works correctly\n\n## Dependencies\nNone - can be applied incrementally","status":"open","priority":3,"issue_type":"task","created_at":"2026-01-12T01:12:24.387994975-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:12:24.387994975-05:00"}
{"id":"mcp_agent_mail-23f","title":"Unit Tests: guard.py - Pre-commit","description":"priority: 2","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.481906-05:00","updated_at":"2026-01-05T21:00:46.98086-05:00","deleted_at":"2026-01-05T21:00:46.98086-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-24j","title":"Replace string normalization while loop with regex","description":"# Replace String Normalization While Loop with Regex\n\n## Problem Statement\nString normalization uses a while loop to collapse multiple spaces, which\niterates O(k) times where k is the number of space sequences to collapse.\n\n## Code Location\n`src/mcp_agent_mail/app.py` - Lines approximately 1054-1055\n\n## Current Code (THE PROBLEM)\n```python\n# Collapse multiple spaces - O(n*k) where k = number of iterations\nwhile \"  \" in trimmed:\n    trimmed = trimmed.replace(\"  \", \" \")\n```\n\n## Impact Analysis\n- Input: \"hello     world\" (5 spaces)\n- Iteration 1: \"hello   world\" (3 spaces) - scans entire string\n- Iteration 2: \"hello  world\" (2 spaces) - scans entire string again\n- Iteration 3: \"hello world\" (1 space) - scans entire string again\n- Total: 3 full string scans for one normalization\n\nWith very long strings or many space sequences, this compounds significantly.\n\n## Required Fix\n```python\nimport re\n\n# O(n) single pass with regex\ntrimmed = re.sub(r' +', ' ', trimmed)\n```\n\n## Regex Explanation\n- `' +'` matches one or more spaces\n- Replacement `' '` collapses to single space\n- Single pass through string, no iteration\n\n## Pre-compiled Pattern (Optional Optimization)\nIf this function is called frequently:\n```python\n# At module level\n_MULTI_SPACE_RE = re.compile(r' +')\n\n# In function\ntrimmed = _MULTI_SPACE_RE.sub(' ', trimmed)\n```\n\n## Semantic Differences to Consider\nThe regex `' +'` matches one or more spaces, which will also \"replace\"\nsingle spaces with themselves. This is slightly different from the while\nloop which only triggers on 2+ spaces.\n\nMore precise regex to match original semantics:\n```python\ntrimmed = re.sub(r' {2,}', ' ', trimmed)  # Only matches 2+ spaces\n```\n\n## Testing Strategy\n1. Test with single space - unchanged\n2. Test with double space - collapsed\n3. Test with 10 spaces - collapsed to 1\n4. Test with multiple space groups - all collapsed\n5. Test empty string - unchanged\n6. Test no spaces - unchanged\n\n## Verification\n```python\ntest_cases = [\n    (\"hello world\", \"hello world\"),      # Single space unchanged\n    (\"hello  world\", \"hello world\"),     # Double collapsed\n    (\"hello     world\", \"hello world\"),  # Many collapsed\n    (\"  leading\", \" leading\"),           # Leading collapsed\n    (\"trailing  \", \"trailing \"),         # Trailing collapsed\n    (\"\", \"\"),                            # Empty unchanged\n]\n\nfor input_str, expected in test_cases:\n    result = re.sub(r' +', ' ', input_str)\n    assert result == expected, f\"Failed: {input_str!r}\"\n```\n\n## Dependencies\nNone - self-contained fix","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-12T01:10:48.840958136-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:10:48.840958136-05:00"}
{"id":"mcp_agent_mail-24o","title":"Integration Tests: Concurrent Access","description":"priority: 2","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.486681-05:00","updated_at":"2026-01-05T21:00:45.888166-05:00","deleted_at":"2026-01-05T21:00:45.888166-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-24w","title":"P2 - Guard Hook Tests","description":"Test pre-commit and pre-push guards.","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T21:02:17.867107-05:00","updated_at":"2026-01-05T21:02:37.56359-05:00","deleted_at":"2026-01-05T21:02:37.56359-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-2e4","title":"Integration Tests: Contact Management Flow","description":"priority: 2","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.484957-05:00","updated_at":"2026-01-05T21:00:46.302378-05:00","deleted_at":"2026-01-05T21:00:46.302378-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-2qc","title":"P2 - CLI Integration Tests","description":"Test CLI commands work correctly.","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T21:02:17.854864-05:00","updated_at":"2026-01-05T21:02:37.391256-05:00","deleted_at":"2026-01-05T21:02:37.391256-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-2rk","title":"Unit Tests: http.py - Rate Limiting","description":"priority: 3","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.480753-05:00","updated_at":"2026-01-05T21:00:47.069147-05:00","deleted_at":"2026-01-05T21:00:47.069147-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-2uf","title":"Phase 4: Cleanup and Polish","description":"# Phase 4: Low Priority Optimizations (Marginal Gains)\n\n## Scope\nThis phase addresses minor inefficiencies and cleanup items that provide marginal\nperformance benefits but improve code quality and prevent future issues.\n\n## Issues Addressed\n\n### Memory and Resource Management\n1. **fetchall() vs streaming** - Loading entire result sets into memory\n2. **Process lock dict growth** - Never cleaned up, potential memory leak\n3. **Connection pool tuning** - Default settings may not be optimal\n\n### Query Optimization\n1. **LIMIT 10000 overfetching** - Fetching far more than needed\n2. **FTS5 trigger overhead** - Triggers fire on every insert/update\n\n### Configuration\n1. **PRAGMA tuning** - synchronous, journal_mode, cache_size\n2. **Pool size optimization** - Based on expected concurrency\n\n## Expected Impact\n- Memory usage: Reduced peak memory for large operations\n- Query efficiency: Marginal latency improvements\n- Stability: Prevent potential issues under sustained load\n\n## Why Low Priority\nThese items:\n- Have small absolute impact on typical workloads\n- Require careful testing to avoid regressions\n- May have complex trade-offs (e.g., PRAGMA synchronous affects durability)\n\n## Implementation Notes\n- Streaming requires refactoring callers to handle iterators\n- Pool tuning should be based on profiling data\n- PRAGMA changes need thorough testing for data integrity","status":"open","priority":3,"issue_type":"feature","created_at":"2026-01-12T01:05:53.354148779-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:05:53.354148779-05:00","dependencies":[{"issue_id":"mcp_agent_mail-2uf","depends_on_id":"mcp_agent_mail-vih","type":"blocks","created_at":"2026-01-12T01:06:18.812119619-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-2uf","depends_on_id":"mcp_agent_mail-gt4","type":"blocks","created_at":"2026-01-12T01:13:12.103227533-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-2uf","depends_on_id":"mcp_agent_mail-b11","type":"blocks","created_at":"2026-01-12T01:13:12.125274015-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-2uf","depends_on_id":"mcp_agent_mail-21j","type":"blocks","created_at":"2026-01-12T01:13:12.1485374-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-2uf","depends_on_id":"mcp_agent_mail-6m1","type":"blocks","created_at":"2026-01-12T01:13:12.172238549-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-2uf","depends_on_id":"mcp_agent_mail-9zj","type":"blocks","created_at":"2026-01-12T01:13:12.194594685-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-2uf","depends_on_id":"mcp_agent_mail-kfo","type":"blocks","created_at":"2026-01-12T01:21:26.855664865-05:00","created_by":"ubuntu"}]}
{"id":"mcp_agent_mail-2x6","title":"Unit Tests: storage.py - Inbox/Outbox","description":"priority: 2","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.472327-05:00","updated_at":"2026-01-05T21:00:47.596135-05:00","deleted_at":"2026-01-05T21:00:47.596135-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-2xf","title":"Regression: Agent Name Validation","description":"priority: 0","status":"closed","priority":0,"issue_type":"task","created_at":"2026-01-05T21:02:17.831463-05:00","updated_at":"2026-01-05T21:43:01.516522-05:00","closed_at":"2026-01-05T21:43:01.516522-05:00","close_reason":"Completed: 38 regression tests for agent name validation covering validate_agent_name_format, generate_agent_name, sanitize_agent_name, and MCP integration flows in both coerce and strict modes"}
{"id":"mcp_agent_mail-3ph","title":"Test Coverage: Achieve 80% Overall","description":"priority: 2","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.488404-05:00","updated_at":"2026-01-05T21:00:45.374664-05:00","deleted_at":"2026-01-05T21:00:45.374664-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-3x5","title":"CLI: Guard Commands","description":"priority: 2","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-05T21:02:17.858346-05:00","updated_at":"2026-01-06T00:13:06.442902-05:00","closed_at":"2026-01-06T00:13:06.442902-05:00","close_reason":"26 tests pass for CLI Guard Commands (status, install, uninstall, check)","dependencies":[{"issue_id":"mcp_agent_mail-3x5","depends_on_id":"mcp_agent_mail-aew","type":"blocks","created_at":"2026-01-05T21:02:54.828161-05:00","created_by":"jemanuel"}]}
{"id":"mcp_agent_mail-47v","title":"Unit Tests: db.py - Session Management","description":"priority: 2","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.468243-05:00","updated_at":"2026-01-05T21:00:48.679337-05:00","deleted_at":"2026-01-05T21:00:48.679337-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-48a","title":"Unit Tests: cli.py - Guard Commands","description":"priority: 2","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.472972-05:00","updated_at":"2026-01-05T21:00:47.425206-05:00","deleted_at":"2026-01-05T21:00:47.425206-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-4em","title":"Performance: Baseline Benchmarks","description":"priority: 3","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-05T21:02:17.877976-05:00","updated_at":"2026-01-06T02:13:34.992751-05:00","closed_at":"2026-01-06T02:13:34.992751-05:00","close_reason":"Added MCP tool latency benchmarks: message send, inbox fetch, search, file reservation conflict check, archive write, and summary report. Tests use unique project keys and create_agent_identity for proper isolation. All tests pass with p95 thresholds.","dependencies":[{"issue_id":"mcp_agent_mail-4em","depends_on_id":"mcp_agent_mail-yzu","type":"blocks","created_at":"2026-01-05T21:02:55.70686-05:00","created_by":"jemanuel"}]}
{"id":"mcp_agent_mail-4qc","title":"Unit Tests: cli.py - Archive Commands","description":"priority: 2","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.47394-05:00","updated_at":"2026-01-05T21:00:47.326503-05:00","deleted_at":"2026-01-05T21:00:47.326503-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-4yy","title":"Add composite database indexes for common query patterns","description":"# Add Composite Database Indexes\n\n## Problem Statement\nThe database has individual column indexes but lacks composite indexes that cover\ncommon multi-column query patterns, forcing table scans and index merges.\n\n## Code Location\n`src/mcp_agent_mail/db.py` - Schema definitions section\n\n## Current State\nIndividual indexes likely exist on primary keys and foreign keys, but composite\nindexes for common WHERE clause combinations are missing.\n\n## Required Indexes\n\n### 1. message_recipients - Project + Recipient Name\n```python\nIndex(\"ix_msgrecip_project_recipient\", \"project_id\", \"recipient_name\")\n```\n**Justification**: Inbox queries filter by project_id AND recipient_name.\nWithout this, requires index merge or table scan.\n\n### 2. file_reservations - Project + Agent + Active Status\n```python\nIndex(\"ix_fileresv_project_agent_active\", \"project_id\", \"agent_id\", \"released_ts\")\n```\n**Justification**: Reservation conflict checks query active reservations\n(released_ts IS NULL) for a project. This covers that pattern.\n\n### 3. agent_links - Project + Target Agent + Status\n```python\nIndex(\"ix_agentlink_project_target_status\", \"project_id\", \"to_agent_id\", \"status\")\n```\n**Justification**: Contact policy lookups filter by project, target, and status.\n\n### 4. messages - Project + Created Timestamp (Descending)\n```python\nIndex(\"ix_msg_project_created_desc\", \"project_id\", \"created_ts\", postgresql_using=\"btree\")\n```\n**Justification**: Most message queries are \"recent messages in project\" which\nneeds project filtering with timestamp ordering.\n\n### 5. FTS5 Covering Index (if applicable)\nReview FTS5 configuration to ensure it includes project_id for filtering\nwithout requiring a JOIN back to the messages table.\n\n## Implementation Notes\n\n### SQLite Index Creation\n```python\nfrom sqlmodel import Index\n\nclass MessageRecipient(SQLModel, table=True):\n    # ... fields ...\n    \n    __table_args__ = (\n        Index(\"ix_msgrecip_project_recipient\", \"project_id\", \"recipient_name\"),\n    )\n```\n\n### Migration Strategy\nSince this project has no production users, we can modify the schema directly.\nIf there was existing data, we'd need an Alembic migration.\n\n## Testing Strategy\n1. Use EXPLAIN QUERY PLAN on common queries before/after\n2. Verify indexes are being used (no \"SCAN TABLE\" in plan)\n3. Benchmark query times with realistic data volumes\n\n## Verification Commands\n```sql\n-- Check existing indexes\nSELECT name, sql FROM sqlite_master WHERE type='index';\n\n-- Analyze query plan\nEXPLAIN QUERY PLAN SELECT * FROM message_recipients \nWHERE project_id = 1 AND recipient_name = 'TestAgent';\n```\n\n## Risk Assessment\n- **Risk**: Indexes slow down INSERT/UPDATE operations\n- **Mitigation**: These tables are read-heavy; read improvement \u003e\u003e write cost\n- **Monitoring**: Track insert times if concerned\n\n## Dependencies\nNone - can be done independently of N+1 fixes","status":"open","priority":0,"issue_type":"task","created_at":"2026-01-12T01:07:57.068827018-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:07:57.068827018-05:00","dependencies":[{"issue_id":"mcp_agent_mail-4yy","depends_on_id":"mcp_agent_mail-k92","type":"blocks","created_at":"2026-01-12T01:12:50.102341328-05:00","created_by":"ubuntu"}]}
{"id":"mcp_agent_mail-55z","title":"Test Coverage: Achieve 95% Overall","description":"priority: 3","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.488615-05:00","updated_at":"2026-01-05T21:00:45.283815-05:00","deleted_at":"2026-01-05T21:00:45.283815-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-59h","title":"Capture baseline performance metrics before optimization","description":"# Capture Baseline Performance Metrics\n\n## Purpose\nBefore ANY optimization changes, capture baseline metrics to enable before/after comparison.\nThis creates the \"before\" snapshot that proves optimizations actually improved things.\n\n## Prerequisites\n- Instrumentation infrastructure complete\n- Benchmarking suite complete\n- E2E tests complete (golden files saved)\n\n## Steps\n\n### 1. Run Full Benchmark Suite\n\nEnsure clean git state, then run benchmarks and save baseline using pytest with the benchmark-save flag.\n\n### 2. Capture Query Count Baselines\n\nCreate docs/performance/baseline-metrics.md with tables for:\n- Query counts per operation (send_message with 1/5/10 recipients, fetch_inbox, etc.)\n- Latency metrics (p50/p95/p99) for each operation\n- Memory usage per operation\n- Throughput measurements\n\n### 3. Save Query Logs\n\nRun each operation with ENABLE_QUERY_TRACKING=true and save the query reports to docs/performance/\n\n### 4. Create Baseline Tag\n\nCreate an annotated git tag marking this commit as the baseline, then push the tag to origin.\n\n## Verification Script (scripts/capture_baseline.py)\n\nCreate a Python script that:\n1. Uses track_queries() context manager to measure each operation\n2. Captures query counts, timing, and memory for varying input sizes\n3. Saves results as JSON to docs/performance/baselines/\n4. Prints summary to console with Rich formatting\n\nThe script should test:\n- send_message with 1, 5, and 10 recipients\n- fetch_inbox with 10, 50, and 100 messages\n- list_outbox with similar message counts\n- search_messages with various query patterns\n- summarize_thread with 5, 20, and 50 message threads\n\n## Acceptance Criteria\n- [ ] Benchmark results saved with timestamp\n- [ ] Query counts documented for all critical operations\n- [ ] Latency metrics (p50/p95/p99) recorded\n- [ ] Memory usage captured\n- [ ] Git tag created for baseline commit\n- [ ] Baseline document in docs/performance/\n\n## Dependencies\n- mcp_agent_mail-dbt (instrumentation infrastructure)\n- mcp_agent_mail-k92 (benchmarking suite)\n- mcp_agent_mail-ab8 (E2E test suite)\n\n## Blocks\n- Phase 1 tasks (need baseline before optimizing)","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-12T01:18:50.342856972-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:18:50.342856972-05:00","dependencies":[{"issue_id":"mcp_agent_mail-59h","depends_on_id":"mcp_agent_mail-k92","type":"blocks","created_at":"2026-01-12T01:20:16.957047623-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-59h","depends_on_id":"mcp_agent_mail-ab8","type":"blocks","created_at":"2026-01-12T01:20:16.988252994-05:00","created_by":"ubuntu"}]}
{"id":"mcp_agent_mail-5cx","title":"Unit Tests: app.py - Core Helpers","description":"priority: 2","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.468876-05:00","updated_at":"2026-01-05T21:00:48.514677-05:00","deleted_at":"2026-01-05T21:00:48.514677-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-6aq","title":"Test Coverage: Achieve 50% Overall","description":"priority: 1","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.488157-05:00","updated_at":"2026-01-05T21:00:45.462014-05:00","deleted_at":"2026-01-05T21:00:45.462014-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-6m1","title":"Review and tune SQLite PRAGMA settings","description":"# Review SQLite PRAGMA Settings\n\n## Problem Statement\nDefault SQLite settings may not be optimal for the MCP Agent Mail workload.\nTuning PRAGMAs can improve performance significantly.\n\n## Code Location\n`src/mcp_agent_mail/db.py` - Engine creation and initialization\n\n## Key PRAGMAs to Review\n\n### 1. journal_mode (Currently likely: DELETE)\n```sql\nPRAGMA journal_mode = WAL;\n```\n- WAL allows concurrent reads during writes\n- Better for read-heavy workloads (which this is)\n- Slight increase in disk usage\n\n### 2. synchronous (Currently likely: FULL)\n```sql\n-- For development/testing:\nPRAGMA synchronous = NORMAL;\n\n-- For production with data safety:\nPRAGMA synchronous = FULL;\n```\n- NORMAL is 2-10x faster but slightly less durable\n- FULL ensures data survives power loss\n- Consider making configurable\n\n### 3. cache_size (Currently likely: default ~2MB)\n```sql\nPRAGMA cache_size = -64000;  -- 64MB (negative = KB)\n```\n- Larger cache = fewer disk reads\n- Set based on available memory\n- Should be 10-25% of expected database size\n\n### 4. temp_store (Currently likely: DEFAULT)\n```sql\nPRAGMA temp_store = MEMORY;\n```\n- Keeps temp tables in memory\n- Faster sorts and joins\n- More memory usage\n\n### 5. mmap_size (Currently likely: 0)\n```sql\nPRAGMA mmap_size = 268435456;  -- 256MB\n```\n- Memory-map the database file\n- Can improve read performance\n- Platform-dependent behavior\n\n## Implementation\n```python\nasync def configure_sqlite_pragmas(engine: AsyncEngine):\n    \"\"\"Configure SQLite for optimal performance.\"\"\"\n    async with engine.begin() as conn:\n        await conn.execute(text(\"PRAGMA journal_mode = WAL\"))\n        await conn.execute(text(\"PRAGMA synchronous = NORMAL\"))\n        await conn.execute(text(\"PRAGMA cache_size = -64000\"))\n        await conn.execute(text(\"PRAGMA temp_store = MEMORY\"))\n        await conn.execute(text(\"PRAGMA mmap_size = 268435456\"))\n```\n\n## Configuration Approach\nMake PRAGMAs configurable via settings:\n\n```python\nclass DatabaseSettings:\n    sqlite_journal_mode: str = \"WAL\"\n    sqlite_synchronous: str = \"NORMAL\"  # NORMAL for dev, FULL for prod\n    sqlite_cache_size_kb: int = 64000\n    sqlite_mmap_size: int = 256 * 1024 * 1024\n```\n\n## Testing Strategy\n1. Benchmark with default settings\n2. Benchmark with optimized settings\n3. Verify data integrity with power-loss simulation\n4. Test on different platforms (Linux, macOS, Windows)\n\n## Risk Assessment\n- **journal_mode=WAL**: Low risk, widely used\n- **synchronous=NORMAL**: Medium risk, document for users\n- **cache_size**: Low risk, just memory tradeoff\n- **mmap_size**: Platform-dependent, test thoroughly\n\n## Verification\n```python\n# Check current settings\nasync with engine.begin() as conn:\n    for pragma in [\"journal_mode\", \"synchronous\", \"cache_size\", \"temp_store\"]:\n        result = await conn.execute(text(f\"PRAGMA {pragma}\"))\n        print(f\"{pragma}: {result.scalar()}\")\n```\n\n## Dependencies\nNone - can be done independently","status":"open","priority":3,"issue_type":"task","created_at":"2026-01-12T01:12:24.57380359-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:12:24.57380359-05:00"}
{"id":"mcp_agent_mail-7a4","title":"P3 - Security Tests","description":"Test security-sensitive areas.","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T21:02:17.869383-05:00","updated_at":"2026-01-05T21:02:37.650483-05:00","deleted_at":"2026-01-05T21:02:37.650483-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-7hm","title":"Create unit tests for all performance utility functions","description":"# Unit Tests for Performance Utilities\n\n## Purpose\nEach optimization introduces new utility functions or modifies existing ones.\nThese need comprehensive unit tests to ensure correctness.\n\n## Test Files to Create\n\n### 1. tests/test_agents_batch.py\n```python\n\"\"\"Tests for batched agent lookup function.\"\"\"\nimport pytest\nfrom mcp_agent_mail.app import _get_agents_batch\n\nclass TestGetAgentsBatch:\n    @pytest.mark.asyncio\n    async def test_empty_names_returns_empty_dict(self, session, project):\n        result = await _get_agents_batch(project, [])\n        assert result == {}\n    \n    @pytest.mark.asyncio\n    async def test_single_name_returns_agent(self, session, project, agent):\n        result = await _get_agents_batch(project, [agent.name])\n        assert agent.name in result\n        assert result[agent.name].id == agent.id\n    \n    @pytest.mark.asyncio\n    async def test_multiple_names_returns_all(self, session, project, agents):\n        names = [a.name for a in agents]\n        result = await _get_agents_batch(project, names)\n        assert len(result) == len(agents)\n        for agent in agents:\n            assert agent.name in result\n    \n    @pytest.mark.asyncio\n    async def test_duplicate_names_handled(self, session, project, agent):\n        result = await _get_agents_batch(project, [agent.name, agent.name, agent.name])\n        assert len(result) == 1\n        assert agent.name in result\n    \n    @pytest.mark.asyncio\n    async def test_nonexistent_names_not_in_result(self, session, project, agent):\n        result = await _get_agents_batch(project, [agent.name, \"NonExistent\"])\n        assert len(result) == 1\n        assert agent.name in result\n        assert \"NonExistent\" not in result\n    \n    @pytest.mark.asyncio\n    async def test_query_count_is_one(self, session, project, agents, query_tracker):\n        names = [a.name for a in agents]\n        async with track_queries() as stats:\n            await _get_agents_batch(project, names)\n        \n        # Should be exactly 1 query regardless of name count\n        assert stats.count == 1, f\"Expected 1 query, got {stats.count}\"\n```\n\n### 2. tests/test_lru_cache.py\n```python\n\"\"\"Tests for O(1) LRU cache implementation.\"\"\"\nimport pytest\nimport time\nfrom mcp_agent_mail.storage import SimpleLRUCache\n\nclass TestSimpleLRUCache:\n    def test_get_returns_none_for_missing(self):\n        cache = SimpleLRUCache(maxsize=10)\n        assert cache.get(\"missing\") is None\n    \n    def test_set_and_get(self):\n        cache = SimpleLRUCache(maxsize=10)\n        cache.set(\"key\", \"value\")\n        assert cache.get(\"key\") == \"value\"\n    \n    def test_evicts_oldest_when_full(self):\n        cache = SimpleLRUCache(maxsize=3)\n        cache.set(\"a\", 1)\n        cache.set(\"b\", 2)\n        cache.set(\"c\", 3)\n        cache.set(\"d\", 4)  # Should evict \"a\"\n        \n        assert cache.get(\"a\") is None\n        assert cache.get(\"b\") == 2\n        assert cache.get(\"c\") == 3\n        assert cache.get(\"d\") == 4\n    \n    def test_get_updates_recency(self):\n        cache = SimpleLRUCache(maxsize=3)\n        cache.set(\"a\", 1)\n        cache.set(\"b\", 2)\n        cache.set(\"c\", 3)\n        \n        cache.get(\"a\")  # Touch \"a\", making it most recent\n        cache.set(\"d\", 4)  # Should evict \"b\" (oldest untouched)\n        \n        assert cache.get(\"a\") == 1  # Still there\n        assert cache.get(\"b\") is None  # Evicted\n    \n    def test_performance_is_o1(self):\n        \"\"\"Verify operations are O(1) not O(n).\"\"\"\n        cache = SimpleLRUCache(maxsize=10000)\n        \n        # Populate cache\n        for i in range(10000):\n            cache.set(f\"key{i}\", i)\n        \n        # Time 10000 get operations\n        start = time.perf_counter()\n        for i in range(10000):\n            cache.get(f\"key{i % 10000}\")\n        elapsed = time.perf_counter() - start\n        \n        # With O(1), should be \u003c 0.1s\n        # With O(n), would be \u003e 10s\n        assert elapsed \u003c 0.5, f\"Operations too slow: {elapsed:.2f}s (likely O(n))\"\n    \n    def test_contains(self):\n        cache = SimpleLRUCache(maxsize=10)\n        cache.set(\"key\", \"value\")\n        assert \"key\" in cache\n        assert \"missing\" not in cache\n    \n    def test_clear(self):\n        cache = SimpleLRUCache(maxsize=10)\n        cache.set(\"a\", 1)\n        cache.set(\"b\", 2)\n        cache.clear()\n        assert cache.get(\"a\") is None\n        assert cache.get(\"b\") is None\n```\n\n### 3. tests/test_pathspec_cache.py\n```python\n\"\"\"Tests for PathSpec compilation caching.\"\"\"\nimport pytest\nfrom mcp_agent_mail.app import _compile_pathspec, _patterns_overlap\n\nclass TestPathSpecCache:\n    def test_returns_pathspec(self):\n        spec = _compile_pathspec(\"src/**/*.py\")\n        assert spec is not None\n    \n    def test_cache_hit_returns_same_object(self):\n        _compile_pathspec.cache_clear()\n        spec1 = _compile_pathspec(\"src/**/*.py\")\n        spec2 = _compile_pathspec(\"src/**/*.py\")\n        assert spec1 is spec2  # Same object from cache\n    \n    def test_different_patterns_different_objects(self):\n        spec1 = _compile_pathspec(\"src/**/*.py\")\n        spec2 = _compile_pathspec(\"tests/**/*.py\")\n        assert spec1 is not spec2\n    \n    def test_cache_stats(self):\n        _compile_pathspec.cache_clear()\n        \n        # First call - miss\n        _compile_pathspec(\"pattern1\")\n        info = _compile_pathspec.cache_info()\n        assert info.misses == 1\n        assert info.hits == 0\n        \n        # Second call same pattern - hit\n        _compile_pathspec(\"pattern1\")\n        info = _compile_pathspec.cache_info()\n        assert info.hits == 1\n    \n    def test_patterns_overlap_uses_cache(self):\n        _compile_pathspec.cache_clear()\n        \n        # Call overlap check multiple times with same patterns\n        for _ in range(100):\n            _patterns_overlap(\"src/**\", \"src/api/**\")\n        \n        info = _compile_pathspec.cache_info()\n        # Should have 2 misses (for the 2 patterns) and 198 hits\n        assert info.misses == 2\n        assert info.hits == 198\n```\n\n### 4. tests/test_heapq_topn.py\n```python\n\"\"\"Tests for heapq.nlargest usage.\"\"\"\nimport pytest\nimport heapq\nimport random\n\ndef get_top_mentions(mentions: dict, n: int = 10) -\u003e list:\n    \"\"\"Example function using heapq.nlargest.\"\"\"\n    return heapq.nlargest(n, mentions.items(), key=lambda x: x[1])\n\nclass TestHeapqTopN:\n    def test_returns_top_n(self):\n        mentions = {\"a\": 5, \"b\": 10, \"c\": 3, \"d\": 8, \"e\": 1}\n        top3 = get_top_mentions(mentions, 3)\n        \n        assert len(top3) == 3\n        assert top3[0] == (\"b\", 10)\n        assert top3[1] == (\"d\", 8)\n        assert top3[2] == (\"a\", 5)\n    \n    def test_handles_fewer_than_n(self):\n        mentions = {\"a\": 5, \"b\": 10}\n        top5 = get_top_mentions(mentions, 5)\n        \n        assert len(top5) == 2\n    \n    def test_matches_sorted_slice(self):\n        # Verify heapq matches sorted()[:n]\n        mentions = {f\"item{i}\": random.randint(0, 1000) for i in range(1000)}\n        \n        heap_result = get_top_mentions(mentions, 10)\n        sorted_result = sorted(mentions.items(), key=lambda x: -x[1])[:10]\n        \n        assert heap_result == sorted_result\n```\n\n## Test Running Commands\n```bash\n# Run all performance unit tests\npytest tests/test_agents_batch.py tests/test_lru_cache.py tests/test_pathspec_cache.py -v\n\n# Run with coverage\npytest tests/test_*.py --cov=mcp_agent_mail --cov-report=html\n```\n\n## Acceptance Criteria\n- [ ] All utility functions have comprehensive unit tests\n- [ ] Tests verify correctness AND performance characteristics\n- [ ] Query count assertions where applicable\n- [ ] All tests pass\n- [ ] Coverage \u003e 90% for new code\n\n## Dependencies\n- mcp_agent_mail-dbt (instrumentation for query counting)\n\n## Blocks\n- Provides confidence for all optimization implementations","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-12T01:20:56.862839163-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:20:56.862839163-05:00","dependencies":[{"issue_id":"mcp_agent_mail-7hm","depends_on_id":"mcp_agent_mail-dbt","type":"blocks","created_at":"2026-01-12T01:21:02.380860652-05:00","created_by":"ubuntu"}]}
{"id":"mcp_agent_mail-7pp","title":"Integration Tests: Full Messaging Flow","description":"priority: 1","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.484043-05:00","updated_at":"2026-01-05T21:00:46.467406-05:00","deleted_at":"2026-01-05T21:00:46.467406-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-7ue","title":"Add serve-stdio command for stdio transport","description":"Add CLI command to run MCP server via stdio transport (requested in GitHub #41). This enables project-local installation patterns where Claude Code launches the server directly.","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-06T23:39:36.679856-05:00","created_by":"jemanuel","updated_at":"2026-01-06T23:45:11.386347-05:00","closed_at":"2026-01-06T23:45:11.386347-05:00","close_reason":"Implemented serve-stdio command with test"}
{"id":"mcp_agent_mail-88l","title":"Unit Tests: llm.py - Provider Integration","description":"priority: 3","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.483764-05:00","updated_at":"2026-01-05T21:00:46.562021-05:00","deleted_at":"2026-01-05T21:00:46.562021-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-8hr","title":"Guards: Pre-push Enforcement","description":"priority: 2","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-05T21:02:17.868497-05:00","updated_at":"2026-01-06T00:21:17.959806-05:00","closed_at":"2026-01-06T00:21:17.959806-05:00","close_reason":"Added 20 comprehensive pre-push enforcement tests covering hook rendering, env controls, conflict detection, pattern matching, multi-commit scenarios, and edge cases. All tests pass.","dependencies":[{"issue_id":"mcp_agent_mail-8hr","depends_on_id":"mcp_agent_mail-irp","type":"blocks","created_at":"2026-01-05T21:02:55.109306-05:00","created_by":"jemanuel"}]}
{"id":"mcp_agent_mail-8pg","title":"Phase 4 verification checkpoint - final validation and documentation","description":"# Phase 4 Verification Checkpoint (Final)\n\n## Purpose\nAfter completing all Phase 4 tasks, perform final verification and generate\ncomprehensive documentation of all performance improvements.\n\n## Verification Steps\n\n### 1. Run Complete Test Suite\n```bash\n# All tests must pass\npytest tests/ -v\n\n# E2E correctness\npytest tests/e2e/ -v\n\n# Benchmarks\npytest tests/benchmarks/ -v\n```\n\n### 2. Verify Phase 4 Specific Fixes\n\n```python\n# Streaming for large results\nasync def test_streaming_memory():\n    # Fetch 100k rows\n    tracemalloc.start()\n    async for row in stream_large_query():\n        process(row)\n    peak = tracemalloc.get_traced_memory()[1]\n    tracemalloc.stop()\n    \n    # Peak should be \u003c\u003c 100k * row_size\n    assert peak \u003c 10 * 1024 * 1024  # \u003c 10MB\n\n# Lock cleanup\ndef test_lock_dict_bounded():\n    lock_manager = LockManager()\n    for i in range(10000):\n        lock_manager.get(f\"key-{i}\")\n    \n    # Should be bounded, not 10000 entries\n    assert len(lock_manager._locks) \u003c= 1000\n\n# LIMIT enforcement\nasync def test_query_limits():\n    # Try to fetch unlimited\n    result = await list_messages(limit=999999)\n    \n    # Should be capped\n    assert len(result) \u003c= 1000\n\n# PRAGMA settings\nasync def test_pragma_applied():\n    async with engine.begin() as conn:\n        result = await conn.execute(text(\"PRAGMA journal_mode\"))\n        assert result.scalar() == \"wal\"\n```\n\n### 3. Generate Final Performance Report\n\nCreate docs/performance/optimization-complete.md:\n\n```markdown\n# Performance Optimization Complete\n\n## Executive Summary\n- Total query count reduction: X%\n- Average latency improvement: Xx\n- Memory usage reduction: X%\n\n## Phase-by-Phase Results\n\n### Phase 1: N+1 Elimination\n| Metric | Before | After | Improvement |\n|--------|--------|-------|-------------|\n| send_message queries | N | 2 | (N-2)/N % |\n| ... | ... | ... | ... |\n\n### Phase 2: Caching \u0026 Batching\n...\n\n### Phase 3: Algorithms\n...\n\n### Phase 4: Cleanup\n...\n\n## Verification Evidence\n- Commit: \u003cfinal hash\u003e\n- E2E tests: X/X passing\n- Benchmark comparison: [link]\n- All golden files match\n\n## Future Recommendations\n- Monitor query counts in production\n- Set up performance regression alerts\n- Consider additional optimizations for X\n```\n\n### 4. Create Performance Monitoring Recommendations\n\nDocument how to maintain performance going forward:\n- CI benchmark comparison\n- Query count assertions in critical tests\n- Memory profiling in load tests\n\n## Acceptance Criteria\n- [ ] ALL tests pass (unit, E2E, benchmarks)\n- [ ] Final report generated with all metrics\n- [ ] Performance improvements documented\n- [ ] Baseline vs final comparison complete\n- [ ] Recommendations for monitoring documented\n\n## Dependencies\n- Phase 3 verification complete\n- All Phase 4 tasks complete\n\n## Blocks\n- Epic completion (mcp_agent_mail-nwz)","status":"open","priority":3,"issue_type":"task","created_at":"2026-01-12T01:19:54.351739379-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:19:54.351739379-05:00","dependencies":[{"issue_id":"mcp_agent_mail-8pg","depends_on_id":"mcp_agent_mail-kfo","type":"blocks","created_at":"2026-01-12T01:20:21.040427679-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-8pg","depends_on_id":"mcp_agent_mail-gt4","type":"blocks","created_at":"2026-01-12T01:20:21.071761542-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-8pg","depends_on_id":"mcp_agent_mail-b11","type":"blocks","created_at":"2026-01-12T01:20:21.100692356-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-8pg","depends_on_id":"mcp_agent_mail-21j","type":"blocks","created_at":"2026-01-12T01:20:21.131102071-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-8pg","depends_on_id":"mcp_agent_mail-6m1","type":"blocks","created_at":"2026-01-12T01:20:21.16093396-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-8pg","depends_on_id":"mcp_agent_mail-9zj","type":"blocks","created_at":"2026-01-12T01:20:21.191280716-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-8pg","depends_on_id":"mcp_agent_mail-tty","type":"blocks","created_at":"2026-01-12T01:27:51.961222248-05:00","created_by":"ubuntu"}]}
{"id":"mcp_agent_mail-8rr","title":"Unit Tests: app.py - MCP Resources","description":"priority: 2","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.470814-05:00","updated_at":"2026-01-05T21:00:47.866436-05:00","deleted_at":"2026-01-05T21:00:47.866436-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-9bz","title":"Milestone: Full Integration Coverage","description":"priority: 2","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-05T21:02:17.881845-05:00","updated_at":"2026-01-06T01:04:04.832257-05:00","closed_at":"2026-01-06T01:04:04.832257-05:00","close_reason":"Full Integration Coverage achieved: CLI Guard Commands (26), HTTP Server/Transport (19), CLI Mail Commands (29), plus Critical Path Coverage milestone complete. All 74 core tests passing.","dependencies":[{"issue_id":"mcp_agent_mail-9bz","depends_on_id":"mcp_agent_mail-tm6","type":"blocks","created_at":"2026-01-05T21:02:57.199388-05:00","created_by":"jemanuel"},{"issue_id":"mcp_agent_mail-9bz","depends_on_id":"mcp_agent_mail-n6z","type":"blocks","created_at":"2026-01-05T21:02:57.30292-05:00","created_by":"jemanuel"},{"issue_id":"mcp_agent_mail-9bz","depends_on_id":"mcp_agent_mail-3x5","type":"blocks","created_at":"2026-01-05T21:02:57.404731-05:00","created_by":"jemanuel"},{"issue_id":"mcp_agent_mail-9bz","depends_on_id":"mcp_agent_mail-9z6","type":"blocks","created_at":"2026-01-05T21:02:57.494389-05:00","created_by":"jemanuel"}]}
{"id":"mcp_agent_mail-9he","title":"Unit Tests: app.py - Messaging","description":"priority: 2","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.469601-05:00","updated_at":"2026-01-05T21:00:48.26081-05:00","deleted_at":"2026-01-05T21:00:48.26081-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-9sh","title":"Unit Tests: app.py - MCP Tools","description":"priority: 1","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.471023-05:00","updated_at":"2026-01-05T21:00:47.765853-05:00","deleted_at":"2026-01-05T21:00:47.765853-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-9tj","title":"Add asyncio.Lock to rate limiter bucket access","description":"# Add Synchronization to Rate Limiter\n\n## Problem Statement\nThe token bucket rate limiter accesses shared state (bucket dict) without\nsynchronization, leading to potential race conditions under concurrent load.\n\n## Code Location\n`src/mcp_agent_mail/http.py` - Lines approximately 306-314\n\n## Current Code (THE PROBLEM)\n```python\nclass TokenBucketRateLimiter:\n    def __init__(self, rate: float, burst: int):\n        self._rate = rate\n        self._burst = burst\n        self._buckets: dict[str, tuple[float, float]] = {}  # No lock!\n    \n    async def check(self, key: str) -\u003e bool:\n        now = time.monotonic()\n        tokens, last_ts = self._buckets.get(key, (float(self._burst), now))\n        \n        # Time-based token replenishment\n        elapsed = now - last_ts\n        tokens = min(float(self._burst), tokens + elapsed * self._rate)\n        \n        if tokens \u003e= 1.0:\n            tokens -= 1.0\n            self._buckets[key] = (tokens, now)  # Race condition!\n            return True\n        \n        self._buckets[key] = (tokens, now)\n        return False\n```\n\n## Race Condition Scenario\n1. Request A reads bucket: tokens=1.0\n2. Request B reads bucket: tokens=1.0 (before A writes)\n3. Request A writes: tokens=0.0\n4. Request B writes: tokens=0.0\n5. Both requests allowed, but should have been 1 allowed, 1 denied!\n\n## Impact Analysis\n- Under concurrent load, rate limits can be exceeded\n- Security/DoS implications if rate limiting is for protection\n- Subtle bug - may only manifest under high concurrency\n\n## Required Fix\n```python\nimport asyncio\n\nclass TokenBucketRateLimiter:\n    def __init__(self, rate: float, burst: int):\n        self._rate = rate\n        self._burst = burst\n        self._buckets: dict[str, tuple[float, float]] = {}\n        self._lock = asyncio.Lock()  # Add lock\n    \n    async def check(self, key: str) -\u003e bool:\n        async with self._lock:  # Synchronize access\n            now = time.monotonic()\n            tokens, last_ts = self._buckets.get(key, (float(self._burst), now))\n            \n            elapsed = now - last_ts\n            tokens = min(float(self._burst), tokens + elapsed * self._rate)\n            \n            if tokens \u003e= 1.0:\n                tokens -= 1.0\n                self._buckets[key] = (tokens, now)\n                return True\n            \n            self._buckets[key] = (tokens, now)\n            return False\n```\n\n## Performance Consideration\nA single global lock may be too coarse if there are many independent keys.\nConsider per-key locking for better concurrency:\n\n```python\nclass TokenBucketRateLimiter:\n    def __init__(self, rate: float, burst: int):\n        self._rate = rate\n        self._burst = burst\n        self._buckets: dict[str, tuple[float, float]] = {}\n        self._locks: dict[str, asyncio.Lock] = {}\n        self._locks_lock = asyncio.Lock()  # Lock for creating locks\n    \n    async def _get_lock(self, key: str) -\u003e asyncio.Lock:\n        if key not in self._locks:\n            async with self._locks_lock:\n                if key not in self._locks:  # Double-check\n                    self._locks[key] = asyncio.Lock()\n        return self._locks[key]\n    \n    async def check(self, key: str) -\u003e bool:\n        lock = await self._get_lock(key)\n        async with lock:\n            # ... rate limit logic\n```\n\n## Testing Strategy\n1. Unit test: Single request succeeds\n2. Unit test: Burst requests up to limit succeed\n3. Unit test: Request after burst denied\n4. Concurrency test: Many concurrent requests don't exceed limit\n5. Stress test: High concurrency doesn't cause errors\n\n## Verification\n```python\nimport asyncio\n\nlimiter = TokenBucketRateLimiter(rate=10, burst=10)\n\nasync def hammer(n_requests: int) -\u003e int:\n    \"\"\"Make many concurrent requests, count allowed.\"\"\"\n    tasks = [limiter.check(\"test-key\") for _ in range(n_requests)]\n    results = await asyncio.gather(*tasks)\n    return sum(results)\n\n# Should allow at most burst (10) requests\nallowed = asyncio.run(hammer(100))\nassert allowed \u003c= 10, f\"Rate limit exceeded: {allowed} \u003e 10\"\n```\n\n## Dependencies\nNone - self-contained fix","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-12T01:10:52.411836481-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:10:52.411836481-05:00"}
{"id":"mcp_agent_mail-9z5","title":"HTTP: Rate Limiting","description":"priority: 2","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-05T21:02:17.865169-05:00","updated_at":"2026-01-06T00:43:39.507538-05:00","closed_at":"2026-01-06T00:43:39.507538-05:00","close_reason":"Added 13 comprehensive rate limiting tests covering token bucket, burst, refill, Redis backend, per-identity keying, and edge cases. All tests pass."}
{"id":"mcp_agent_mail-9z6","title":"HTTP: Server and Transport","description":"priority: 2","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-05T21:02:17.86218-05:00","updated_at":"2026-01-06T00:27:46.401925-05:00","closed_at":"2026-01-06T00:27:46.401925-05:00","close_reason":"19 HTTP server/transport tests implemented and passing: config, health, SSE, tool calls, resources, CORS, error handling"}
{"id":"mcp_agent_mail-9zj","title":"Tune connection pool size based on workload","description":"# Tune Connection Pool Size\n\n## Problem Statement\nDefault connection pool settings may not be optimal for expected concurrency\nlevels, leading to either connection exhaustion or wasted resources.\n\n## Code Location\n`src/mcp_agent_mail/db.py` - async_sessionmaker and engine creation\n\n## Current Settings (Likely Defaults)\n```python\nengine = create_async_engine(\n    database_url,\n    # SQLAlchemy defaults:\n    # pool_size=5\n    # max_overflow=10\n    # pool_timeout=30\n    # pool_recycle=-1\n)\n```\n\n## Impact Analysis\n- **pool_size too small**: Connection exhaustion, timeouts under load\n- **pool_size too large**: Wasted memory, file descriptors\n- **max_overflow too small**: Can't handle burst traffic\n- **pool_timeout too short**: Spurious timeout errors\n\n## Recommended Settings\n```python\n# Calculate based on expected concurrency\n# Rule of thumb: pool_size = max_concurrent_requests / 2\n\nengine = create_async_engine(\n    database_url,\n    pool_size=10,           # Base connections always open\n    max_overflow=20,        # Additional connections for bursts\n    pool_timeout=30,        # Wait time before timeout\n    pool_recycle=3600,      # Recycle connections hourly\n    pool_pre_ping=True,     # Verify connection before use\n    echo=False,             # Disable SQL logging (set True for debug)\n)\n```\n\n## Configuration Approach\nMake pool settings configurable:\n\n```python\nfrom pydantic import BaseSettings\n\nclass DatabaseSettings(BaseSettings):\n    pool_size: int = 10\n    max_overflow: int = 20\n    pool_timeout: int = 30\n    pool_recycle: int = 3600\n    \n    class Config:\n        env_prefix = \"DB_\"\n\n\nsettings = DatabaseSettings()\n\nengine = create_async_engine(\n    database_url,\n    pool_size=settings.pool_size,\n    max_overflow=settings.max_overflow,\n    pool_timeout=settings.pool_timeout,\n    pool_recycle=settings.pool_recycle,\n)\n```\n\n## Monitoring\nAdd connection pool metrics:\n\n```python\nfrom sqlalchemy import event\n\n@event.listens_for(engine.sync_engine, \"checkout\")\ndef on_checkout(dbapi_conn, connection_record, connection_proxy):\n    pool = engine.sync_engine.pool\n    logger.debug(\n        f\"Pool: size={pool.size()}, \"\n        f\"checked_out={pool.checkedout()}, \"\n        f\"overflow={pool.overflow()}\"\n    )\n```\n\n## Testing Strategy\n1. Load test with default settings, observe pool behavior\n2. Load test with tuned settings, compare\n3. Test connection exhaustion scenario\n4. Test burst traffic handling\n\n## Pool Size Guidelines\n| Concurrent Users | pool_size | max_overflow |\n|------------------|-----------|--------------|\n| 1-10             | 5         | 10           |\n| 10-50            | 10        | 20           |\n| 50-100           | 20        | 40           |\n| 100+             | 50        | 100          |\n\n## Dependencies\nNone - configuration change","status":"open","priority":3,"issue_type":"task","created_at":"2026-01-12T01:12:24.745350248-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:12:24.745350248-05:00"}
{"id":"mcp_agent_mail-ab8","title":"Create E2E correctness validation test suite for proving isomorphism","description":"# E2E Correctness Validation Test Suite\n\n## Purpose\nBefore optimizing any code, we need tests that capture the EXACT outputs of operations.\nAfter optimization, we run the same tests to PROVE the optimization is isomorphic\n(produces identical outputs for identical inputs).\n\n## Why This Is Critical\n- Performance optimizations must not change behavior\n- \"It seems to work\" is not proof - we need automated validation\n- Regression prevention for all future changes\n- Documents expected behavior as executable specifications\n\n## Test Categories\n\n### 1. Message Operations (tests/e2e/test_message_correctness.py)\n\n```python\n\"\"\"E2E correctness tests for message operations.\n\nThese tests capture exact outputs to verify optimizations don't change behavior.\nRun BEFORE and AFTER any optimization to prove isomorphism.\n\"\"\"\nimport json\nimport pytest\nfrom datetime import datetime, timezone\nfrom pathlib import Path\n\n# Golden file directory for expected outputs\nGOLDEN_DIR = Path(__file__).parent / \"golden\"\n\n\ndef normalize_output(data: dict) -\u003e dict:\n    \"\"\"Normalize output for comparison (remove timestamps, IDs that vary).\"\"\"\n    normalized = data.copy()\n    \n    # Remove fields that legitimately vary between runs\n    fields_to_remove = [\"id\", \"created_ts\", \"updated_ts\", \"inception_ts\", \"last_active_ts\"]\n    \n    def remove_fields(obj):\n        if isinstance(obj, dict):\n            return {k: remove_fields(v) for k, v in obj.items() if k not in fields_to_remove}\n        elif isinstance(obj, list):\n            return [remove_fields(item) for item in obj]\n        return obj\n    \n    return remove_fields(normalized)\n\n\ndef save_golden(name: str, data: dict) -\u003e None:\n    \"\"\"Save golden file for future comparison.\"\"\"\n    GOLDEN_DIR.mkdir(parents=True, exist_ok=True)\n    path = GOLDEN_DIR / f\"{name}.json\"\n    normalized = normalize_output(data)\n    path.write_text(json.dumps(normalized, indent=2, sort_keys=True))\n\n\ndef load_golden(name: str) -\u003e dict:\n    \"\"\"Load golden file for comparison.\"\"\"\n    path = GOLDEN_DIR / f\"{name}.json\"\n    if not path.exists():\n        pytest.skip(f\"Golden file {path} not found. Run with --save-golden first.\")\n    return json.loads(path.read_text())\n\n\ndef assert_matches_golden(name: str, actual: dict, save: bool = False) -\u003e None:\n    \"\"\"Assert output matches golden file, optionally saving new golden.\"\"\"\n    normalized = normalize_output(actual)\n    \n    if save:\n        save_golden(name, actual)\n        return\n    \n    expected = load_golden(name)\n    \n    # Detailed diff on mismatch\n    if normalized != expected:\n        import difflib\n        actual_str = json.dumps(normalized, indent=2, sort_keys=True)\n        expected_str = json.dumps(expected, indent=2, sort_keys=True)\n        diff = \"\\n\".join(difflib.unified_diff(\n            expected_str.splitlines(),\n            actual_str.splitlines(),\n            fromfile=\"expected\",\n            tofile=\"actual\",\n            lineterm=\"\"\n        ))\n        pytest.fail(f\"Output differs from golden file:\\n{diff}\")\n\n\nclass TestSendMessageCorrectness:\n    \"\"\"Verify send_message produces correct outputs.\"\"\"\n    \n    @pytest.mark.asyncio\n    async def test_single_recipient_output(\n        self, \n        app_context,\n        sample_project,\n        sample_agents,\n        request,\n    ):\n        \"\"\"Send to single recipient - verify exact output structure.\"\"\"\n        result = await send_message(\n            project_key=sample_project.human_key,\n            sender_name=sample_agents[0].name,\n            to=[sample_agents[1].name],\n            subject=\"Test Subject\",\n            body_md=\"Test body content\",\n        )\n        \n        save = request.config.getoption(\"--save-golden\", default=False)\n        assert_matches_golden(\"send_message_single_recipient\", result, save=save)\n    \n    @pytest.mark.asyncio\n    async def test_multiple_recipients_output(\n        self,\n        app_context,\n        sample_project,\n        sample_agents,\n        request,\n    ):\n        \"\"\"Send to multiple TO/CC/BCC - verify exact output.\"\"\"\n        result = await send_message(\n            project_key=sample_project.human_key,\n            sender_name=sample_agents[0].name,\n            to=[sample_agents[1].name, sample_agents[2].name],\n            cc=[sample_agents[3].name],\n            bcc=[sample_agents[4].name],\n            subject=\"Multi-recipient Test\",\n            body_md=\"Testing multiple recipients\",\n        )\n        \n        save = request.config.getoption(\"--save-golden\", default=False)\n        assert_matches_golden(\"send_message_multi_recipient\", result, save=save)\n    \n    @pytest.mark.asyncio\n    async def test_reply_message_output(\n        self,\n        app_context,\n        sample_project,\n        sample_agents,\n        sample_message,  # Fixture that creates a message to reply to\n        request,\n    ):\n        \"\"\"Reply to message - verify threading works correctly.\"\"\"\n        result = await reply_message(\n            project_key=sample_project.human_key,\n            message_id=sample_message.id,\n            sender_name=sample_agents[1].name,\n            body_md=\"This is a reply\",\n        )\n        \n        save = request.config.getoption(\"--save-golden\", default=False)\n        assert_matches_golden(\"reply_message\", result, save=save)\n\n\nclass TestInboxOutboxCorrectness:\n    \"\"\"Verify inbox/outbox listing produces correct outputs.\"\"\"\n    \n    @pytest.mark.asyncio\n    async def test_fetch_inbox_output(\n        self,\n        app_context,\n        sample_project,\n        sample_agents,\n        populated_inbox,  # Fixture that sends several messages\n        request,\n    ):\n        \"\"\"Fetch inbox - verify message list structure.\"\"\"\n        result = await fetch_inbox(\n            project_key=sample_project.human_key,\n            agent_name=sample_agents[1].name,\n            limit=10,\n            include_bodies=True,\n        )\n        \n        save = request.config.getoption(\"--save-golden\", default=False)\n        assert_matches_golden(\"fetch_inbox\", result, save=save)\n    \n    @pytest.mark.asyncio\n    async def test_list_outbox_output(\n        self,\n        app_context,\n        sample_project,\n        sample_agents,\n        populated_inbox,\n        request,\n    ):\n        \"\"\"List outbox - verify sent message structure.\"\"\"\n        result = await list_outbox(\n            project_key=sample_project.human_key,\n            agent_name=sample_agents[0].name,\n            limit=10,\n        )\n        \n        save = request.config.getoption(\"--save-golden\", default=False)\n        assert_matches_golden(\"list_outbox\", result, save=save)\n\n\nclass TestSearchCorrectness:\n    \"\"\"Verify search produces correct outputs.\"\"\"\n    \n    @pytest.mark.asyncio\n    async def test_search_messages_output(\n        self,\n        app_context,\n        sample_project,\n        populated_inbox,\n        request,\n    ):\n        \"\"\"Search messages - verify result structure.\"\"\"\n        result = await search_messages(\n            project_key=sample_project.human_key,\n            query=\"Test\",\n            limit=10,\n        )\n        \n        save = request.config.getoption(\"--save-golden\", default=False)\n        assert_matches_golden(\"search_messages\", result, save=save)\n\n\nclass TestSummarizeCorrectness:\n    \"\"\"Verify thread summarization produces correct outputs.\"\"\"\n    \n    @pytest.mark.asyncio\n    async def test_summarize_thread_output(\n        self,\n        app_context,\n        sample_project,\n        sample_thread,  # Fixture with a multi-message thread\n        request,\n    ):\n        \"\"\"Summarize thread - verify summary structure.\"\"\"\n        result = await summarize_thread(\n            project_key=sample_project.human_key,\n            thread_id=sample_thread.thread_id,\n            include_examples=True,\n        )\n        \n        save = request.config.getoption(\"--save-golden\", default=False)\n        assert_matches_golden(\"summarize_thread\", result, save=save)\n```\n\n### 2. Pytest Configuration (tests/conftest.py additions)\n\n```python\ndef pytest_addoption(parser):\n    parser.addoption(\n        \"--save-golden\",\n        action=\"store_true\",\n        default=False,\n        help=\"Save current outputs as golden files (run before optimization)\",\n    )\n\n\n@pytest.fixture\nasync def app_context():\n    \"\"\"Setup complete application context for E2E tests.\"\"\"\n    # Initialize database, settings, etc.\n    ...\n\n\n@pytest.fixture\nasync def sample_project(app_context):\n    \"\"\"Create a sample project for testing.\"\"\"\n    return await ensure_project(human_key=\"/tmp/test-project\")\n\n\n@pytest.fixture\nasync def sample_agents(app_context, sample_project):\n    \"\"\"Create 5 sample agents for testing.\"\"\"\n    agents = []\n    for i in range(5):\n        agent = await register_agent(\n            project_key=sample_project.human_key,\n            program=\"test-agent\",\n            model=\"test-model\",\n            task_description=f\"Test agent {i}\",\n        )\n        agents.append(agent)\n    return agents\n\n\n@pytest.fixture\nasync def populated_inbox(app_context, sample_project, sample_agents):\n    \"\"\"Send several messages to populate inbox for testing.\"\"\"\n    sender = sample_agents[0]\n    recipient = sample_agents[1]\n    \n    for i in range(5):\n        await send_message(\n            project_key=sample_project.human_key,\n            sender_name=sender.name,\n            to=[recipient.name],\n            subject=f\"Test Message {i}\",\n            body_md=f\"This is test message number {i}\",\n        )\n    \n    return {\"sender\": sender, \"recipient\": recipient, \"count\": 5}\n\n\n@pytest.fixture\nasync def sample_thread(app_context, sample_project, sample_agents):\n    \"\"\"Create a multi-message thread for testing.\"\"\"\n    thread_id = \"TEST-THREAD-001\"\n    \n    # Initial message\n    msg1 = await send_message(\n        project_key=sample_project.human_key,\n        sender_name=sample_agents[0].name,\n        to=[sample_agents[1].name],\n        subject=\"Thread Test\",\n        body_md=\"Starting a thread\",\n        thread_id=thread_id,\n    )\n    \n    # Reply\n    msg2 = await reply_message(\n        project_key=sample_project.human_key,\n        message_id=msg1[\"id\"],\n        sender_name=sample_agents[1].name,\n        body_md=\"Replying to thread\",\n    )\n    \n    # Another reply\n    msg3 = await reply_message(\n        project_key=sample_project.human_key,\n        message_id=msg2[\"id\"],\n        sender_name=sample_agents[0].name,\n        body_md=\"Final message in thread\",\n    )\n    \n    return {\"thread_id\": thread_id, \"messages\": [msg1, msg2, msg3]}\n```\n\n### 3. Running E2E Tests\n\n```bash\n# BEFORE optimization: Save golden files\npytest tests/e2e/ --save-golden -v\n\n# AFTER optimization: Verify outputs match\npytest tests/e2e/ -v\n\n# If tests fail, you broke something!\n# If tests pass, optimization is proven isomorphic\n```\n\n## File Structure\n```\ntests/\n  e2e/\n    __init__.py\n    conftest.py\n    test_message_correctness.py\n    test_inbox_outbox_correctness.py\n    test_search_correctness.py\n    test_summarize_correctness.py\n    test_reservation_correctness.py\n    golden/\n      send_message_single_recipient.json\n      send_message_multi_recipient.json\n      fetch_inbox.json\n      ... etc\n```\n\n## Verification Workflow\n\n```\n1. Create instrumentation infrastructure\n2. Create E2E test suite (this task)\n3. Run: pytest tests/e2e/ --save-golden -v\n4. Commit golden files\n5. Run optimization (e.g., fix N+1)\n6. Run: pytest tests/e2e/ -v\n7. All tests pass → Optimization is isomorphic ✓\n8. Any test fails → Optimization changed behavior - FIX IT\n```\n\n## Acceptance Criteria\n- [ ] All critical operations have E2E tests\n- [ ] Golden file comparison works correctly\n- [ ] Timestamps/IDs properly normalized for comparison\n- [ ] Clear diff output on mismatch\n- [ ] --save-golden flag works\n- [ ] Fixtures create realistic test data\n- [ ] Tests can run independently\n\n## Dependencies\n- Instrumentation infrastructure (for query counting in tests)\n\n## Blocks\n- All Phase 1-4 optimization work (need correctness proof)","status":"open","priority":0,"issue_type":"task","created_at":"2026-01-12T01:18:13.841542732-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:18:13.841542732-05:00","dependencies":[{"issue_id":"mcp_agent_mail-ab8","depends_on_id":"mcp_agent_mail-dbt","type":"blocks","created_at":"2026-01-12T01:20:16.927525747-05:00","created_by":"ubuntu"}]}
{"id":"mcp_agent_mail-ada","title":"Unit Tests: app.py - Agent Operations","description":"priority: 2","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.469373-05:00","updated_at":"2026-01-05T21:00:48.350222-05:00","deleted_at":"2026-01-05T21:00:48.350222-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-aea","title":"Errors: Database Failures","description":"priority: 2","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-05T21:02:17.851404-05:00","updated_at":"2026-01-06T00:21:22.479073-05:00","closed_at":"2026-01-06T00:21:22.479073-05:00","close_reason":"Created tests/test_database_failures.py with 20 tests covering database auto-creation, retry_on_db_lock decorator, transaction rollback, session cleanup, and SQLite configuration (WAL mode, busy_timeout, synchronous mode)","dependencies":[{"issue_id":"mcp_agent_mail-aea","depends_on_id":"mcp_agent_mail-mm2","type":"blocks","created_at":"2026-01-05T21:02:54.533343-05:00","created_by":"jemanuel"}]}
{"id":"mcp_agent_mail-aew","title":"Core: File Reservation Lifecycle","description":"priority: 1","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-05T21:02:17.836703-05:00","updated_at":"2026-01-05T22:02:46.295934-05:00","closed_at":"2026-01-05T22:02:46.295934-05:00","close_reason":"17 tests (16 pass, 1 skip) covering: create exclusive/shared reservations, conflict detection (ex-ex, ex-sh, sh-sh), pattern overlap, TTL, manual release, renew, force release, multiple paths, git artifacts"}
{"id":"mcp_agent_mail-aid","title":"Fix N+1 pattern in _list_inbox() recipient fetching","description":"# Fix N+1 Query Pattern in _list_inbox()\n\n## Problem Statement\nSimilar to _list_outbox(), the inbox listing likely fetches recipients per-message,\ncausing N+1 query behavior.\n\n## Code Location\n`src/mcp_agent_mail/app.py` - Search for \"_list_inbox\" or \"fetch_inbox\"\n\n## Investigation Required\n1. Find the inbox listing implementation\n2. Verify it has the same N+1 pattern as outbox\n3. Apply the same fix pattern\n\n## Expected Fix Pattern\nSame as _list_outbox:\n1. Collect all message IDs\n2. Single query for all recipients: `WHERE message_id IN (...)`\n3. Group results by message_id using defaultdict\n4. Use pre-fetched data in the message loop\n\n## Testing Strategy\nSame as _list_outbox tests but for inbox endpoint.\n\n## Dependencies\n- Should be done alongside or after _list_outbox fix to ensure consistency\n\n## Note\nThis task exists because the analysis identified N+1 patterns in outbox listing.\nInbox almost certainly has the same pattern since the code is likely similar.\nVerify during implementation.","status":"open","priority":0,"issue_type":"task","created_at":"2026-01-12T01:07:10.291628635-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:07:10.291628635-05:00","dependencies":[{"issue_id":"mcp_agent_mail-aid","depends_on_id":"mcp_agent_mail-jxj","type":"blocks","created_at":"2026-01-12T01:13:19.430013059-05:00","created_by":"ubuntu"}]}
{"id":"mcp_agent_mail-aop","title":"Milestones","description":"---","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T21:02:17.88018-05:00","updated_at":"2026-01-05T21:02:38.054787-05:00","deleted_at":"2026-01-05T21:02:38.054787-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-aqs","title":"E2E: Multi-Agent Development Workflow","description":"priority: 4","status":"closed","priority":4,"issue_type":"task","created_at":"2026-01-05T21:02:17.878946-05:00","updated_at":"2026-01-06T02:13:22.627246-05:00","closed_at":"2026-01-06T02:13:22.627246-05:00","close_reason":"9 E2E tests implemented and passing (test_e2e_multi_agent_workflow.py)","dependencies":[{"issue_id":"mcp_agent_mail-aqs","depends_on_id":"mcp_agent_mail-n6z","type":"blocks","created_at":"2026-01-05T21:02:56.043931-05:00","created_by":"jemanuel"},{"issue_id":"mcp_agent_mail-aqs","depends_on_id":"mcp_agent_mail-irp","type":"blocks","created_at":"2026-01-05T21:02:56.345936-05:00","created_by":"jemanuel"}]}
{"id":"mcp_agent_mail-az3","title":"Implement read-write lock for schema operations","description":"# Implement Read-Write Lock for Schema Operations\n\n## Problem Statement\nSchema operations use a single coarse-grained lock, preventing concurrent\nreads even when no writes are happening.\n\n## Code Location\n`src/mcp_agent_mail/db.py` - Lines approximately 235-251\n\n## Current Code (THE PROBLEM)\n```python\n# Single lock for ALL schema operations\n_schema_lock = asyncio.Lock()\n\nasync def ensure_schema():\n    async with _schema_lock:  # Blocks ALL concurrent schema ops\n        # Check if schema exists\n        # Create tables if needed\n        pass\n\nasync def get_schema_version():\n    async with _schema_lock:  # Even reads block each other!\n        # Just reading version\n        pass\n```\n\n## Impact Analysis\n- Multiple agents checking schema version block each other\n- Read operations don't need exclusive access\n- Write operations (schema creation/migration) are rare\n- Current lock is write-lock for all operations\n\n## Required Fix\nImplement a read-write lock:\n\n```python\nimport asyncio\nfrom contextlib import asynccontextmanager\n\nclass AsyncRWLock:\n    \"\"\"Async read-write lock allowing concurrent reads, exclusive writes.\"\"\"\n    \n    def __init__(self):\n        self._read_count = 0\n        self._read_lock = asyncio.Lock()\n        self._write_lock = asyncio.Lock()\n    \n    @asynccontextmanager\n    async def read_lock(self):\n        \"\"\"Acquire read lock - multiple readers allowed.\"\"\"\n        async with self._read_lock:\n            self._read_count += 1\n            if self._read_count == 1:\n                await self._write_lock.acquire()\n        try:\n            yield\n        finally:\n            async with self._read_lock:\n                self._read_count -= 1\n                if self._read_count == 0:\n                    self._write_lock.release()\n    \n    @asynccontextmanager\n    async def write_lock(self):\n        \"\"\"Acquire write lock - exclusive access.\"\"\"\n        await self._write_lock.acquire()\n        try:\n            yield\n        finally:\n            self._write_lock.release()\n\n\n# Usage\n_schema_rwlock = AsyncRWLock()\n\nasync def ensure_schema():\n    async with _schema_rwlock.write_lock():  # Exclusive\n        # Schema modification\n        pass\n\nasync def get_schema_version():\n    async with _schema_rwlock.read_lock():  # Shared\n        # Just reading\n        pass\n```\n\n## RWLock Properties\n- Multiple readers can hold read_lock simultaneously\n- Writer waits for all readers to release\n- Readers wait for writer to release\n- Prevents writer starvation (new readers don't queue-jump waiting writer)\n\n## Alternative: Use Existing Library\n```python\nfrom aiorwlock import RWLock  # pip install aiorwlock\n\n_schema_lock = RWLock()\n\nasync def read_schema():\n    async with _schema_lock.reader:\n        ...\n\nasync def write_schema():\n    async with _schema_lock.writer:\n        ...\n```\n\n## Testing Strategy\n1. Multiple concurrent reads succeed simultaneously\n2. Write blocks new reads\n3. Reads block writes until complete\n4. No deadlocks under stress\n\n## Verification\n```python\nimport asyncio\nimport time\n\nrwlock = AsyncRWLock()\nread_times = []\n\nasync def reader(id: int):\n    start = time.perf_counter()\n    async with rwlock.read_lock():\n        await asyncio.sleep(0.1)  # Simulate read\n    read_times.append(time.perf_counter() - start)\n\n# Run 10 concurrent readers\nasyncio.run(asyncio.gather(*[reader(i) for i in range(10)]))\n\n# With RWLock: All readers finish in ~0.1s (parallel)\n# With plain Lock: Takes ~1.0s (serial)\ntotal_time = max(read_times)\nassert total_time \u003c 0.2, f\"Reads should be parallel: {total_time:.3f}s\"\n```\n\n## Dependencies\nNone - self-contained implementation","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-12T01:10:53.813736679-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:10:53.813736679-05:00"}
{"id":"mcp_agent_mail-b11","title":"Fix process lock dict memory leak - add cleanup","description":"# Fix Process Lock Dictionary Memory Leak\n\n## Problem Statement\nThe process lock dictionary grows unboundedly as new lock keys are added\nbut never removed, even after the operations complete.\n\n## Code Location\nSearch for lock dictionary usage in storage.py or db.py.\n\n## Current Code (THE PROBLEM)\n```python\n_process_locks: dict[str, asyncio.Lock] = {}\n\nasync def get_lock(key: str) -\u003e asyncio.Lock:\n    if key not in _process_locks:\n        _process_locks[key] = asyncio.Lock()\n    return _process_locks[key]\n\n# Locks are never removed!\n# After 10,000 unique keys, dict has 10,000 entries\n```\n\n## Impact Analysis\n- Each Lock object is ~100 bytes\n- 10,000 unique lock keys = ~1MB leaked\n- Over long-running server lifetime, can grow significantly\n- Also pollutes dict lookup performance\n\n## Required Fix Options\n\n### Option 1: WeakValueDictionary (Automatic Cleanup)\n```python\nfrom weakref import WeakValueDictionary\n\n_process_locks: WeakValueDictionary[str, asyncio.Lock] = WeakValueDictionary()\n\n# Locks automatically removed when no longer referenced\n```\n\n**Caveat**: Lock might be GC'd between get and use. Need careful handling.\n\n### Option 2: LRU Dict with Expiry\n```python\nfrom collections import OrderedDict\nimport time\n\nclass ExpiringLockDict:\n    def __init__(self, max_age: float = 3600, max_size: int = 10000):\n        self._locks: OrderedDict[str, tuple[asyncio.Lock, float]] = OrderedDict()\n        self._max_age = max_age\n        self._max_size = max_size\n    \n    def get(self, key: str) -\u003e asyncio.Lock:\n        self._cleanup()\n        if key in self._locks:\n            lock, _ = self._locks[key]\n            self._locks.move_to_end(key)\n            self._locks[key] = (lock, time.monotonic())\n            return lock\n        lock = asyncio.Lock()\n        self._locks[key] = (lock, time.monotonic())\n        return lock\n    \n    def _cleanup(self):\n        now = time.monotonic()\n        # Remove expired\n        while self._locks:\n            key, (lock, ts) = next(iter(self._locks.items()))\n            if now - ts \u003e self._max_age and not lock.locked():\n                self._locks.pop(key)\n            else:\n                break\n        # Remove oldest if over size\n        while len(self._locks) \u003e self._max_size:\n            key, (lock, _) = self._locks.popitem(last=False)\n            if lock.locked():\n                # Don't remove locked locks, put back\n                self._locks[key] = (lock, time.monotonic())\n                break\n```\n\n### Option 3: Explicit Release\n```python\n@asynccontextmanager\nasync def acquire_lock(key: str):\n    lock = _process_locks.setdefault(key, asyncio.Lock())\n    try:\n        await lock.acquire()\n        yield lock\n    finally:\n        lock.release()\n        # Clean up if no waiters\n        if key in _process_locks and not lock.locked():\n            del _process_locks[key]\n```\n\n## Recommended Approach\nOption 2 (LRU with expiry) is safest:\n- Bounded size prevents unbounded growth\n- Expiry handles inactive keys\n- Doesn't remove actively-used locks\n\n## Testing Strategy\n1. Create 10,000 unique lock keys\n2. Verify dict size stays bounded\n3. Verify old unused keys are cleaned up\n4. Verify active locks are not removed\n5. Memory profile over extended run\n\n## Verification\n```python\nlock_dict = ExpiringLockDict(max_age=1.0, max_size=100)\n\n# Create many locks\nfor i in range(1000):\n    lock_dict.get(f\"key-{i}\")\n\n# Size should be bounded\nassert len(lock_dict._locks) \u003c= 100\n\n# Wait for expiry\nawait asyncio.sleep(2.0)\nlock_dict.get(\"trigger-cleanup\")\n\n# Old locks should be cleaned\nassert len(lock_dict._locks) \u003c 50\n```\n\n## Dependencies\nNone - self-contained fix","status":"open","priority":3,"issue_type":"task","created_at":"2026-01-12T01:12:24.227385301-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:12:24.227385301-05:00"}
{"id":"mcp_agent_mail-b41","title":"Unit Tests: db.py - Migrations","description":"priority: 3","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.468601-05:00","updated_at":"2026-01-05T21:00:48.597636-05:00","deleted_at":"2026-01-05T21:00:48.597636-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-bko","title":"Unit Tests: http.py - Server Setup","description":"priority: 2","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.475868-05:00","updated_at":"2026-01-05T21:00:47.240944-05:00","deleted_at":"2026-01-05T21:00:47.240944-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-c2x","title":"Errors: Git Archive Failures","description":"priority: 2","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-05T21:02:17.85316-05:00","updated_at":"2026-01-06T00:36:13.482789-05:00","closed_at":"2026-01-06T00:36:13.482789-05:00","close_reason":"Added 21 comprehensive tests for Git archive failure handling covering auto-creation, repo initialization, concurrent writes, lock healing, file path sanitization, and large attachments. All tests pass.","dependencies":[{"issue_id":"mcp_agent_mail-c2x","depends_on_id":"mcp_agent_mail-mm2","type":"blocks","created_at":"2026-01-05T21:02:54.631277-05:00","created_by":"jemanuel"}]}
{"id":"mcp_agent_mail-ctq","title":"Milestone: Production Ready","description":"priority: 3","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-05T21:02:17.882507-05:00","updated_at":"2026-01-06T02:10:13.146794-05:00","closed_at":"2026-01-06T02:10:13.146794-05:00","close_reason":"All dependencies satisfied: Full Integration Coverage (74 tests), Concurrency (13 tests), Security Input Sanitization (28 tests). Production ready milestone complete.","dependencies":[{"issue_id":"mcp_agent_mail-ctq","depends_on_id":"mcp_agent_mail-9bz","type":"blocks","created_at":"2026-01-05T21:02:57.578989-05:00","created_by":"jemanuel"},{"issue_id":"mcp_agent_mail-ctq","depends_on_id":"mcp_agent_mail-yh8","type":"blocks","created_at":"2026-01-05T21:02:57.684143-05:00","created_by":"jemanuel"},{"issue_id":"mcp_agent_mail-ctq","depends_on_id":"mcp_agent_mail-e4m","type":"blocks","created_at":"2026-01-05T21:02:57.801425-05:00","created_by":"jemanuel"}]}
{"id":"mcp_agent_mail-cyw","title":"Fix N+1 query pattern in _deliver_message() agent lookups","description":"# Fix N+1 Query Pattern in _deliver_message()\n\n## Problem Statement\nWhen sending a message to multiple recipients, `_deliver_message()` looks up each agent\nindividually, resulting in N database queries for N recipients.\n\n## Code Location\n`src/mcp_agent_mail/app.py` - Lines approximately 3409-3411\n\n## Current Code (THE PROBLEM)\n```python\n# This issues 3 SEPARATE database queries per recipient list!\nto_agents = [await _get_agent(project, name) for name in to_names]\ncc_agents = [await _get_agent(project, name) for name in cc_names]\nbcc_agents = [await _get_agent(project, name) for name in bcc_names]\n```\n\n## Impact Analysis\n- Message to 5 TO recipients + 2 CC + 1 BCC = 8 SELECT queries\n- Each query has connection overhead, query parsing, result serialization\n- Latency: ~5-10ms per query × 8 = 40-80ms of unnecessary latency\n- Under load: Connection pool exhaustion, query queue buildup\n\n## Required Fix\nReplace with single batched lookup:\n\n```python\n# Combine all names and fetch in ONE query\nall_recipient_names = list(set(to_names + cc_names + bcc_names))\nagents_map = await _get_agents_batch(project, all_recipient_names)\n\n# Now partition by role using O(1) dict lookups\nto_agents = [agents_map.get(name) for name in to_names if name in agents_map]\ncc_agents = [agents_map.get(name) for name in cc_names if name in agents_map]\nbcc_agents = [agents_map.get(name) for name in bcc_names if name in agents_map]\n\n# Validate all recipients were found\nmissing = set(to_names + cc_names + bcc_names) - set(agents_map.keys())\nif missing:\n    raise ValueError(f\"Unknown recipients: {missing}\")\n```\n\n## Error Handling Consideration\nThe current code may silently ignore non-existent recipients (returns None).\nNeed to verify existing behavior and preserve it exactly:\n- Does the current code raise an error for unknown recipients?\n- If so, the fix must also raise\n- If not, the fix must also silently skip\n\nRead the full function to understand error handling before implementing.\n\n## Testing Strategy\n1. Send message to single recipient - verify 1 query\n2. Send to 5 TO + 3 CC + 2 BCC - verify still 1 query\n3. Send to non-existent recipient - verify same error as before\n4. Send with duplicate names in to/cc - verify deduplication works\n\n## Verification\nUse SQLAlchemy echo=True or query logging to count queries before/after.\nExpected: Query count drops from N to 1.\n\n## Dependencies\n- Requires: _get_agents_batch function (mcp_agent_mail-XXX)","status":"open","priority":0,"issue_type":"task","created_at":"2026-01-12T01:07:08.40582703-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:07:08.40582703-05:00","dependencies":[{"issue_id":"mcp_agent_mail-cyw","depends_on_id":"mcp_agent_mail-dwu","type":"blocks","created_at":"2026-01-12T01:12:50.014777861-05:00","created_by":"ubuntu"}]}
{"id":"mcp_agent_mail-dbt","title":"Build query counting and performance instrumentation infrastructure","description":"# Query Counting and Performance Instrumentation Infrastructure\n\n## Overview\nBefore any optimization work, we need infrastructure to measure and log performance.\nThis is the foundation for proving optimizations work.\n\n## Why This Must Come First\n- Can't prove N+1 eliminated without counting queries\n- Can't measure latency improvements without timing infrastructure\n- Can't capture baselines without instrumentation\n- All other performance work depends on this\n\n## Required Components\n\n### 1. SQLAlchemy Query Counter (src/mcp_agent_mail/instrumentation.py)\n\n\\`\\`\\`python\n\"\"\"Performance instrumentation for MCP Agent Mail.\"\"\"\nfrom __future__ import annotations\n\nimport asyncio\nimport contextvars\nimport time\nfrom collections.abc import AsyncIterator, Callable\nfrom contextlib import asynccontextmanager\nfrom dataclasses import dataclass, field\nfrom functools import wraps\nfrom typing import Any, ParamSpec, TypeVar\n\nfrom rich.console import Console\nfrom rich.table import Table\nfrom sqlalchemy import event\nfrom sqlalchemy.engine import Engine\n\n# Context variable for per-request query tracking\n_query_context: contextvars.ContextVar[QueryStats | None] = contextvars.ContextVar(\n    \"_query_context\", default=None\n)\n\nconsole = Console()\n\n@dataclass\nclass QueryStats:\n    \"\"\"Statistics for queries within a context.\"\"\"\n    queries: list[QueryRecord] = field(default_factory=list)\n    start_time: float = field(default_factory=time.perf_counter)\n    \n    @property\n    def count(self) -\u003e int:\n        return len(self.queries)\n    \n    @property\n    def total_time(self) -\u003e float:\n        return sum(q.duration for q in self.queries)\n    \n    @property\n    def elapsed(self) -\u003e float:\n        return time.perf_counter() - self.start_time\n    \n    def add_query(self, sql: str, params: Any, duration: float) -\u003e None:\n        self.queries.append(QueryRecord(sql=sql, params=params, duration=duration))\n    \n    def summary(self) -\u003e str:\n        return (\n            f\"Queries: {self.count}, \"\n            f\"Query time: {self.total_time*1000:.2f}ms, \"\n            f\"Total time: {self.elapsed*1000:.2f}ms\"\n        )\n    \n    def print_report(self) -\u003e None:\n        \"\"\"Print detailed query report using Rich.\"\"\"\n        table = Table(title=\"Query Performance Report\")\n        table.add_column(\"#\", style=\"dim\")\n        table.add_column(\"Time (ms)\", justify=\"right\")\n        table.add_column(\"SQL\", overflow=\"fold\")\n        \n        for i, q in enumerate(self.queries, 1):\n            table.add_row(str(i), f\"{q.duration*1000:.2f}\", q.sql[:100])\n        \n        console.print(table)\n        console.print(f\"\\n[bold]Summary:[/bold] {self.summary()}\")\n    \n    def detect_n_plus_one(self, threshold: int = 3) -\u003e list[str]:\n        \"\"\"Detect potential N+1 patterns by finding repeated similar queries.\n        \n        Returns list of warning messages for detected patterns.\n        \"\"\"\n        # Group queries by their normalized form (parameterized)\n        from collections import Counter\n        \n        def normalize_sql(sql: str) -\u003e str:\n            # Simple normalization - could be more sophisticated\n            import re\n            return re.sub(r'\\d+', 'N', sql)\n        \n        patterns = Counter(normalize_sql(q.sql) for q in self.queries)\n        \n        warnings = []\n        for pattern, count in patterns.items():\n            if count \u003e= threshold:\n                warnings.append(\n                    f\"Possible N+1: Query pattern repeated {count} times:\\n  {pattern[:80]}...\"\n                )\n        return warnings\n\n\n@dataclass\nclass QueryRecord:\n    \"\"\"Record of a single query execution.\"\"\"\n    sql: str\n    params: Any\n    duration: float\n\n\ndef install_query_hooks(engine: Engine) -\u003e None:\n    \"\"\"Install SQLAlchemy event hooks for query tracking.\n    \n    Call this once during application startup.\n    \"\"\"\n    @event.listens_for(engine, \"before_cursor_execute\")\n    def before_cursor_execute(conn, cursor, statement, parameters, context, executemany):\n        context._query_start_time = time.perf_counter()\n    \n    @event.listens_for(engine, \"after_cursor_execute\")\n    def after_cursor_execute(conn, cursor, statement, parameters, context, executemany):\n        duration = time.perf_counter() - context._query_start_time\n        stats = _query_context.get()\n        if stats is not None:\n            stats.add_query(\n                sql=statement,\n                params=parameters,\n                duration=duration,\n            )\n\n\n@asynccontextmanager\nasync def track_queries() -\u003e AsyncIterator[QueryStats]:\n    \"\"\"Context manager to track all queries within a scope.\n    \n    Usage:\n        async with track_queries() as stats:\n            await some_database_operation()\n        \n        print(f\"Executed {stats.count} queries\")\n        stats.print_report()\n        \n        # Check for N+1 patterns\n        warnings = stats.detect_n_plus_one()\n        for warning in warnings:\n            print(f\"WARNING: {warning}\")\n    \"\"\"\n    stats = QueryStats()\n    token = _query_context.set(stats)\n    try:\n        yield stats\n    finally:\n        _query_context.reset(token)\n\n\n# Type variables for decorator\nP = ParamSpec(\"P\")\nT = TypeVar(\"T\")\n\n\ndef timed(name: str | None = None) -\u003e Callable[[Callable[P, T]], Callable[P, T]]:\n    \"\"\"Decorator to time function execution with Rich output.\n    \n    Usage:\n        @timed(\"send_message\")\n        async def send_message(...):\n            ...\n    \"\"\"\n    def decorator(func: Callable[P, T]) -\u003e Callable[P, T]:\n        func_name = name or func.__name__\n        \n        @wraps(func)\n        async def async_wrapper(*args: P.args, **kwargs: P.kwargs) -\u003e T:\n            start = time.perf_counter()\n            try:\n                return await func(*args, **kwargs)\n            finally:\n                elapsed = time.perf_counter() - start\n                console.print(\n                    f\"[dim]⏱ {func_name}: {elapsed*1000:.2f}ms[/dim]\"\n                )\n        \n        @wraps(func)\n        def sync_wrapper(*args: P.args, **kwargs: P.kwargs) -\u003e T:\n            start = time.perf_counter()\n            try:\n                return func(*args, **kwargs)\n            finally:\n                elapsed = time.perf_counter() - start\n                console.print(\n                    f\"[dim]⏱ {func_name}: {elapsed*1000:.2f}ms[/dim]\"\n                )\n        \n        if asyncio.iscoroutinefunction(func):\n            return async_wrapper  # type: ignore\n        return sync_wrapper  # type: ignore\n    \n    return decorator\n\\`\\`\\`\n\n### 2. Integration with Engine Creation (db.py modification)\n\n\\`\\`\\`python\n# In db.py, after engine creation:\nfrom .instrumentation import install_query_hooks\n\n# In the engine creation function:\nif settings.enable_query_tracking:\n    install_query_hooks(engine.sync_engine)\n\\`\\`\\`\n\n### 3. Settings Addition\n\n\\`\\`\\`python\n# Add to settings:\nenable_query_tracking: bool = decouple_config(\n    \"ENABLE_QUERY_TRACKING\", \n    default=\"false\", \n    cast=bool\n)\n\\`\\`\\`\n\n### 4. Test Utilities (tests/conftest.py addition)\n\n\\`\\`\\`python\nimport pytest\nfrom mcp_agent_mail.instrumentation import track_queries, QueryStats\n\n@pytest.fixture\nasync def query_tracker():\n    \"\"\"Fixture that provides query tracking for tests.\"\"\"\n    async with track_queries() as stats:\n        yield stats\n\n\n# Usage in tests:\nasync def test_send_message_query_count(query_tracker, sample_project):\n    await send_message(...)\n    \n    assert query_tracker.count \u003c= 5, (\n        f\"Expected ≤5 queries, got {query_tracker.count}. \"\n        f\"Possible N+1 pattern. Queries:\\n\"\n        + \"\\n\".join(q.sql[:80] for q in query_tracker.queries)\n    )\n\\`\\`\\`\n\n## File Structure\n\\`\\`\\`\nsrc/mcp_agent_mail/\n  instrumentation.py      # New file with all instrumentation code\n  \ntests/\n  conftest.py             # Add query_tracker fixture\n  test_instrumentation.py # Unit tests for instrumentation itself\n\\`\\`\\`\n\n## Unit Tests for Instrumentation (tests/test_instrumentation.py)\n\n\\`\\`\\`python\n\"\"\"Tests for performance instrumentation.\"\"\"\nimport pytest\nfrom mcp_agent_mail.instrumentation import (\n    QueryStats,\n    QueryRecord,\n    track_queries,\n    timed,\n)\n\nclass TestQueryStats:\n    def test_empty_stats(self):\n        stats = QueryStats()\n        assert stats.count == 0\n        assert stats.total_time == 0.0\n    \n    def test_add_query(self):\n        stats = QueryStats()\n        stats.add_query(\"SELECT 1\", None, 0.001)\n        assert stats.count == 1\n        assert stats.total_time == pytest.approx(0.001)\n    \n    def test_multiple_queries(self):\n        stats = QueryStats()\n        stats.add_query(\"SELECT 1\", None, 0.001)\n        stats.add_query(\"SELECT 2\", None, 0.002)\n        stats.add_query(\"SELECT 3\", None, 0.003)\n        assert stats.count == 3\n        assert stats.total_time == pytest.approx(0.006)\n    \n    def test_summary_format(self):\n        stats = QueryStats()\n        stats.add_query(\"SELECT 1\", None, 0.001)\n        summary = stats.summary()\n        assert \"Queries: 1\" in summary\n        assert \"ms\" in summary\n\n\nclass TestNPlusOneDetection:\n    \"\"\"Tests for automatic N+1 pattern detection.\"\"\"\n    \n    def test_detects_repeated_queries(self):\n        stats = QueryStats()\n        # Simulate N+1: same query pattern repeated many times\n        for i in range(10):\n            stats.add_query(f\"SELECT * FROM agents WHERE id = {i}\", None, 0.001)\n        \n        warnings = stats.detect_n_plus_one(threshold=5)\n        \n        assert len(warnings) == 1\n        assert \"Possible N+1\" in warnings[0]\n        assert \"10 times\" in warnings[0]\n    \n    def test_no_warning_for_varied_queries(self):\n        stats = QueryStats()\n        # Different queries - not N+1\n        stats.add_query(\"SELECT * FROM agents\", None, 0.001)\n        stats.add_query(\"SELECT * FROM messages\", None, 0.001)\n        stats.add_query(\"INSERT INTO agents VALUES (...)\", None, 0.001)\n        \n        warnings = stats.detect_n_plus_one(threshold=3)\n        \n        assert len(warnings) == 0\n    \n    def test_threshold_respected(self):\n        stats = QueryStats()\n        # 4 similar queries\n        for i in range(4):\n            stats.add_query(f\"SELECT * FROM agents WHERE id = {i}\", None, 0.001)\n        \n        # Threshold of 5 - should not trigger\n        warnings = stats.detect_n_plus_one(threshold=5)\n        assert len(warnings) == 0\n        \n        # Threshold of 3 - should trigger\n        warnings = stats.detect_n_plus_one(threshold=3)\n        assert len(warnings) == 1\n\n\nclass TestTrackQueries:\n    @pytest.mark.asyncio\n    async def test_context_manager_returns_stats(self):\n        async with track_queries() as stats:\n            pass\n        assert isinstance(stats, QueryStats)\n    \n    @pytest.mark.asyncio\n    async def test_nested_contexts_independent(self):\n        async with track_queries() as outer:\n            async with track_queries() as inner:\n                # Inner context should be separate\n                pass\n            # Outer should still work\n        assert isinstance(outer, QueryStats)\n        assert isinstance(inner, QueryStats)\n\n\nclass TestTimedDecorator:\n    @pytest.mark.asyncio\n    async def test_async_function(self, capsys):\n        @timed(\"test_func\")\n        async def slow_func():\n            await asyncio.sleep(0.01)\n            return 42\n        \n        result = await slow_func()\n        assert result == 42\n        \n        captured = capsys.readouterr()\n        assert \"test_func\" in captured.out\n        assert \"ms\" in captured.out\n    \n    def test_sync_function(self, capsys):\n        @timed(\"sync_test\")\n        def fast_func():\n            return 42\n        \n        result = fast_func()\n        assert result == 42\n        \n        captured = capsys.readouterr()\n        assert \"sync_test\" in captured.out\n\\`\\`\\`\n\n## Verification\nAfter implementing:\n1. Run \\`pytest tests/test_instrumentation.py -v\\` - all tests pass\n2. In a test, wrap DB operations with track_queries() - see query counts\n3. Add @timed to a function - see timing output\n4. Call detect_n_plus_one() on stats to find N+1 patterns\n\n## Acceptance Criteria\n- [ ] QueryStats correctly counts queries\n- [ ] track_queries() context manager works in async code\n- [ ] @timed decorator logs execution time\n- [ ] detect_n_plus_one() identifies repeated query patterns\n- [ ] Rich console output is clear and readable\n- [ ] All unit tests pass\n- [ ] No performance overhead when tracking disabled\n\n## Dependencies\nNone - this is foundational infrastructure.\n\n## Blocks\n- Benchmarking suite (k92)\n- E2E tests (ab8)\n- All performance testing\n- Baseline capture","status":"open","priority":0,"issue_type":"task","created_at":"2026-01-12T01:17:31.410538742-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:28:59.777856351-05:00"}
{"id":"mcp_agent_mail-dcy","title":"Fix N+1 in summarize_thread() - batch message and recipient fetching","description":"# Fix N+1 in summarize_thread()\n\n## Problem Statement\nThread summarization likely fetches messages and their recipients in a loop,\ncausing N+1 query patterns similar to inbox/outbox listing.\n\n## Code Location\n`src/mcp_agent_mail/app.py` - Search for \"summarize_thread\" function\n\n## Investigation Required\n1. Find the summarize_thread implementation\n2. Trace the data access patterns\n3. Identify N+1 loops (per-message queries for participants, recipients, etc.)\n\n## Expected Issues\nBased on analysis, summarize_thread likely:\n- Fetches all messages in a thread (1 query - OK)\n- For each message, fetches sender info (N queries - BAD)\n- For each message, fetches recipients (N queries - BAD)\n- For each message, fetches read/ack status (N queries - BAD)\n\n## Fix Pattern\nSame as other N+1 fixes:\n1. Collect all message IDs from the thread\n2. Batch fetch all related data:\n   - All senders: `WHERE id IN (sender_ids)`\n   - All recipients: `WHERE message_id IN (message_ids)`\n   - All read statuses: `WHERE message_id IN (message_ids)`\n3. Build lookup dicts from results\n4. Assemble thread summary using lookups\n\n## Performance Impact\nA thread with 50 messages currently issues 50-150 queries.\nAfter fix: 4-5 queries total regardless of thread size.\n\n## Testing Strategy\n1. Create thread with 1 message, verify query count\n2. Create thread with 20 messages, verify still ~5 queries\n3. Verify summary output is identical before/after\n\n## Dependencies\n- Requires _get_agents_batch for sender lookups\n- Similar pattern to inbox/outbox fixes","status":"open","priority":0,"issue_type":"task","created_at":"2026-01-12T01:07:58.023043838-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:07:58.023043838-05:00","dependencies":[{"issue_id":"mcp_agent_mail-dcy","depends_on_id":"mcp_agent_mail-dwu","type":"blocks","created_at":"2026-01-12T01:12:50.037727325-05:00","created_by":"ubuntu"}]}
{"id":"mcp_agent_mail-dvk","title":"Unit Tests: app.py - Project Operations","description":"priority: 2","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.469149-05:00","updated_at":"2026-01-05T21:00:48.431232-05:00","deleted_at":"2026-01-05T21:00:48.431232-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-dwu","title":"Create batched agent lookup function (_get_agents_batch)","description":"# Create Batched Agent Lookup Function\n\n## Problem Statement\nThe current `_get_agent()` function fetches a single agent by name, requiring N separate\ndatabase queries when looking up N agents. This is a classic N+1 pattern.\n\n## Current Code Location\n`src/mcp_agent_mail/app.py` - The `_get_agent()` function (search for \"async def _get_agent\")\n\n## Current Implementation Pattern\n```python\nasync def _get_agent(project: Project, name: str) -\u003e Agent | None:\n    async with _async_session() as session:\n        stmt = select(Agent).where(Agent.project_id == project.id, Agent.name == name)\n        result = await session.execute(stmt)\n        return result.scalar_one_or_none()\n```\n\n## Required Implementation\nCreate a new function `_get_agents_batch()` that accepts a list of names and returns all\nmatching agents in a single query:\n\n```python\nasync def _get_agents_batch(project: Project, names: list[str]) -\u003e dict[str, Agent]:\n    \"\"\"Batch fetch agents by name - single query instead of N.\n    \n    Args:\n        project: The project to search within\n        names: List of agent names to look up\n        \n    Returns:\n        Dict mapping name -\u003e Agent for found agents. Missing names are not in dict.\n        \n    Performance:\n        O(1) database query regardless of len(names)\n        Previous: O(n) queries where n = len(names)\n    \"\"\"\n    if not names:\n        return {}\n    \n    # Deduplicate names to avoid redundant work\n    unique_names = list(set(names))\n    \n    async with _async_session() as session:\n        stmt = select(Agent).where(\n            Agent.project_id == project.id,\n            Agent.name.in_(unique_names)\n        )\n        result = await session.execute(stmt)\n        agents = result.scalars().all()\n        return {agent.name: agent for agent in agents}\n```\n\n## Why This Design\n1. **Returns dict** - O(1) lookup by name for callers\n2. **Deduplicates** - Handles case where same name appears in to/cc/bcc\n3. **Handles missing** - Missing names simply aren't in the dict (callers can check)\n4. **Single session** - Reuses connection from pool efficiently\n\n## Testing Requirements\n1. Unit test: Empty names list returns empty dict\n2. Unit test: Single name returns correct agent\n3. Unit test: Multiple names returns all agents\n4. Unit test: Duplicate names handled correctly\n5. Unit test: Non-existent names not in result dict\n6. Integration test: Verify query count is 1 regardless of input size\n\n## Verification\nAfter implementation, run with SQL logging enabled and verify only 1 SELECT is issued\nregardless of how many names are passed in.\n\n## Dependencies\nNone - this is a new utility function that will be used by subsequent tasks.","status":"open","priority":0,"issue_type":"task","created_at":"2026-01-12T01:07:07.828287763-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:07:07.828287763-05:00","dependencies":[{"issue_id":"mcp_agent_mail-dwu","depends_on_id":"mcp_agent_mail-k92","type":"blocks","created_at":"2026-01-12T01:12:50.081521085-05:00","created_by":"ubuntu"}]}
{"id":"mcp_agent_mail-e4m","title":"Concurrency: Multiple Agents","description":"priority: 3","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-05T21:02:17.876769-05:00","updated_at":"2026-01-06T02:19:46.303396-05:00","closed_at":"2026-01-06T02:19:46.303396-05:00","close_reason":"13 concurrency tests pass: agent registration, message sending, file reservation conflicts, inbox fetching, archive writes, deadlock prevention, race conditions. Tests made resilient to transient async failures.","dependencies":[{"issue_id":"mcp_agent_mail-e4m","depends_on_id":"mcp_agent_mail-uvf","type":"blocks","created_at":"2026-01-05T21:02:55.413216-05:00","created_by":"jemanuel"},{"issue_id":"mcp_agent_mail-e4m","depends_on_id":"mcp_agent_mail-aew","type":"blocks","created_at":"2026-01-05T21:02:55.576713-05:00","created_by":"jemanuel"}]}
{"id":"mcp_agent_mail-e9z","title":"P4 - E2E Scenario Tests","description":"Complete end-to-end scenarios.","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T21:02:17.878396-05:00","updated_at":"2026-01-05T21:02:37.963853-05:00","deleted_at":"2026-01-05T21:02:37.963853-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-enu","title":"CLI: Archive Commands","description":"priority: 2","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-05T21:02:17.859253-05:00","updated_at":"2026-01-05T23:39:38.961247-05:00","closed_at":"2026-01-05T23:39:38.961247-05:00","close_reason":"24 comprehensive CLI archive command tests added and passing","dependencies":[{"issue_id":"mcp_agent_mail-enu","depends_on_id":"mcp_agent_mail-uvf","type":"blocks","created_at":"2026-01-05T21:02:54.9184-05:00","created_by":"jemanuel"}]}
{"id":"mcp_agent_mail-f26","title":"Unit Tests: share.py - Archive Save","description":"priority: 2","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.483229-05:00","updated_at":"2026-01-05T21:00:46.787388-05:00","deleted_at":"2026-01-05T21:00:46.787388-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-fuj","title":"Integration Tests: File Reservation Conflicts","description":"priority: 1","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.484292-05:00","updated_at":"2026-01-05T21:00:46.383572-05:00","deleted_at":"2026-01-05T21:00:46.383572-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-g77","title":"Fix LRU cache O(n) operations - replace list with OrderedDict","description":"# Fix LRU Cache O(n) → O(1) Operations\n\n## Problem Statement\nThe SimpleLRUCache class uses a list to track access order, requiring O(n) \nlist.remove() and list operations on every cache hit. This makes cache access\nlinear time instead of constant time.\n\n## Code Location\n`src/mcp_agent_mail/storage.py` - Line 87 and surrounding SimpleLRUCache class\n\n## Current Code (THE PROBLEM)\n```python\nclass SimpleLRUCache:\n    def __init__(self, maxsize: int = 128):\n        self._cache: dict[str, Any] = {}\n        self._order: list[str] = []  # THIS IS THE PROBLEM\n        self._maxsize = maxsize\n    \n    def get(self, key: str) -\u003e Any | None:\n        if key not in self._cache:\n            return None\n        self._order.remove(key)  # O(n) - scans entire list!\n        self._order.append(key)  # O(1) amortized\n        return self._cache[key]\n    \n    def set(self, key: str, value: Any) -\u003e None:\n        if key in self._cache:\n            self._order.remove(key)  # O(n) again!\n        self._cache[key] = value\n        self._order.append(key)\n        while len(self._order) \u003e self._maxsize:\n            oldest = self._order.pop(0)  # O(n) - shifts all elements!\n            del self._cache[oldest]\n```\n\n## Impact Analysis\n- Every cache HIT is O(n) where n = cache size\n- With maxsize=128, every hit scans up to 128 elements\n- Under load with frequent cache access, this dominates runtime\n- Cache meant to IMPROVE performance is actually hurting it!\n\n## Required Fix\nReplace list-based tracking with collections.OrderedDict which provides O(1)\nmove_to_end() and popitem() operations:\n\n```python\nfrom collections import OrderedDict\nfrom typing import Any\n\nclass SimpleLRUCache:\n    \"\"\"LRU cache with O(1) access, update, and eviction.\"\"\"\n    \n    def __init__(self, maxsize: int = 128):\n        self._cache: OrderedDict[str, Any] = OrderedDict()\n        self._maxsize = maxsize\n    \n    def get(self, key: str) -\u003e Any | None:\n        \"\"\"Get value and mark as recently used. O(1) operation.\"\"\"\n        if key not in self._cache:\n            return None\n        # O(1) - OrderedDict maintains doubly-linked list internally\n        self._cache.move_to_end(key)\n        return self._cache[key]\n    \n    def set(self, key: str, value: Any) -\u003e None:\n        \"\"\"Set value and mark as recently used. O(1) operation.\"\"\"\n        if key in self._cache:\n            self._cache.move_to_end(key)\n        self._cache[key] = value\n        # Evict oldest if over capacity - O(1) per eviction\n        while len(self._cache) \u003e self._maxsize:\n            self._cache.popitem(last=False)  # Remove oldest (first)\n    \n    def __contains__(self, key: str) -\u003e bool:\n        \"\"\"Check if key exists without updating LRU order.\"\"\"\n        return key in self._cache\n    \n    def clear(self) -\u003e None:\n        \"\"\"Clear all cached entries.\"\"\"\n        self._cache.clear()\n```\n\n## Why OrderedDict\n- Python's OrderedDict uses a doubly-linked list internally\n- move_to_end() is O(1) - just pointer manipulation\n- popitem(last=False) is O(1) - removes head of list\n- No manual bookkeeping needed\n\n## Alternative: functools.lru_cache\nCould also use @lru_cache decorator, but:\n- Requires hashable arguments\n- Less control over cache behavior\n- SimpleLRUCache may have specific semantics we need to preserve\n\n## Testing Strategy\n1. Unit test: get() returns correct value\n2. Unit test: get() updates access order (verify with iteration)\n3. Unit test: set() evicts oldest when at capacity\n4. Unit test: maxsize=1 works correctly\n5. Performance test: 10000 operations completes in \u003c100ms\n\n## Verification\n```python\nimport timeit\n\ncache = SimpleLRUCache(maxsize=1000)\n# Populate\nfor i in range(1000):\n    cache.set(f\"key{i}\", i)\n\n# Benchmark get operations\ntime = timeit.timeit(lambda: cache.get(\"key500\"), number=100000)\nprint(f\"100k gets: {time:.3f}s\")  # Should be \u003c 1s with O(1), \u003e\u003e 1s with O(n)\n```\n\n## Dependencies\nNone - self-contained fix","status":"open","priority":0,"issue_type":"task","created_at":"2026-01-12T01:07:57.207854635-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:07:57.207854635-05:00"}
{"id":"mcp_agent_mail-gt4","title":"Replace fetchall() with cursor iteration for large result sets","description":"# Replace fetchall() with Streaming for Large Results\n\n## Problem Statement\nCode uses fetchall() which loads entire result sets into memory, causing\nmemory pressure for large queries.\n\n## Code Locations\nSearch for `.fetchall()` or `.all()` on large queries throughout codebase.\n\n## Current Pattern (THE PROBLEM)\n```python\nresult = await session.execute(select(Message).where(...))\nmessages = result.scalars().all()  # Loads ALL into memory\n\nfor msg in messages:  # Memory already allocated\n    process(msg)\n```\n\n## Impact Analysis\n- 10,000 messages × 10KB each = 100MB memory spike\n- GC pressure from large allocations\n- Potential OOM for very large result sets\n\n## Required Fix\nUse streaming/cursor iteration:\n\n```python\n# Option 1: stream_scalars for row-by-row processing\nasync with session.stream_scalars(select(Message).where(...)) as stream:\n    async for msg in stream:\n        process(msg)  # Memory for one row at a time\n\n# Option 2: yield_per for batched streaming\nresult = await session.execute(\n    select(Message).where(...).execution_options(yield_per=100)\n)\nfor msg in result.scalars():\n    process(msg)  # Fetches in batches of 100\n```\n\n## SQLModel/SQLAlchemy Async Streaming\n```python\nfrom sqlalchemy.ext.asyncio import AsyncSession\n\nasync def process_large_results(session: AsyncSession):\n    \"\"\"Process large result set with minimal memory.\"\"\"\n    stmt = select(Message).where(Message.project_id == project_id)\n    \n    # stream() returns async iterator\n    async with session.stream(stmt) as result:\n        async for row in result:\n            msg = row[0]  # Unpack from Row\n            await process_message(msg)\n```\n\n## When to Use Streaming\n- Unknown result size that could be large\n- Processing can be done row-by-row\n- Memory is constrained\n- Latency to first result matters\n\n## When fetchall() Is OK\n- Known small result sets (\u003c 1000 rows)\n- Need random access to results\n- Need to process results multiple times\n- Results needed all at once for aggregation\n\n## Implementation Notes\n- Streaming keeps database cursor open longer\n- May conflict with connection pool settings\n- Test with realistic data volumes\n\n## Testing Strategy\n1. Query returning 100,000 rows with fetchall() - measure memory\n2. Same query with streaming - measure memory\n3. Verify identical results\n4. Check for connection pool exhaustion\n\n## Verification\n```python\nimport tracemalloc\n\ntracemalloc.start()\n\n# fetchall approach\nresult = await session.execute(select(Message).limit(10000))\nmessages = result.scalars().all()\nfetchall_memory = tracemalloc.get_traced_memory()[1]\n\ntracemalloc.reset_peak()\n\n# streaming approach\nasync with session.stream_scalars(select(Message).limit(10000)) as stream:\n    async for msg in stream:\n        pass  # Just iterate\nstreaming_memory = tracemalloc.get_traced_memory()[1]\n\nprint(f\"fetchall: {fetchall_memory / 1e6:.1f}MB\")\nprint(f\"streaming: {streaming_memory / 1e6:.1f}MB\")\n```\n\n## Dependencies\nNone - can be done independently, file by file","status":"open","priority":3,"issue_type":"task","created_at":"2026-01-12T01:12:23.423590282-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:12:23.423590282-05:00"}
{"id":"mcp_agent_mail-h1m","title":"P1 - Core Functionality Tests","description":"These test the primary user-facing functionality.","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T21:02:17.834592-05:00","updated_at":"2026-01-05T21:02:37.107921-05:00","deleted_at":"2026-01-05T21:02:37.107921-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-h4o","title":"Add @lru_cache to PathSpec pattern compilation","description":"# Cache PathSpec Pattern Compilation\n\n## Problem Statement\nThe `_patterns_overlap()` function compiles PathSpec patterns on every call.\nPathSpec compilation involves regex compilation which is expensive. In\nreservation conflict detection loops, the same patterns are recompiled repeatedly.\n\n## Code Location\n`src/mcp_agent_mail/app.py` - Lines approximately 2938-2939\n\n## Current Code (THE PROBLEM)\n```python\ndef _patterns_overlap(a: str, b: str) -\u003e bool:\n    \"\"\"Check if two gitignore-style patterns could match overlapping paths.\"\"\"\n    # COMPILES REGEX EVERY SINGLE CALL!\n    a_spec = PathSpec.from_lines(\"gitwildmatch\", [a])\n    b_spec = PathSpec.from_lines(\"gitwildmatch\", [b])\n    \n    # ... overlap detection logic ...\n```\n\n## Impact Analysis\n- Reservation conflict check compares pattern A against N existing reservations\n- Each comparison compiles both patterns = 2 compilations\n- N reservations × 2 = 2N regex compilations per conflict check\n- With 100 active reservations, that's 200 regex compilations per check!\n\n## Required Fix\nAdd memoization to pattern compilation:\n\n```python\nfrom functools import lru_cache\nfrom pathspec import PathSpec\n\n@lru_cache(maxsize=512)\ndef _compile_pathspec(pattern: str) -\u003e PathSpec:\n    \"\"\"Compile a gitignore pattern to PathSpec with caching.\n    \n    Cache hit ratio should be very high since:\n    - Same reservation patterns are checked repeatedly\n    - Agents typically use a small set of patterns\n    \n    maxsize=512 balances memory usage with hit rate.\n    \"\"\"\n    return PathSpec.from_lines(\"gitwildmatch\", [pattern])\n\n\ndef _patterns_overlap(a: str, b: str) -\u003e bool:\n    \"\"\"Check if two gitignore-style patterns could match overlapping paths.\"\"\"\n    a_spec = _compile_pathspec(a)  # O(1) if cached\n    b_spec = _compile_pathspec(b)  # O(1) if cached\n    \n    # ... rest of function unchanged ...\n```\n\n## Why lru_cache\n- Standard library, no dependencies\n- Thread-safe in Python 3.2+\n- Automatic eviction when maxsize exceeded\n- Can inspect cache stats with `_compile_pathspec.cache_info()`\n\n## Cache Size Reasoning\n- maxsize=512 chosen because:\n  - Typical project has 10-50 distinct reservation patterns\n  - 512 handles 10 projects × 50 patterns with room to spare\n  - Each PathSpec is relatively small (pattern string + compiled regex)\n  - Memory overhead ~50KB for 512 entries\n\n## Testing Strategy\n1. Call _patterns_overlap with same patterns 1000 times\n2. Verify cache_info shows high hit rate\n3. Benchmark: should be 10-50x faster than uncached version\n\n## Verification\n```python\n# After implementation\n_compile_pathspec.cache_clear()  # Reset cache\nfor _ in range(1000):\n    _patterns_overlap(\"src/**/*.py\", \"src/api/*.py\")\n    \ninfo = _compile_pathspec.cache_info()\nprint(f\"Hits: {info.hits}, Misses: {info.misses}\")\n# Expected: ~2 misses, ~1998 hits\n```\n\n## Edge Cases\n- Empty pattern: Should be handled by PathSpec\n- Invalid pattern: Let PathSpec raise its normal error (don't cache errors)\n- Very long pattern: Still cached, but consider if this could be DoS vector\n\n## Dependencies\nNone - self-contained fix","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-12T01:09:21.988589258-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:09:21.988589258-05:00"}
{"id":"mcp_agent_mail-hqk","title":"MCP Resources: Read Access","description":"priority: 1","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-05T21:02:17.843304-05:00","updated_at":"2026-01-05T22:46:29.773477-05:00","closed_at":"2026-01-05T22:46:29.773477-05:00","close_reason":"Completed P1 MCP Resources Read Access tests - 18 tests covering project, agents, inbox, outbox, thread, and file_reservations resources","dependencies":[{"issue_id":"mcp_agent_mail-hqk","depends_on_id":"mcp_agent_mail-mm2","type":"blocks","created_at":"2026-01-05T21:02:54.329922-05:00","created_by":"jemanuel"}]}
{"id":"mcp_agent_mail-i0u","title":"E2E Test Script: Multi-Agent Workflow","description":"priority: 1","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.486948-05:00","updated_at":"2026-01-05T21:00:45.7694-05:00","deleted_at":"2026-01-05T21:00:45.7694-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-i3n","title":"Integration Tests: Guard Pre-commit","description":"priority: 2","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.485595-05:00","updated_at":"2026-01-05T21:00:46.147751-05:00","deleted_at":"2026-01-05T21:00:46.147751-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-i8h","title":"E2E Test Script: Disaster Recovery","description":"priority: 2","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.487445-05:00","updated_at":"2026-01-05T21:00:45.650175-05:00","deleted_at":"2026-01-05T21:00:45.650175-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-i8s","title":"Phase 2 verification checkpoint - validate caching and batching","description":"# Phase 2 Verification Checkpoint\n\n## Purpose\nAfter completing all Phase 2 tasks, verify:\n1. PathSpec caching working (cache hits observed)\n2. Git commit batching reducing commit count\n3. Per-project locks enabling parallelism\n4. Snippet optimization reducing FTS overhead\n5. heapq.nlargest working correctly\n\n## Verification Steps\n\n### 1. Run E2E Correctness Tests\n```bash\npytest tests/e2e/ -v\n# Must pass - all outputs identical to golden files\n```\n\n### 2. Verify PathSpec Cache Hits\n```python\nfrom mcp_agent_mail.app import _compile_pathspec\n\n# After running reservation conflict checks\ninfo = _compile_pathspec.cache_info()\nprint(f\"Cache hits: {info.hits}, misses: {info.misses}\")\n# Expected: High hit rate (\u003e90%) after warmup\n```\n\n### 3. Verify Git Commit Batching\n```bash\n# Before: Multiple operations = multiple commits\n# After: Related operations = single commit\n\n# Count commits for a multi-file operation\ngit log --oneline -5\n# Should see consolidated commits like:\n# \"agent: BlueLake session setup (profile + reservation)\"\n```\n\n### 4. Verify Parallel Project Operations\n```python\n# Run operations on two different projects simultaneously\nimport asyncio\nimport time\n\nasync def test_parallel():\n    start = time.perf_counter()\n    await asyncio.gather(\n        operation_on_project_a(),\n        operation_on_project_b(),\n    )\n    elapsed = time.perf_counter() - start\n    \n    # Should be ~1x single operation time, not 2x\n    assert elapsed \u003c 1.5 * single_operation_time\n```\n\n### 5. Verify Snippet Optimization\n```python\n# Search should only call snippet() once per row\n# Monitor FTS function calls before/after\n```\n\n### 6. Verify heapq.nlargest Correctness\n```python\n# Verify top-N results are identical\nold_result = sorted(items, key=k, reverse=True)[:10]\nnew_result = heapq.nlargest(10, items, key=k)\nassert old_result == new_result\n```\n\n### 7. Run Performance Benchmarks\n```bash\npytest tests/benchmarks/ -v --benchmark-compare=phase1\n```\n\n## Acceptance Criteria\n- [ ] E2E tests pass\n- [ ] PathSpec cache hit rate \u003e 90%\n- [ ] Git commits consolidated for related operations\n- [ ] Cross-project operations run in parallel\n- [ ] Snippet calls reduced from 3x to 1x per row\n- [ ] Phase 2 report generated\n\n## Dependencies\n- Phase 1 verification complete\n- All Phase 2 tasks complete\n\n## Blocks\n- Phase 3 feature","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-12T01:19:51.966115819-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:19:51.966115819-05:00","dependencies":[{"issue_id":"mcp_agent_mail-i8s","depends_on_id":"mcp_agent_mail-pyi","type":"blocks","created_at":"2026-01-12T01:20:19.464744249-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-i8s","depends_on_id":"mcp_agent_mail-h4o","type":"blocks","created_at":"2026-01-12T01:20:19.494022627-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-i8s","depends_on_id":"mcp_agent_mail-roy","type":"blocks","created_at":"2026-01-12T01:20:19.522898047-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-i8s","depends_on_id":"mcp_agent_mail-pk2","type":"blocks","created_at":"2026-01-12T01:20:19.552177577-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-i8s","depends_on_id":"mcp_agent_mail-1kw","type":"blocks","created_at":"2026-01-12T01:20:19.579812804-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-i8s","depends_on_id":"mcp_agent_mail-0uh","type":"blocks","created_at":"2026-01-12T01:20:19.609913779-05:00","created_by":"ubuntu"}]}
{"id":"mcp_agent_mail-i9c","title":"Replace list membership check with set for O(1) lookup","description":"# Replace List Membership with Set for O(1) Lookup\n\n## Problem Statement\nCode checks `if item not in list` inside a loop, resulting in O(n²) total\ncomplexity when the list grows.\n\n## Code Location\n`src/mcp_agent_mail/share.py` - Line approximately 696\n\n## Current Code (THE PROBLEM)\n```python\nselected = []\nfor record in all_records:\n    # O(n) membership check on every iteration!\n    if found_record not in selected:\n        selected.append(found_record)\n```\n\n## Impact Analysis\n- First check: scan 0 items\n- Second check: scan 1 item\n- Third check: scan 2 items\n- ...\n- Nth check: scan N-1 items\n- Total: 0 + 1 + 2 + ... + (N-1) = N(N-1)/2 = O(n²)\n\nFor 1000 records, that's ~500,000 comparisons!\n\n## Required Fix\n```python\nselected_ids: set[int] = set()  # O(1) membership test\nselected_list: list[Record] = []  # Maintain order if needed\n\nfor record in all_records:\n    if record.id not in selected_ids:  # O(1)!\n        selected_ids.add(record.id)\n        selected_list.append(record)\n```\n\n## Why Use ID for Set\nRecords may not be hashable directly (SQLModel objects with relationships).\nUsing the ID (which is just an int) guarantees hashability.\n\n## Alternative: Use dict for Deduplication\n```python\n# If order doesn't matter and you just need unique records\nselected = {record.id: record for record in all_records}\nunique_records = list(selected.values())\n```\n\n## Finding Similar Patterns\nSearch the codebase for:\n```\nif .* not in .*:\n    .*.append(\n```\n\nThis pattern often indicates O(n²) list membership checks.\n\n## Testing Strategy\n1. Verify deduplication works correctly\n2. Verify order is preserved (if required)\n3. Benchmark with 1000, 10000 records\n4. Verify O(n) vs O(n²) scaling\n\n## Verification\n```python\nimport time\n\ndef old_way(n):\n    selected = []\n    for i in range(n):\n        if i not in selected:\n            selected.append(i)\n    return selected\n\ndef new_way(n):\n    selected_set = set()\n    selected_list = []\n    for i in range(n):\n        if i not in selected_set:\n            selected_set.add(i)\n            selected_list.append(i)\n    return selected_list\n\n# Time both\nfor n in [100, 1000, 10000]:\n    t1 = time.perf_counter()\n    old_way(n)\n    old_time = time.perf_counter() - t1\n    \n    t2 = time.perf_counter()\n    new_way(n)\n    new_time = time.perf_counter() - t2\n    \n    print(f\"n={n}: old={old_time:.4f}s, new={new_time:.4f}s, speedup={old_time/new_time:.1f}x\")\n```\n\n## Dependencies\nNone - self-contained fix","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-12T01:10:49.542961875-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:10:49.542961875-05:00"}
{"id":"mcp_agent_mail-ihs","title":"P3 - Concurrent Access Tests","description":"Test behavior under concurrent load.","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T21:02:17.87517-05:00","updated_at":"2026-01-05T21:02:37.760344-05:00","deleted_at":"2026-01-05T21:02:37.760344-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-ill","title":"Unit Tests: app.py - Contact Management","description":"priority: 2","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.470372-05:00","updated_at":"2026-01-05T21:00:48.065732-05:00","deleted_at":"2026-01-05T21:00:48.065732-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-ipa","title":"P2 - Error Handling Tests","description":"Test that errors are handled gracefully with clear messages.","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T21:02:17.846757-05:00","updated_at":"2026-01-05T21:02:37.297157-05:00","deleted_at":"2026-01-05T21:02:37.297157-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-irp","title":"Guards: Pre-commit Enforcement","description":"priority: 2","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-05T21:02:17.867804-05:00","updated_at":"2026-01-05T23:24:02.101521-05:00","closed_at":"2026-01-05T23:24:02.101521-05:00","close_reason":"14 comprehensive pre-commit enforcement tests added and passing","dependencies":[{"issue_id":"mcp_agent_mail-irp","depends_on_id":"mcp_agent_mail-aew","type":"blocks","created_at":"2026-01-05T21:02:55.010915-05:00","created_by":"jemanuel"}]}
{"id":"mcp_agent_mail-jto","title":"Bug: File handle exhaustion (EMFILE) under heavy load","description":"GitHub Issue #59 - After extended sessions, 'Too many open files' error occurs.\n\n**Root Cause Analysis:**\n- LRU repo cache (maxsize=8) helps but doesn't prevent EMFILE\n- Evicted repos tracked in _evicted list may not close promptly due to reference counts\n- Error message shows 'Freed 1 cached repos' indicating cache had minimal repos when error occurred\n\n**Investigation Needed:**\n1. Check if other file handles accumulate (locks, config files, etc.)\n2. Consider increasing LRU cache size or more aggressive cleanup\n3. Add file handle monitoring/metrics\n\n**Current Mitigation:**\n- clear_repo_cache() called on error (app.py:324)\n- Recovery works but user must retry operation\n\nReference: https://github.com/Dicklesworthstone/mcp_agent_mail/issues/59","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-05T21:17:09.114719-05:00","created_by":"jemanuel","updated_at":"2026-01-05T23:53:47.633305-05:00","closed_at":"2026-01-05T23:53:47.633305-05:00","close_reason":"Implemented LRU cache improvements: increased maxsize to 16, added opportunistic cleanup on get() calls, added warning logging, added monitoring stats. Added 19 tests."}
{"id":"mcp_agent_mail-jts","title":"P1 - MCP Protocol Tests","description":"Test all MCP tools and resources work correctly.","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T21:02:17.840562-05:00","updated_at":"2026-01-05T21:02:37.19954-05:00","deleted_at":"2026-01-05T21:02:37.19954-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-jxj","title":"Fix N+1 query pattern in _list_outbox() recipient fetching","description":"# Fix N+1 Query Pattern in _list_outbox()\n\n## Problem Statement\nWhen listing outbox messages, the code fetches recipients for each message in a loop,\nresulting in N additional queries for N messages.\n\n## Code Location\n`src/mcp_agent_mail/app.py` - Lines approximately 3024-3029\n\n## Current Code (THE PROBLEM)\n```python\nfor msg in message_rows:\n    # THIS RUNS ONCE PER MESSAGE - N+1!\n    recs = await session.execute(\n        select(MessageRecipient.recipient_name, MessageRecipient.kind)\n        .where(MessageRecipient.message_id == msg.id)\n    )\n    recipients = recs.all()\n    # ... build response\n```\n\n## Impact Analysis\n- Listing 20 messages = 20 additional SELECT queries\n- Plus 1 initial query for messages = 21 total queries\n- Each listing operation is dramatically slower than necessary\n- User-facing latency: 200-400ms instead of 20-40ms\n\n## Required Fix\nFetch all recipients in a single query using IN clause:\n\n```python\n# First, collect all message IDs\nmsg_ids = [msg.id for msg in message_rows]\n\n# Single query for ALL recipients across all messages\nif msg_ids:\n    recs_stmt = select(MessageRecipient).where(\n        MessageRecipient.message_id.in_(msg_ids)\n    )\n    all_recs = (await session.execute(recs_stmt)).scalars().all()\n    \n    # Group by message_id in Python - O(n) single pass\n    from collections import defaultdict\n    recs_by_msg: dict[int, list[MessageRecipient]] = defaultdict(list)\n    for rec in all_recs:\n        recs_by_msg[rec.message_id].append(rec)\nelse:\n    recs_by_msg = {}\n\n# Now iterate messages and use pre-fetched recipients\nfor msg in message_rows:\n    recipients = recs_by_msg.get(msg.id, [])\n    # ... build response using recipients\n```\n\n## Why defaultdict\n- Handles messages with no recipients gracefully\n- Avoids KeyError for edge cases\n- O(1) lookup per message\n\n## Alternative: JOIN Approach\nCould also rewrite the initial query to JOIN messages with recipients:\n```python\nstmt = (\n    select(Message, MessageRecipient)\n    .outerjoin(MessageRecipient)\n    .where(Message.sender_id == agent.id)\n)\n```\nHowever, this complicates result processing. The two-query approach is simpler and\nstill reduces N+1 to 2 queries total.\n\n## Testing Strategy\n1. List outbox with 0 messages - verify no errors\n2. List outbox with 1 message, 3 recipients - verify 2 queries total\n3. List outbox with 20 messages - verify still 2 queries total\n4. List outbox with message that has 0 recipients - verify handled\n\n## Verification\nEnable query logging. Before: 1 + N queries. After: 2 queries total.\n\n## Similar Patterns to Check\nAfter fixing _list_outbox, search for similar patterns:\n- _list_inbox (likely same issue)\n- fetch_inbox tool handler\n- Any other place that loops over messages and fetches related data","status":"open","priority":0,"issue_type":"task","created_at":"2026-01-12T01:07:09.321252784-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:07:09.321252784-05:00"}
{"id":"mcp_agent_mail-k92","title":"Create performance benchmarking suite","description":"# Create Performance Benchmarking Suite\n\n## Problem Statement\nBefore implementing optimizations, we need baseline performance metrics.\nAfter implementing, we need to verify improvements.\n\n## Objective\nCreate a reusable benchmarking suite that measures:\n- Query latency (p50, p95, p99)\n- Throughput (requests/second)\n- Memory usage (peak, average)\n- Query counts (to detect N+1)\n\n## Proposed Structure\n```\ntests/\n  benchmarks/\n    __init__.py\n    conftest.py           # Shared fixtures\n    bench_send_message.py\n    bench_fetch_inbox.py\n    bench_list_outbox.py\n    bench_search.py\n    bench_summarize.py\n    utils.py              # Timing, memory helpers\n```\n\n## Core Utilities\n```python\n# tests/benchmarks/utils.py\nimport time\nimport tracemalloc\nfrom dataclasses import dataclass\nfrom statistics import mean, quantiles\n\n@dataclass\nclass BenchmarkResult:\n    name: str\n    iterations: int\n    total_time: float\n    times: list[float]\n    peak_memory: int\n    query_count: int\n    \n    @property\n    def p50(self) -\u003e float:\n        return quantiles(self.times, n=100)[49]\n    \n    @property\n    def p95(self) -\u003e float:\n        return quantiles(self.times, n=100)[94]\n    \n    @property\n    def p99(self) -\u003e float:\n        return quantiles(self.times, n=100)[98]\n    \n    @property\n    def throughput(self) -\u003e float:\n        return self.iterations / self.total_time\n    \n    def __str__(self) -\u003e str:\n        return (\n            f\"{self.name}:\\n\"\n            f\"  Iterations: {self.iterations}\\n\"\n            f\"  Latency: p50={self.p50*1000:.2f}ms, p95={self.p95*1000:.2f}ms, p99={self.p99*1000:.2f}ms\\n\"\n            f\"  Throughput: {self.throughput:.1f} req/s\\n\"\n            f\"  Peak Memory: {self.peak_memory / 1e6:.1f}MB\\n\"\n            f\"  Queries/request: {self.query_count / self.iterations:.1f}\"\n        )\n\n\nclass QueryCounter:\n    \"\"\"Context manager to count database queries.\"\"\"\n    def __init__(self):\n        self.count = 0\n    \n    def __enter__(self):\n        # Hook into SQLAlchemy query events\n        ...\n        return self\n    \n    def __exit__(self, *args):\n        # Unhook\n        ...\n```\n\n## Example Benchmark\n```python\n# tests/benchmarks/bench_send_message.py\nimport pytest\nfrom .utils import BenchmarkResult, QueryCounter\n\n@pytest.mark.benchmark\nasync def test_send_message_performance(db_session, sample_project, sample_agents):\n    \"\"\"Benchmark send_message operation.\"\"\"\n    iterations = 100\n    times = []\n    \n    tracemalloc.start()\n    \n    with QueryCounter() as qc:\n        total_start = time.perf_counter()\n        \n        for i in range(iterations):\n            start = time.perf_counter()\n            await send_message(\n                project=sample_project,\n                sender=sample_agents[0],\n                to=[a.name for a in sample_agents[1:6]],  # 5 recipients\n                subject=f\"Test {i}\",\n                body=\"Benchmark test message\",\n            )\n            times.append(time.perf_counter() - start)\n        \n        total_time = time.perf_counter() - total_start\n    \n    peak_memory = tracemalloc.get_traced_memory()[1]\n    tracemalloc.stop()\n    \n    result = BenchmarkResult(\n        name=\"send_message (5 recipients)\",\n        iterations=iterations,\n        total_time=total_time,\n        times=times,\n        peak_memory=peak_memory,\n        query_count=qc.count,\n    )\n    \n    print(result)\n    \n    # Assertions for regression testing\n    assert result.p95 \u003c 0.100, \"p95 latency should be \u003c 100ms\"\n    assert result.query_count / iterations \u003c 10, \"Should use \u003c 10 queries per send\"\n```\n\n## Running Benchmarks\n```bash\n# Run all benchmarks\npytest tests/benchmarks/ -v --benchmark\n\n# Run specific benchmark\npytest tests/benchmarks/bench_send_message.py -v\n\n# Save results for comparison\npytest tests/benchmarks/ --benchmark-save=baseline\n\n# Compare against baseline\npytest tests/benchmarks/ --benchmark-compare=baseline\n```\n\n## Integration with CI\n```yaml\n# .github/workflows/benchmark.yml\non:\n  pull_request:\n    paths: ['src/**', 'tests/benchmarks/**']\n\njobs:\n  benchmark:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - run: uv sync\n      - run: pytest tests/benchmarks/ --benchmark-compare=main\n```\n\n## Dependencies\nShould be created early in Phase 1 to establish baselines before changes.","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-12T01:12:24.910676087-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:12:24.910676087-05:00","dependencies":[{"issue_id":"mcp_agent_mail-k92","depends_on_id":"mcp_agent_mail-dbt","type":"blocks","created_at":"2026-01-12T01:20:16.899096867-05:00","created_by":"ubuntu"}]}
{"id":"mcp_agent_mail-kfo","title":"Phase 3 verification checkpoint - validate algorithm and concurrency fixes","description":"# Phase 3 Verification Checkpoint\n\n## Purpose\nAfter completing all Phase 3 tasks, verify:\n1. String normalization uses O(n) regex\n2. Set membership is O(1)\n3. Sort key uses O(1) dict lookup\n4. File existence uses minimal stat() calls\n5. Rate limiter is thread-safe\n6. Schema operations use read-write lock\n\n## Verification Steps\n\n### 1. Run E2E Correctness Tests\n```bash\npytest tests/e2e/ -v\n# Must pass\n```\n\n### 2. Verify Algorithm Improvements\n\nEach algorithm fix should have a unit test proving correctness AND performance:\n\n```python\n# String normalization\ndef test_string_normalization_performance():\n    # String with many spaces\n    s = \"a\" + \"  \" * 1000 + \"b\"\n    \n    start = time.perf_counter()\n    result = normalize_string(s)\n    elapsed = time.perf_counter() - start\n    \n    assert \"  \" not in result  # Correctness\n    assert elapsed \u003c 0.001  # O(n) performance\n\n# Set membership\ndef test_deduplication_performance():\n    records = [Record(id=i) for i in range(10000)]\n    \n    start = time.perf_counter()\n    unique = deduplicate(records)\n    elapsed = time.perf_counter() - start\n    \n    assert len(unique) == 10000  # Correctness\n    assert elapsed \u003c 0.1  # O(n) not O(n²)\n\n# Sort key\ndef test_sort_key_performance():\n    items = [Item(priority=random.choice(PRIORITIES)) for _ in range(10000)]\n    \n    start = time.perf_counter()\n    sorted_items = sort_by_priority(items)\n    elapsed = time.perf_counter() - start\n    \n    # Correctness: items sorted by priority\n    # Performance: O(n log n) not O(n * p * log n)\n    assert elapsed \u003c 0.1\n```\n\n### 3. Verify Concurrency Fixes\n\n```python\n# Rate limiter thread safety\nasync def test_rate_limiter_concurrent():\n    limiter = TokenBucketRateLimiter(rate=10, burst=10)\n    \n    # 100 concurrent requests\n    results = await asyncio.gather(*[\n        limiter.check(\"key\") for _ in range(100)\n    ])\n    \n    # Should allow at most 10 (burst size)\n    assert sum(results) \u003c= 10\n\n# RW lock allows concurrent reads\nasync def test_rwlock_concurrent_reads():\n    lock = AsyncRWLock()\n    read_times = []\n    \n    async def reader():\n        async with lock.read_lock():\n            start = time.perf_counter()\n            await asyncio.sleep(0.1)\n            read_times.append(time.perf_counter() - start)\n    \n    await asyncio.gather(*[reader() for _ in range(10)])\n    \n    # All reads should complete in ~0.1s (parallel)\n    # Not ~1.0s (serial)\n    assert max(read_times) \u003c 0.2\n```\n\n### 4. Verify stat() Reduction\n\n```bash\n# Use strace to count stat syscalls\nstrace -e stat,statx python -c \"...\" 2\u003e\u00261 | grep -c stat\n# Should be reduced\n```\n\n### 5. Run Performance Benchmarks\n```bash\npytest tests/benchmarks/ -v --benchmark-compare=phase2\n```\n\n## Acceptance Criteria\n- [ ] E2E tests pass\n- [ ] All algorithm unit tests pass\n- [ ] Concurrency tests pass\n- [ ] stat() calls reduced\n- [ ] Phase 3 report generated\n\n## Dependencies\n- Phase 2 verification complete\n- All Phase 3 tasks complete","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-12T01:19:52.969311554-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:19:52.969311554-05:00","dependencies":[{"issue_id":"mcp_agent_mail-kfo","depends_on_id":"mcp_agent_mail-i8s","type":"blocks","created_at":"2026-01-12T01:20:20.238024562-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-kfo","depends_on_id":"mcp_agent_mail-24j","type":"blocks","created_at":"2026-01-12T01:20:20.265951658-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-kfo","depends_on_id":"mcp_agent_mail-i9c","type":"blocks","created_at":"2026-01-12T01:20:20.295824093-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-kfo","depends_on_id":"mcp_agent_mail-qem","type":"blocks","created_at":"2026-01-12T01:20:20.326548931-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-kfo","depends_on_id":"mcp_agent_mail-try","type":"blocks","created_at":"2026-01-12T01:20:20.354308973-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-kfo","depends_on_id":"mcp_agent_mail-9tj","type":"blocks","created_at":"2026-01-12T01:20:20.38164059-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-kfo","depends_on_id":"mcp_agent_mail-az3","type":"blocks","created_at":"2026-01-12T01:20:20.409778493-05:00","created_by":"ubuntu"}]}
{"id":"mcp_agent_mail-kkp","title":"Regression: Session Context Management","description":"priority: 0","status":"closed","priority":0,"issue_type":"task","created_at":"2026-01-05T21:02:17.830137-05:00","updated_at":"2026-01-05T21:41:50.572476-05:00","closed_at":"2026-01-05T21:41:50.572476-05:00","close_reason":"All 10 session context regression tests implemented (9 pass, 1 skip)"}
{"id":"mcp_agent_mail-l69","title":"P2 - HTTP Transport Tests","description":"Test HTTP/SSE transport layer.","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T21:02:17.860107-05:00","updated_at":"2026-01-05T21:02:37.476429-05:00","deleted_at":"2026-01-05T21:02:37.476429-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-lps","title":"Unit Tests: config.py","description":"priority: 2","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.467634-05:00","updated_at":"2026-01-05T21:00:48.766376-05:00","deleted_at":"2026-01-05T21:00:48.766376-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-m55","title":"Unit Tests: app.py - File Reservations","description":"priority: 2","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.469857-05:00","updated_at":"2026-01-05T21:00:48.16821-05:00","deleted_at":"2026-01-05T21:00:48.16821-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-mik","title":"Unit Tests: guard.py - Pre-push","description":"priority: 2","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.482758-05:00","updated_at":"2026-01-05T21:00:46.89123-05:00","deleted_at":"2026-01-05T21:00:46.89123-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-mj0","title":"Errors: Invalid Inputs","description":"priority: 2","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-05T21:02:17.849989-05:00","updated_at":"2026-01-05T23:30:14.939838-05:00","closed_at":"2026-01-05T23:30:14.939838-05:00","close_reason":"Created 25 tests for P2 Errors: Invalid Inputs - all tests pass","dependencies":[{"issue_id":"mcp_agent_mail-mj0","depends_on_id":"mcp_agent_mail-mm2","type":"blocks","created_at":"2026-01-05T21:02:54.436249-05:00","created_by":"jemanuel"}]}
{"id":"mcp_agent_mail-mm2","title":"Core: Project and Agent Setup","description":"priority: 1","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-05T21:02:17.838873-05:00","updated_at":"2026-01-05T21:51:54.451595-05:00","closed_at":"2026-01-05T21:51:54.451595-05:00","close_reason":"All 14 project/agent setup tests implemented and passing"}
{"id":"mcp_agent_mail-mzo","title":"Phase 1: Critical N+1 Query Elimination and Index Optimization","description":"# Phase 1: Critical Priority Optimizations (10x+ Latency Reduction)\n\n## Scope\nThis phase targets the most impactful performance issues - N+1 query patterns and missing indexes\nthat cause order-of-magnitude performance degradation.\n\n## Why This Is Critical\nN+1 queries are the #1 cause of database performance issues. In MCP Agent Mail:\n- send_message with 5 recipients issues 5 separate SELECT queries instead of 1\n- Listing 20 inbox messages issues 20 additional queries for recipients\n- These compound: sending to 5 recipients who each have 20 messages = 100+ queries\n\n## Expected Impact\n- send_message: 5-10x latency reduction\n- fetch_inbox/list_outbox: 10-20x latency reduction  \n- Search queries: 3-5x improvement with proper indexes\n\n## Key Files\n- src/mcp_agent_mail/app.py - Lines 3409-3411, 3024-3029\n- src/mcp_agent_mail/db.py - Schema definitions and indexes\n- src/mcp_agent_mail/storage.py - Line 87 (LRU cache)\n\n## Verification Strategy\n1. Write baseline benchmark capturing query counts and latencies\n2. Implement fix\n3. Re-run benchmark, verify identical outputs with fewer queries\n4. Run full test suite to confirm no behavioral changes\n\n## Tasks (7 total)\n1. Create batch agent lookup function (_get_agents_batch) - dwu\n2. Fix N+1 in _deliver_message() agent lookups - cyw\n3. Fix N+1 in _list_outbox() recipient fetching - jxj\n4. Fix N+1 in _list_inbox() recipient fetching - aid\n5. Fix N+1 in summarize_thread() message/recipient fetching - dcy\n6. Add composite database indexes for common query patterns - 4yy\n7. Fix LRU cache O(n) → O(1) with OrderedDict - g77\n\n## Task Dependency Structure\n- dwu (batch function) is foundational - cyw and dcy depend on it\n- jxj (outbox fix) should be done before aid (inbox fix) for consistency\n- g77 and 4yy are independent and can be done in parallel\n- All tasks must complete before Phase 1 verification checkpoint (pyi)","status":"open","priority":0,"issue_type":"feature","created_at":"2026-01-12T01:05:50.822840177-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:28:08.879138173-05:00","dependencies":[{"issue_id":"mcp_agent_mail-mzo","depends_on_id":"mcp_agent_mail-dwu","type":"blocks","created_at":"2026-01-12T01:13:00.951350401-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-mzo","depends_on_id":"mcp_agent_mail-cyw","type":"blocks","created_at":"2026-01-12T01:13:00.992335862-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-mzo","depends_on_id":"mcp_agent_mail-jxj","type":"blocks","created_at":"2026-01-12T01:13:01.016724045-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-mzo","depends_on_id":"mcp_agent_mail-aid","type":"blocks","created_at":"2026-01-12T01:13:01.038265436-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-mzo","depends_on_id":"mcp_agent_mail-4yy","type":"blocks","created_at":"2026-01-12T01:13:01.060561799-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-mzo","depends_on_id":"mcp_agent_mail-g77","type":"blocks","created_at":"2026-01-12T01:13:01.083023924-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-mzo","depends_on_id":"mcp_agent_mail-dcy","type":"blocks","created_at":"2026-01-12T01:13:01.103826413-05:00","created_by":"ubuntu"}]}
{"id":"mcp_agent_mail-n6z","title":"CLI: Mail Commands","description":"priority: 2","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-05T21:02:17.856438-05:00","updated_at":"2026-01-05T23:31:35.272046-05:00","closed_at":"2026-01-05T23:31:35.272046-05:00","close_reason":"29 comprehensive CLI mail command tests added and passing","dependencies":[{"issue_id":"mcp_agent_mail-n6z","depends_on_id":"mcp_agent_mail-uvf","type":"blocks","created_at":"2026-01-05T21:02:54.732874-05:00","created_by":"jemanuel"}]}
{"id":"mcp_agent_mail-njf","title":"Core: Contact Management Flow","description":"priority: 1","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-05T21:02:17.837354-05:00","updated_at":"2026-01-05T22:33:52.72945-05:00","closed_at":"2026-01-05T22:33:52.72945-05:00","close_reason":"Completed P1 Contact Management Flow tests - 15 tests covering contact request/approval workflow, policies, cross-project contacts, and macros"}
{"id":"mcp_agent_mail-nvh","title":"Unit Tests: app.py - Macros","description":"priority: 2","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.470616-05:00","updated_at":"2026-01-05T21:00:47.960622-05:00","deleted_at":"2026-01-05T21:00:47.960622-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-nwz","title":"Performance Optimization Initiative: Eliminate Gross Inefficiencies in Core System","description":"# Performance Optimization Initiative\n\n## Overview\nComprehensive performance optimization effort targeting provably isomorphic transformations that\nimprove latency, throughput, and responsiveness while guaranteeing identical outputs for identical inputs.\n\n## Background \u0026 Motivation\nDeep code analysis identified 20+ specific optimization opportunities across the MCP Agent Mail\ncodebase. These range from critical N+1 query patterns causing 10x+ unnecessary latency to\nalgorithmic improvements that reduce O(n²) operations to O(n) or O(1).\n\n## Guiding Principles\n1. **Isomorphic transformations only** - Changes must produce identical outputs for identical inputs\n2. **Measurable improvements** - Each change should have baseline metrics and post-implementation verification\n3. **No feature changes** - Pure performance work, no behavioral modifications\n4. **Incremental delivery** - Ship improvements in phases, validate each before proceeding\n\n## Success Criteria\n- Eliminate all identified N+1 query patterns\n- Reduce p95 latency for send_message by 5-10x\n- Reduce p95 latency for inbox/outbox listing by 10-20x\n- Enable true parallelism across projects (currently serialized)\n- Fix all O(n²) algorithms to O(n) or better\n\n## Phase Structure\n- Phase 1: Critical Priority (N+1 elimination, indexes, LRU cache) - 10x+ improvement potential\n- Phase 2: High Priority (caching, batching, per-project locks) - 3-5x improvement\n- Phase 3: Medium Priority (algorithms, concurrency) - 30-50% improvement\n- Phase 4: Low Priority (cleanup, polish) - marginal gains\n\n## Technical Context\n- Stack: Python 3.14, SQLModel/SQLAlchemy async, SQLite with FTS5, Git persistence\n- Key files: app.py (~9000 lines), storage.py (~2500 lines), db.py, http.py, share.py\n- No backwards compatibility concerns (early development, no production users)","status":"open","priority":1,"issue_type":"feature","created_at":"2026-01-12T01:05:12.406011106-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:05:12.406011106-05:00","dependencies":[{"issue_id":"mcp_agent_mail-nwz","depends_on_id":"mcp_agent_mail-mzo","type":"blocks","created_at":"2026-01-12T01:13:19.34487395-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-nwz","depends_on_id":"mcp_agent_mail-r6n","type":"blocks","created_at":"2026-01-12T01:13:19.366586463-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-nwz","depends_on_id":"mcp_agent_mail-vih","type":"blocks","created_at":"2026-01-12T01:13:19.386311312-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-nwz","depends_on_id":"mcp_agent_mail-2uf","type":"blocks","created_at":"2026-01-12T01:13:19.40646028-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-nwz","depends_on_id":"mcp_agent_mail-8pg","type":"blocks","created_at":"2026-01-12T01:20:21.21970083-05:00","created_by":"ubuntu"}]}
{"id":"mcp_agent_mail-p78","title":"Unit Tests: http.py - Authentication","description":"priority: 2","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.478298-05:00","updated_at":"2026-01-05T21:00:47.157832-05:00","deleted_at":"2026-01-05T21:00:47.157832-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-pk2","title":"Implement per-project commit locks instead of global lock","description":"# Implement Per-Project Commit Locks\n\n## Problem Statement\nAll projects share a single global `.commit.lock` file, serializing ALL Git\noperations across ALL projects. This prevents any parallelism.\n\n## Code Location\n`src/mcp_agent_mail/storage.py` - Line approximately 1218\n\n## Current Code (THE PROBLEM)\n```python\n# Single lock for the entire working tree!\ncommit_lock_path = Path(working_tree).resolve() / \".commit.lock\"\n```\n\n## Impact Analysis\n- Agent A working on Project X blocks Agent B working on Project Y\n- With 10 concurrent agents on different projects, only 1 can commit at a time\n- Throughput limited to ~1 commit per 50-100ms = 10-20 commits/sec TOTAL\n- Should be 10-20 commits/sec PER PROJECT\n\n## Required Fix\nMove lock to per-project directory:\n\n```python\ndef _get_project_lock_path(archive_path: Path) -\u003e Path:\n    \"\"\"Get the lock file path for a specific project.\n    \n    Lock is placed within the project's archive directory,\n    enabling parallel commits across different projects.\n    \"\"\"\n    return archive_path / \".commit.lock\"\n\n# Usage:\ncommit_lock_path = _get_project_lock_path(project_archive_path)\n```\n\n## Directory Structure Change\n```\nBefore:\n  /working/tree/.commit.lock     \u003c- Global lock\n  /working/tree/projects/foo/\n  /working/tree/projects/bar/\n\nAfter:\n  /working/tree/projects/foo/.commit.lock  \u003c- Per-project\n  /working/tree/projects/bar/.commit.lock  \u003c- Per-project\n```\n\n## Implementation Details\n\n### Finding Project Archive Path\nNeed to trace from operation context to project archive path.\nMay require passing archive_path through more functions.\n\n### Lock File Cleanup\n- Stale locks should be cleaned up\n- Consider using fcntl.flock() for automatic cleanup on process exit\n- Or implement lock timeout with staleness check\n\n### Backwards Compatibility\n- Old global lock file can be removed or ignored\n- No migration needed since locks are ephemeral\n\n## Concurrency Model After Fix\n```\nProject A operations: [lock A] -\u003e commit -\u003e [unlock A]\nProject B operations: [lock B] -\u003e commit -\u003e [unlock B]\n                      ^^^ These can now happen in parallel!\n```\n\n## Testing Strategy\n1. Start two agents on different projects simultaneously\n2. Have both perform commit operations\n3. Verify both complete without blocking each other\n4. Verify no data corruption in either project\n\n## Verification\nUse strace or timing to verify parallel execution:\n```bash\n# In terminal 1\ntime python -c \"agent_operation_on_project_a()\"\n\n# In terminal 2 (simultaneously)  \ntime python -c \"agent_operation_on_project_b()\"\n\n# Both should complete in ~same time, not 2x the time\n```\n\n## Risk Assessment\n- **Medium risk**: Locking changes require careful testing\n- **Concern**: Race conditions during lock acquisition\n- **Mitigation**: Thorough integration tests with concurrent access\n\n## Dependencies\n- Should be done after commit batching for clean implementation","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-12T01:09:23.557640132-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:09:23.557640132-05:00","dependencies":[{"issue_id":"mcp_agent_mail-pk2","depends_on_id":"mcp_agent_mail-roy","type":"blocks","created_at":"2026-01-12T01:12:50.058614282-05:00","created_by":"ubuntu"}]}
{"id":"mcp_agent_mail-pyi","title":"Phase 1 verification checkpoint - validate N+1 elimination and correctness","description":"# Phase 1 Verification Checkpoint\n\n## Purpose\nAfter completing all Phase 1 tasks, verify:\n1. All N+1 patterns are eliminated (query counts reduced)\n2. All operations produce identical outputs (isomorphism proven)\n3. Performance improved as expected\n\n## Verification Steps\n\n### 1. Run E2E Correctness Tests\n```bash\n# This MUST pass - outputs must match golden files\npytest tests/e2e/ -v\n\n# If any test fails, the optimization changed behavior - FIX BEFORE PROCEEDING\n```\n\n### 2. Verify Query Count Reductions\n\nRun the query count verification for each fixed operation:\n\n| Operation | Before | After | Expected |\n|-----------|--------|-------|----------|\n| send_message (5 recipients) | ~8 | 1-2 | ≤3 queries |\n| send_message (10 recipients) | ~13 | 1-2 | ≤3 queries |\n| fetch_inbox (20 messages) | ~21 | 2-3 | ≤5 queries |\n| list_outbox (20 messages) | ~21 | 2-3 | ≤5 queries |\n| summarize_thread (20 messages) | ~40+ | 3-5 | ≤10 queries |\n\n### 3. Run Performance Benchmarks\n```bash\n# Compare against baseline\npytest tests/benchmarks/ -v --benchmark-compare=baseline\n\n# Expected improvements:\n# - send_message: 5-10x faster\n# - fetch_inbox: 10-20x faster\n# - list_outbox: 10-20x faster\n```\n\n### 4. Verify LRU Cache O(1)\n```bash\n# Run cache performance test\npytest tests/test_lru_cache_performance.py -v\n\n# Should complete 100k operations in \u003c 1 second\n```\n\n### 5. Verify Indexes Are Used\n```sql\n-- Run EXPLAIN QUERY PLAN on critical queries\n-- Should see \"USING INDEX\" not \"SCAN TABLE\"\n```\n\n### 6. Generate Phase 1 Report\n\nCreate docs/performance/phase1-results.md:\n\n```markdown\n# Phase 1 Optimization Results\n\nCompleted: YYYY-MM-DD\nCommit: \u003chash\u003e\n\n## Summary\nAll N+1 query patterns eliminated. All E2E tests passing.\n\n## Query Count Improvements\n\n| Operation | Before | After | Reduction |\n|-----------|--------|-------|-----------|\n| send_message (5r) | X | Y | Z% |\n| ... | ... | ... | ... |\n\n## Latency Improvements\n\n| Operation | Before p95 | After p95 | Speedup |\n|-----------|------------|-----------|---------|\n| ... | ... | ... | ... |\n\n## Verification Evidence\n- E2E tests: All X tests passing\n- Benchmark comparison: [link to results]\n- Query logs: [link to before/after comparison]\n```\n\n## Acceptance Criteria\n- [ ] ALL E2E tests pass (correctness preserved)\n- [ ] Query counts reduced as expected for all N+1 fixes\n- [ ] Latency improved by at least 5x for send_message\n- [ ] Latency improved by at least 10x for inbox/outbox\n- [ ] LRU cache operations complete in O(1) time\n- [ ] Phase 1 report generated and committed\n\n## What To Do If Tests Fail\n\n### E2E Test Failure\n1. DO NOT PROCEED to Phase 2\n2. Examine the diff in test output\n3. Identify which optimization changed behavior\n4. Fix the optimization to preserve exact output\n5. Re-run E2E tests until passing\n\n### Performance Not Improved\n1. Verify instrumentation is correct\n2. Check if optimization was applied correctly\n3. Review query logs for unexpected patterns\n4. May need to revisit implementation\n\n## Dependencies\n- All Phase 1 tasks must be complete\n- E2E test suite must exist\n- Baseline metrics must be captured\n\n## Blocks\n- Phase 2 feature (cannot start Phase 2 until Phase 1 verified)","status":"open","priority":0,"issue_type":"task","created_at":"2026-01-12T01:19:51.789839795-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:19:51.789839795-05:00","dependencies":[{"issue_id":"mcp_agent_mail-pyi","depends_on_id":"mcp_agent_mail-dwu","type":"blocks","created_at":"2026-01-12T01:20:18.545150156-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-pyi","depends_on_id":"mcp_agent_mail-cyw","type":"blocks","created_at":"2026-01-12T01:20:18.576503886-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-pyi","depends_on_id":"mcp_agent_mail-jxj","type":"blocks","created_at":"2026-01-12T01:20:18.606383715-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-pyi","depends_on_id":"mcp_agent_mail-aid","type":"blocks","created_at":"2026-01-12T01:20:18.63737295-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-pyi","depends_on_id":"mcp_agent_mail-4yy","type":"blocks","created_at":"2026-01-12T01:20:18.666281562-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-pyi","depends_on_id":"mcp_agent_mail-g77","type":"blocks","created_at":"2026-01-12T01:20:18.694335848-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-pyi","depends_on_id":"mcp_agent_mail-dcy","type":"blocks","created_at":"2026-01-12T01:20:18.723771371-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-pyi","depends_on_id":"mcp_agent_mail-ab8","type":"blocks","created_at":"2026-01-12T01:20:18.750927237-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-pyi","depends_on_id":"mcp_agent_mail-7hm","type":"blocks","created_at":"2026-01-12T01:21:02.40964484-05:00","created_by":"ubuntu"}]}
{"id":"mcp_agent_mail-qe7","title":"Security: Path Traversal","description":"priority: 3","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-05T21:02:17.871514-05:00","updated_at":"2026-01-06T00:49:03.166984-05:00","closed_at":"2026-01-06T00:49:03.166984-05:00","close_reason":"Created 27 tests covering path traversal security: agent name sanitization, archive tree/content path validation, project slug sanitization, file reservation patterns, attachment path handling, and archive extraction security.","dependencies":[{"issue_id":"mcp_agent_mail-qe7","depends_on_id":"mcp_agent_mail-aew","type":"blocks","created_at":"2026-01-05T21:02:55.30258-05:00","created_by":"jemanuel"}]}
{"id":"mcp_agent_mail-qem","title":"Replace list.index() in sort key with pre-computed dict","description":"# Replace list.index() in Sort Key with Pre-computed Dict\n\n## Problem Statement\nSort key function uses `list.index()` which is O(p) where p = list length,\nmaking the overall sort O(n * p * log n) instead of O(n log n).\n\n## Code Location\n`src/mcp_agent_mail/share.py` - Line approximately 383\n\n## Current Code (THE PROBLEM)\n```python\npreferred_order = [\"high\", \"normal\", \"low\", \"info\"]\n\n# This calls .index() for EVERY comparison!\nhints.sort(key=lambda hint: preferred_order.index(hint.key) if hint.key in preferred_order else len(preferred_order))\n```\n\n## Impact Analysis\n- sort() makes O(n log n) comparisons\n- Each comparison calls .index() which is O(p)\n- Total: O(n * p * log n) instead of O(n log n)\n- For n=1000 hints and p=4 priorities: 4x slower than necessary\n\n## Required Fix\nPre-compute a dict for O(1) lookups:\n\n```python\npreferred_order = [\"high\", \"normal\", \"low\", \"info\"]\n\n# Pre-compute once: O(p)\norder_map = {key: idx for idx, key in enumerate(preferred_order)}\ndefault_order = len(preferred_order)\n\n# Sort with O(1) lookups\nhints.sort(key=lambda hint: order_map.get(hint.key, default_order))\n```\n\n## Why This Is Better\n- Dict lookup is O(1) vs list.index() O(p)\n- Dict creation is O(p) one-time cost\n- Sort is now pure O(n log n)\n\n## Finding Similar Patterns\nSearch for `.index(` usage within lambda or sort key contexts:\n```\n\\.sort\\(.*\\.index\\(\nsorted\\(.*\\.index\\(\n```\n\n## Testing Strategy\n1. Verify sort order is identical before/after\n2. Benchmark with large hint lists\n3. Test with keys not in preferred_order\n\n## Verification\n```python\nimport random\nimport time\n\npreferred_order = [\"high\", \"normal\", \"low\", \"info\", \"debug\", \"trace\"]\n\n# Generate test data\nhints = [type('Hint', (), {'key': random.choice(preferred_order + ['unknown'])})() \n         for _ in range(10000)]\n\n# Old way\ndef old_sort(hints):\n    return sorted(hints, key=lambda h: preferred_order.index(h.key) if h.key in preferred_order else len(preferred_order))\n\n# New way\ndef new_sort(hints):\n    order_map = {k: i for i, k in enumerate(preferred_order)}\n    default = len(preferred_order)\n    return sorted(hints, key=lambda h: order_map.get(h.key, default))\n\n# Verify identical results\nold_result = old_sort(hints.copy())\nnew_result = new_sort(hints.copy())\nassert [h.key for h in old_result] == [h.key for h in new_result]\n\n# Benchmark\nt1 = time.perf_counter()\nfor _ in range(100):\n    old_sort(hints.copy())\nold_time = time.perf_counter() - t1\n\nt2 = time.perf_counter()\nfor _ in range(100):\n    new_sort(hints.copy())\nnew_time = time.perf_counter() - t2\n\nprint(f\"Old: {old_time:.3f}s, New: {new_time:.3f}s, Speedup: {old_time/new_time:.1f}x\")\n```\n\n## Dependencies\nNone - self-contained fix","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-12T01:10:51.163791202-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:10:51.163791202-05:00"}
{"id":"mcp_agent_mail-r6n","title":"Phase 2: Caching and Git Operation Batching","description":"# Phase 2: High Priority Optimizations (3-5x Improvement)\n\n## Scope\nThis phase targets expensive repeated computations (PathSpec compilation) and inefficient\nGit operations (multiple commits, global locks).\n\n## Why This Matters\n- PathSpec.from_lines() compiles regex patterns - expensive when called repeatedly\n- Each Git commit involves index manipulation, SHA computation, and disk sync\n- Global commit lock serializes ALL projects - no parallelism possible\n\n## Expected Impact\n- PathSpec operations: 10-50x improvement through caching\n- Git operations: 3-5x improvement through batching\n- Cross-project operations: Full parallelism instead of serialization\n\n## Key Files\n- src/mcp_agent_mail/app.py - Line 2938-2939 (PathSpec compilation)\n- src/mcp_agent_mail/storage.py - Lines 737-851 (commits), 1218 (global lock)\n- src/mcp_agent_mail/http.py - Lines 1388-1389 (snippet calls)\n\n## Technical Approach\n\n### PathSpec Caching\nThe pathspec library compiles glob patterns into regex. This compilation is deterministic\nand expensive. Adding @lru_cache(maxsize=512) around pattern compilation eliminates\nredundant work in reservation conflict detection loops.\n\n### Git Commit Batching\nCurrently, operations like register_agent + file_reservation create separate commits.\nThese can be consolidated into atomic multi-path commits, reducing Git overhead.\n\n### Per-Project Locks\nThe current `.commit.lock` is at the working tree root, serializing all projects.\nMoving to `projects/\u003cslug\u003e/.commit.lock` enables true parallelism.\n\n## Tasks\n1. Add @lru_cache to PathSpec compilation\n2. Consolidate Git commits for related operations\n3. Implement per-project commit locks\n4. Fix triple snippet() calls in search\n5. Replace sorted()[:n] with heapq.nlargest()","status":"open","priority":1,"issue_type":"feature","created_at":"2026-01-12T01:05:51.817252389-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:05:51.817252389-05:00","dependencies":[{"issue_id":"mcp_agent_mail-r6n","depends_on_id":"mcp_agent_mail-mzo","type":"blocks","created_at":"2026-01-12T01:06:18.773869151-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-r6n","depends_on_id":"mcp_agent_mail-h4o","type":"blocks","created_at":"2026-01-12T01:13:10.500589976-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-r6n","depends_on_id":"mcp_agent_mail-roy","type":"blocks","created_at":"2026-01-12T01:13:10.523414854-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-r6n","depends_on_id":"mcp_agent_mail-pk2","type":"blocks","created_at":"2026-01-12T01:13:10.546568452-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-r6n","depends_on_id":"mcp_agent_mail-1kw","type":"blocks","created_at":"2026-01-12T01:13:10.568191737-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-r6n","depends_on_id":"mcp_agent_mail-0uh","type":"blocks","created_at":"2026-01-12T01:13:10.587622482-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-r6n","depends_on_id":"mcp_agent_mail-pyi","type":"blocks","created_at":"2026-01-12T01:21:26.776571701-05:00","created_by":"ubuntu"}]}
{"id":"mcp_agent_mail-roy","title":"Consolidate Git commits for related operations","description":"# Consolidate Git Commits for Related Operations\n\n## Problem Statement\nOperations that modify multiple files create separate Git commits for each file,\nmultiplying Git overhead unnecessarily.\n\n## Code Location\n`src/mcp_agent_mail/storage.py` - Lines approximately 737-851 and other commit sites\n\n## Current Behavior\n```python\n# register_agent creates one commit\nawait _commit(archive.repo, settings, f\"agent: profile {agent['name']}\", [profile_path])\n\n# Then file_reservation_paths creates another commit  \nawait _commit(archive.repo, settings, f\"file_reservation: {pattern}\", [reservation_path])\n\n# Two separate commits for what is logically one operation!\n```\n\n## Impact Analysis\nEach Git commit involves:\n- Reading and updating the index\n- Computing SHA-1 hashes for tree objects\n- Writing commit object to pack\n- Potentially triggering hooks\n- fsync() for durability\n\nTwo commits = 2x this overhead.\n\n## Required Fix\nCreate a batched commit helper:\n\n```python\nasync def _commit_batch(\n    repo: Repo,\n    settings: ServerSettings,\n    message: str,\n    paths: list[str],\n) -\u003e str | None:\n    \"\"\"Batch multiple paths into a single atomic commit.\n    \n    Args:\n        repo: Git repository\n        settings: Server settings for author info\n        message: Commit message\n        paths: List of paths to add and commit\n        \n    Returns:\n        Commit SHA if successful, None if nothing to commit\n    \"\"\"\n    if not paths:\n        return None\n        \n    # Add all paths to index\n    for path in paths:\n        repo.index.add([path])\n    \n    # Single commit\n    return repo.index.commit(\n        message,\n        author=Actor(settings.git_author_name, settings.git_author_email),\n        committer=Actor(settings.git_author_name, settings.git_author_email),\n    ).hexsha\n```\n\n## Usage Pattern\n```python\n# Before: 2 commits\nawait _commit(repo, settings, \"agent profile\", [profile_path])\nawait _commit(repo, settings, \"reservation\", [reservation_path])\n\n# After: 1 commit with both files\nawait _commit_batch(\n    repo, settings,\n    f\"agent: {agent['name']} + reservation: {pattern}\",\n    [profile_path, reservation_path]\n)\n```\n\n## Implementation Considerations\n\n### Commit Message Format\nWith multiple files, commit messages should be descriptive:\n```\nagent: BlueLake session setup\n\n- profile: agents/BlueLake/profile.json\n- reservation: file_reservations/abc123.json\n```\n\n### Atomicity\nBatched commits are atomic - either all files are committed or none.\nThis is actually BETTER than separate commits for related operations.\n\n### Where to Apply\nFind all places where multiple commits happen in sequence for related operations:\n1. Agent registration + initial reservation\n2. Message send + inbox/outbox copies\n3. Any other multi-file operations\n\n## Testing Strategy\n1. Perform operation that currently creates 2 commits\n2. Verify only 1 commit is created after fix\n3. Verify commit contains all expected files\n4. Verify git log shows proper history\n\n## Risk Assessment\n- **Low risk**: Git commits are atomic\n- **Consideration**: Commit messages less granular\n- **Mitigation**: Use detailed commit messages listing all changes\n\n## Dependencies\nNone - but should be coordinated with per-project locks task","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-12T01:09:23.409860588-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:09:23.409860588-05:00"}
{"id":"mcp_agent_mail-tm6","title":"Milestone: Critical Path Coverage","description":"priority: 1","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-05T21:02:17.880711-05:00","updated_at":"2026-01-06T00:13:24.562538-05:00","closed_at":"2026-01-06T00:13:24.562538-05:00","close_reason":"All P0-P1 critical path tests completed: kkp (Session Context), yhk (Datetime), aew (File Reservation), uvf (Message Delivery)","dependencies":[{"issue_id":"mcp_agent_mail-tm6","depends_on_id":"mcp_agent_mail-yhk","type":"blocks","created_at":"2026-01-05T21:02:56.643878-05:00","created_by":"jemanuel"},{"issue_id":"mcp_agent_mail-tm6","depends_on_id":"mcp_agent_mail-kkp","type":"blocks","created_at":"2026-01-05T21:02:56.778506-05:00","created_by":"jemanuel"},{"issue_id":"mcp_agent_mail-tm6","depends_on_id":"mcp_agent_mail-uvf","type":"blocks","created_at":"2026-01-05T21:02:56.920383-05:00","created_by":"jemanuel"},{"issue_id":"mcp_agent_mail-tm6","depends_on_id":"mcp_agent_mail-aew","type":"blocks","created_at":"2026-01-05T21:02:57.079543-05:00","created_by":"jemanuel"}]}
{"id":"mcp_agent_mail-try","title":"Consolidate repeated file existence checks into single stat()","description":"# Consolidate File Existence Checks\n\n## Problem Statement\nCode performs multiple separate file existence checks (stat syscalls) when\na single stat() call with exception handling would suffice.\n\n## Code Location\n`src/mcp_agent_mail/storage.py` - Lines approximately 403-424\n\n## Current Code (THE PROBLEM)\n```python\n# Multiple stat() syscalls for the same logical check\nif not self._path.exists():        # stat() #1\n    return False\n    \nif self._metadata_path.exists():   # stat() #2\n    metadata = self._load_metadata()\nelif self._legacy_path.exists():   # stat() #3\n    metadata = self._load_legacy()\n```\n\n## Impact Analysis\n- Each .exists() is a stat() syscall\n- stat() requires kernel context switch\n- 3 syscalls where 1-2 would suffice\n- On network filesystems, latency compounds significantly\n\n## Required Fix\nUse exception handling for control flow:\n\n```python\ndef _load_project_data(self) -\u003e ProjectData | None:\n    \"\"\"Load project data with minimal syscalls.\"\"\"\n    try:\n        # Try primary path first - one stat() + read if exists\n        return self._load_metadata()\n    except FileNotFoundError:\n        pass\n    \n    try:\n        # Fall back to legacy path\n        return self._load_legacy()\n    except FileNotFoundError:\n        return None\n```\n\nOr use single stat() with result caching:\n\n```python\ndef _check_paths(self) -\u003e tuple[bool, bool, bool]:\n    \"\"\"Check multiple path existence with minimal syscalls.\"\"\"\n    main_exists = self._path.exists()\n    if not main_exists:\n        return False, False, False\n    \n    # Only check sub-paths if main exists\n    metadata_exists = self._metadata_path.exists()\n    legacy_exists = self._legacy_path.exists() if not metadata_exists else False\n    \n    return main_exists, metadata_exists, legacy_exists\n```\n\n## EAFP vs LBYL\nPython idiom \"Easier to Ask Forgiveness than Permission\" (EAFP) suggests:\n- Try the operation, handle exception if it fails\n- Rather than checking if operation will succeed then doing it\n\nThis is especially efficient for file operations where the check and\noperation are separate syscalls.\n\n## Finding Similar Patterns\nSearch for consecutive .exists() calls:\n```python\nif .*\\.exists\\(\\)\n```\n\n## Testing Strategy\n1. Verify behavior unchanged for existing files\n2. Verify behavior unchanged for missing files\n3. Use strace to count stat() syscalls before/after\n4. Benchmark on local and network filesystems\n\n## Verification\n```bash\n# Count syscalls before fix\nstrace -e stat,statx,newfstatat python -c \"project._check_paths()\" 2\u003e\u00261 | grep -c stat\n\n# Count syscalls after fix\nstrace -e stat,statx,newfstatat python -c \"project._check_paths()\" 2\u003e\u00261 | grep -c stat\n```\n\n## Dependencies\nNone - self-contained fix","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-12T01:10:51.317969551-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:10:51.317969551-05:00"}
{"id":"mcp_agent_mail-tty","title":"Add CI performance regression detection","description":"# CI Performance Regression Detection\n\n## Purpose\nAdd automated checks to CI that detect performance regressions, particularly N+1 query patterns being reintroduced.\n\n## Why This Is Essential\n- Performance fixes are easy to undo accidentally\n- New code can introduce N+1 patterns without developers realizing\n- CI is the safety net for catching regressions\n\n## Components\n\n### 1. Query Count Assertions in Integration Tests\n\nAdd to pytest configuration (tests/conftest.py):\n\n\\`\\`\\`python\nimport pytest\nfrom mcp_agent_mail.instrumentation import track_queries, install_query_hooks\nfrom mcp_agent_mail.db import get_engine\n\n@pytest.fixture(scope=\"session\", autouse=True)\ndef setup_query_tracking():\n    \"\"\"Install query tracking for all tests.\"\"\"\n    engine = get_engine()\n    install_query_hooks(engine.sync_engine)\n\n@pytest.fixture\ndef assert_max_queries():\n    \"\"\"Fixture that provides query count assertion.\"\"\"\n    class QueryAsserter:\n        async def __aenter__(self):\n            self._stats = None\n            self._ctx = track_queries()\n            self._stats = await self._ctx.__aenter__()\n            return self\n        \n        async def __aexit__(self, *args):\n            await self._ctx.__aexit__(*args)\n        \n        def assert_max(self, max_count: int, message: str = \"\"):\n            actual = self._stats.count\n            if actual \u003e max_count:\n                query_list = \"\\n\".join(\n                    f\"  {i+1}. {q.sql[:80]}...\"\n                    for i, q in enumerate(self._stats.queries)\n                )\n                pytest.fail(\n                    f\"Query count {actual} exceeds max {max_count}. \"\n                    f\"{message}\\nQueries:\\n{query_list}\"\n                )\n    \n    return QueryAsserter()\n\\`\\`\\`\n\n### 2. Add Query Count Tests for Critical Operations\n\n\\`\\`\\`python\n# tests/test_performance_guards.py\n\"\"\"Performance guard tests - fail if N+1 patterns detected.\"\"\"\n\nimport pytest\n\nclass TestSendMessageQueryCount:\n    @pytest.mark.asyncio\n    async def test_single_recipient_max_queries(\n        self, session, project, agents, assert_max_queries\n    ):\n        \"\"\"send_message with 1 recipient should use ≤3 queries.\"\"\"\n        async with assert_max_queries as qa:\n            await send_message(\n                project_key=project.human_key,\n                sender_name=agents[0].name,\n                to=[agents[1].name],\n                subject=\"Test\",\n                body_md=\"Test\",\n            )\n        qa.assert_max(3, \"Possible N+1 in send_message\")\n    \n    @pytest.mark.asyncio\n    async def test_ten_recipients_max_queries(\n        self, session, project, agents, assert_max_queries\n    ):\n        \"\"\"send_message with 10 recipients should still use ≤5 queries.\"\"\"\n        async with assert_max_queries as qa:\n            await send_message(\n                project_key=project.human_key,\n                sender_name=agents[0].name,\n                to=[a.name for a in agents[1:11]],\n                subject=\"Test\",\n                body_md=\"Test\",\n            )\n        # If this fails with \u003e5 queries, N+1 pattern detected!\n        qa.assert_max(5, \"N+1 pattern: queries scale with recipient count\")\n\n\nclass TestInboxQueryCount:\n    @pytest.mark.asyncio\n    async def test_inbox_with_fifty_messages(\n        self, session, project, agents, populated_inbox_50, assert_max_queries\n    ):\n        \"\"\"fetch_inbox with 50 messages should use ≤5 queries.\"\"\"\n        async with assert_max_queries as qa:\n            await fetch_inbox(\n                project_key=project.human_key,\n                agent_name=agents[1].name,\n                limit=50,\n            )\n        qa.assert_max(5, \"N+1 pattern: queries scale with message count\")\n\n\nclass TestOutboxQueryCount:\n    @pytest.mark.asyncio\n    async def test_outbox_with_fifty_messages(\n        self, session, project, agents, populated_outbox_50, assert_max_queries\n    ):\n        \"\"\"list_outbox with 50 messages should use ≤5 queries.\"\"\"\n        async with assert_max_queries as qa:\n            await list_outbox(\n                project_key=project.human_key,\n                agent_name=agents[0].name,\n                limit=50,\n            )\n        qa.assert_max(5, \"N+1 pattern: queries scale with message count\")\n\\`\\`\\`\n\n### 3. GitHub Actions Workflow\n\n\\`\\`\\`yaml\n# .github/workflows/performance-check.yml\nname: Performance Checks\n\non:\n  pull_request:\n    paths:\n      - 'src/**'\n      - 'tests/**'\n\njobs:\n  query-count-check:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - uses: astral-sh/setup-uv@v1\n      \n      - name: Install dependencies\n        run: uv sync\n      \n      - name: Run performance guard tests\n        run: |\n          uv run pytest tests/test_performance_guards.py -v --tb=long\n        env:\n          ENABLE_QUERY_TRACKING: \"true\"\n\\`\\`\\`\n\n### 4. Pre-commit Hook (Optional)\n\n\\`\\`\\`bash\n#!/bin/bash\n# .git/hooks/pre-commit\n# Run quick performance checks before commit\n\nif git diff --cached --name-only | grep -q \"^src/\"; then\n    echo \"Running performance guard tests...\"\n    pytest tests/test_performance_guards.py -x -q\nfi\n\\`\\`\\`\n\n## Acceptance Criteria\n- [ ] Query tracking enabled in test environment\n- [ ] assert_max_queries fixture available in tests\n- [ ] Performance guard tests for send_message, inbox, outbox\n- [ ] CI workflow runs performance checks on PRs\n- [ ] Tests fail if N+1 patterns detected\n\n## Dependencies\n- mcp_agent_mail-dbt (instrumentation infrastructure)\n- mcp_agent_mail-ab8 (E2E test suite foundation)\n\n## Blocks\n- Provides long-term protection against performance regressions","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-12T01:27:40.364623173-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:27:40.364623173-05:00","dependencies":[{"issue_id":"mcp_agent_mail-tty","depends_on_id":"mcp_agent_mail-dbt","type":"blocks","created_at":"2026-01-12T01:27:45.447734439-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-tty","depends_on_id":"mcp_agent_mail-ab8","type":"blocks","created_at":"2026-01-12T01:27:45.478111323-05:00","created_by":"ubuntu"}]}
{"id":"mcp_agent_mail-u8g","title":"Integration Tests: Thread Conversations","description":"priority: 2","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.485294-05:00","updated_at":"2026-01-05T21:00:46.22444-05:00","deleted_at":"2026-01-05T21:00:46.22444-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-udi","title":"E2E: Disaster Recovery","description":"priority: 4","status":"closed","priority":4,"issue_type":"task","created_at":"2026-01-05T21:02:17.879357-05:00","updated_at":"2026-01-06T02:34:40.022403-05:00","closed_at":"2026-01-06T02:34:40.022403-05:00","close_reason":"5 E2E disaster recovery tests implemented and passing","dependencies":[{"issue_id":"mcp_agent_mail-udi","depends_on_id":"mcp_agent_mail-enu","type":"blocks","created_at":"2026-01-05T21:02:56.527536-05:00","created_by":"jemanuel"}]}
{"id":"mcp_agent_mail-uvf","title":"Core: Message Delivery Flow","description":"priority: 1","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-05T21:02:17.835672-05:00","updated_at":"2026-01-05T22:06:09.098428-05:00","closed_at":"2026-01-05T22:06:09.098428-05:00","close_reason":"Created 21 comprehensive tests for message delivery flow covering: basic sending, multiple recipients (to/cc/bcc), threads, importance levels, ack flags, inbox fetching, replies, read/acknowledge workflows, and error handling. All tests passing."}
{"id":"mcp_agent_mail-vih","title":"Phase 3: Algorithm and Concurrency Improvements","description":"# Phase 3: Medium Priority Optimizations (30-50% Improvement)\n\n## Scope\nThis phase addresses algorithmic inefficiencies and concurrency issues that cause\nmoderate but noticeable performance degradation.\n\n## Issues Addressed\n\n### Algorithmic Problems\n1. **String normalization O(n*k)** - While loop with replace() iterates k times\n2. **List membership O(n²)** - Checking `if x not in list` in a loop\n3. **Index lookup in sort O(n*p)** - Using list.index() as sort key\n4. **Repeated stat() calls** - Multiple file existence checks\n\n### Concurrency Problems\n1. **Rate limiter race condition** - No synchronization on bucket access\n2. **Schema lock too coarse** - Single lock for all schema operations\n3. **Memory leak in process locks** - Lock dict grows unboundedly\n\n## Expected Impact\n- String operations: k× improvement (k = iterations eliminated)\n- Collection operations: n× improvement (n = collection size)\n- Concurrent operations: Reduced contention, better throughput\n\n## Key Files\n- src/mcp_agent_mail/app.py - Lines 1054-1055\n- src/mcp_agent_mail/share.py - Lines 383, 696\n- src/mcp_agent_mail/storage.py - Lines 403-424\n- src/mcp_agent_mail/http.py - Lines 306-314\n- src/mcp_agent_mail/db.py - Lines 235-251\n\n## Technical Notes\n\n### Why These Are Medium Priority\nThese issues cause real performance problems but typically in specific scenarios:\n- String normalization: Only affects subjects/bodies with multiple spaces\n- List membership: Only affects collections that grow large\n- Race conditions: Only manifest under concurrent load\n\n### Implementation Considerations\n- Regex for string normalization is cleaner but slightly different semantics (tabs?)\n- Set membership requires hashable elements (may need to hash by ID)\n- RWLock implementation adds complexity - ensure it's worth it","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-12T01:05:52.421204634-05:00","created_by":"ubuntu","updated_at":"2026-01-12T01:05:52.421204634-05:00","dependencies":[{"issue_id":"mcp_agent_mail-vih","depends_on_id":"mcp_agent_mail-r6n","type":"blocks","created_at":"2026-01-12T01:06:18.790884586-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-vih","depends_on_id":"mcp_agent_mail-24j","type":"blocks","created_at":"2026-01-12T01:13:11.153106468-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-vih","depends_on_id":"mcp_agent_mail-i9c","type":"blocks","created_at":"2026-01-12T01:13:11.17538116-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-vih","depends_on_id":"mcp_agent_mail-qem","type":"blocks","created_at":"2026-01-12T01:13:11.195094317-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-vih","depends_on_id":"mcp_agent_mail-try","type":"blocks","created_at":"2026-01-12T01:13:11.216408409-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-vih","depends_on_id":"mcp_agent_mail-9tj","type":"blocks","created_at":"2026-01-12T01:13:11.237984335-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-vih","depends_on_id":"mcp_agent_mail-az3","type":"blocks","created_at":"2026-01-12T01:13:11.259260066-05:00","created_by":"ubuntu"},{"issue_id":"mcp_agent_mail-vih","depends_on_id":"mcp_agent_mail-i8s","type":"blocks","created_at":"2026-01-12T01:21:26.823685949-05:00","created_by":"ubuntu"}]}
{"id":"mcp_agent_mail-w51","title":"HTTP: Authentication","description":"priority: 2","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-05T21:02:17.864235-05:00","updated_at":"2026-01-06T00:34:24.246687-05:00","closed_at":"2026-01-06T00:34:24.246687-05:00","close_reason":"23 HTTP authentication tests: bearer auth, localhost bypass, CORS/health bypass, RBAC, OAuth metadata, JWT helpers, rate limiting"}
{"id":"mcp_agent_mail-xhf","title":"E2E Test Script: Performance Under Load","description":"priority: 3","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.487882-05:00","updated_at":"2026-01-05T21:00:45.544252-05:00","deleted_at":"2026-01-05T21:00:45.544252-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-y35","title":"Testing Infrastructure Foundation","description":"priority: 1","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.465256-05:00","updated_at":"2026-01-05T21:00:48.939985-05:00","deleted_at":"2026-01-05T21:00:48.939985-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-y59","title":"P3 - Performance Tests","description":"Test performance characteristics.","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T21:02:17.877469-05:00","updated_at":"2026-01-05T21:02:37.860358-05:00","deleted_at":"2026-01-05T21:02:37.860358-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-yh8","title":"Security: Input Sanitization","description":"priority: 3","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-05T21:02:17.870336-05:00","updated_at":"2026-01-06T00:53:26.090299-05:00","closed_at":"2026-01-06T00:53:26.090299-05:00","close_reason":"Added 28 comprehensive security tests: FTS sanitization, SQL injection prevention, path traversal, DoS handling, null byte injection, unicode attacks, XSS prevention, and malformed JSON handling. All tests pass.","dependencies":[{"issue_id":"mcp_agent_mail-yh8","depends_on_id":"mcp_agent_mail-uvf","type":"blocks","created_at":"2026-01-05T21:02:55.202381-05:00","created_by":"jemanuel"}]}
{"id":"mcp_agent_mail-yhk","title":"Regression: Datetime Naive/Aware Handling","description":"priority: 0","status":"closed","priority":0,"issue_type":"task","created_at":"2026-01-05T21:02:17.828319-05:00","updated_at":"2026-01-05T21:34:15.142142-05:00","closed_at":"2026-01-05T21:34:15.142142-05:00","close_reason":"All 20 datetime regression tests implemented and passing"}
{"id":"mcp_agent_mail-yi0","title":"Unit Tests: cli.py - Core Commands","description":"priority: 2","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.472698-05:00","updated_at":"2026-01-05T21:00:47.51517-05:00","deleted_at":"2026-01-05T21:00:47.51517-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-ypm","title":"Unit Tests: storage.py - Archive Operations","description":"priority: 2","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.471913-05:00","updated_at":"2026-01-05T21:00:47.676906-05:00","deleted_at":"2026-01-05T21:00:47.676906-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-yzu","title":"MCP Tools: Happy Path Coverage","description":"priority: 1","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-05T21:02:17.841695-05:00","updated_at":"2026-01-05T22:36:39.811899-05:00","closed_at":"2026-01-05T22:36:39.811899-05:00","close_reason":"All MCP tools have happy path coverage across existing test suite: test_server.py, test_project_agent_setup.py, test_message_delivery_regression.py, test_contact_management_flow.py, test_file_reservation_lifecycle.py, test_macros.py, test_guard_tools.py","dependencies":[{"issue_id":"mcp_agent_mail-yzu","depends_on_id":"mcp_agent_mail-mm2","type":"blocks","created_at":"2026-01-05T21:02:54.212411-05:00","created_by":"jemanuel"}]}
{"id":"mcp_agent_mail-z00","title":"Integration Tests: HTTP Transport","description":"priority: 2","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.486362-05:00","updated_at":"2026-01-05T21:00:45.977935-05:00","deleted_at":"2026-01-05T21:00:45.977935-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-z9g","title":"Unit Tests: share.py - Archive Restore","description":"priority: 2","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.483519-05:00","updated_at":"2026-01-05T21:00:46.661028-05:00","deleted_at":"2026-01-05T21:00:46.661028-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-zgs","title":"P0 - Critical Regression Tests","description":"These tests prevent recurrence of known bugs and must pass before any release.","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T21:02:17.82546-05:00","updated_at":"2026-01-05T21:02:37.011981-05:00","deleted_at":"2026-01-05T21:02:37.011981-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"mcp_agent_mail-zmk","title":"Unit Tests: models.py","description":"priority: 2","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-05T20:56:40.466891-05:00","updated_at":"2026-01-05T21:00:48.845938-05:00","deleted_at":"2026-01-05T21:00:48.845938-05:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
